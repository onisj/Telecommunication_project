{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRv8adCyfT6f"
   },
   "source": [
    "# **CUSTOMER CHURN PREDICTION FOR TELECOMMUNICATIONS COMPANY USING MACHINE LEARNING**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXZnQ0Q5eo-e"
   },
   "source": [
    "## **Business Overview**\n",
    "\n",
    "The telecommunications industry is a rapidly growing sector that is constantly evolving to meet the demands of consumers. As technology advances and user behavior changes, telecom operators face a variety of challenges that can impact their business success. In order to stay competitive and meet customer needs, it is important for telecom companies to regularly analyze their data to identify relevant problems and opportunities for improvement.\n",
    "\n",
    "This Project is part of the \"telecom data\" cluster of projects, targeting the telecom industry. It is recommended to go through the [\"Exploratory Data Analysis\"](https://www.projectpro.io/data-science-use-cases/telecom-data-analysis-project) notebook before executing this project to understand the Data better. \n",
    "\n",
    "**Aim:**\n",
    "\n",
    "The aim of a churn prediction notebook is to develop a machine learning model that can predict which customers are likely to churn or discontinue their use of a service or product. Churn prediction is a critical business problem for companies that operate on a subscription or recurring revenue model, such as telecommunications companies. \n",
    "\n",
    "While the project will involve building a churn prediction model, the primary focus will be on the importance of monitoring and adapting to changes in the data that may affect the accuracy and effectiveness of the model over time. The project will also emphasize the need for a feedback loop that allows for continuous improvement and refinement of the model based on new data and changing business requirements. By highlighting these concepts, the project aims to help businesses understand the importance of staying agile and adaptable in their machine learning approaches, rather than solely focusing on the accuracy of a single model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![image.png](https://images.unsplash.com/photo-1584438784894-089d6a62b8fa?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1170&q=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yaKuQlUUooQ"
   },
   "source": [
    "\n",
    "\n",
    "**What is Churn Prediction?**\n",
    "\n",
    "Churn prediction is the process of identifying customers who are likely to discontinue using a service or product. In the context of the telecom sector, churn prediction is the process of identifying customers who are likely to switch to a competitor or terminate their contract with their current service provider.\n",
    "\n",
    "Churn prediction is a critical problem in the telecom sector, as it has a significant impact on a service provider's revenue and profitability. The telecom sector is highly competitive, and service providers are constantly vying for customers' attention. It helps service providers identify customers who are at risk of leaving and take proactive measures to retain them.\n",
    "\n",
    "**Challenges**\n",
    "\n",
    "* Churn prediction is a challenging problem, as it involves analyzing large volumes of data from multiple sources. Telecom service providers generate a vast amount of data from customer interactions, network performance, and billing systems. This data is typically stored in disparate systems, making it difficult to analyze and derive insights.\n",
    "\n",
    "* Another challenge in churn prediction is the diversity of customer behavior. Customers have different reasons for leaving a service provider, and these reasons can be difficult to predict. Some customers may leave due to poor network performance, while others may switch to a competitor offering a better deal. Predicting churn accurately requires understanding these different customer behaviors and identifying the most critical predictors of churn.\n",
    "\n",
    "**Business Impact of Churn Prediction**\n",
    "\n",
    "Churn prediction has a significant impact on a telecom service provider's business. A high churn rate can result in a loss of revenue and profitability. On the other hand, an effective churn prediction model can help service providers identify customers who are at risk of leaving and take proactive measures to retain them. \n",
    "\n",
    "Here are some of the key business impacts of churn prediction:\n",
    "\n",
    "* **Revenue Protection**: Churn prediction helps service providers protect their revenue by identifying customers who are likely to leave and taking proactive measures to retain them. This can include offering discounts, upgrading service plans, or providing additional services. By retaining customers, service providers can maintain their revenue stream and avoid the cost of acquiring new customers.\n",
    "\n",
    "* **Customer Retention**: Churn prediction helps service providers retain their existing customers by identifying their needs and preferences. By understanding why customers leave, service providers can make improvements to their service and provide a better customer experience. This can help to build customer loyalty and increase the lifetime value of a customer.\n",
    "\n",
    "* **Cost Reduction**: Churn prediction can help service providers reduce the cost of acquiring new customers. Acquiring new customers is more expensive than retaining existing ones, and churn prediction can help service providers focus their marketing efforts on the most valuable customers.\n",
    "\n",
    "* **Competitive Advantage**: Churn prediction can provide a significant competitive advantage in the telecom sector. By retaining customers and improving the customer experience, service providers can differentiate themselves from their competitors. This can help to increase market share and profitability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdkeckhCKFMo"
   },
   "source": [
    "## **Approach**\n",
    "\n",
    "**Data exploration**\n",
    "\n",
    "* Load the dataset and examine its structure and contents.\n",
    "* Explore the distribution of the target variable (churn) and the features.\n",
    "\n",
    "**Data preprocessing**\n",
    "\n",
    "* Handle missing values by imputing them with appropriate values.\n",
    "* Handle outliers by removing or transforming them.\n",
    "* Encode categorical variables using one-hot encoding.\n",
    "* Scale numerical variables using Standard scaler.\n",
    "\n",
    "\n",
    "**Model training**\n",
    "\n",
    "* Split the data into training and validation sets.\n",
    "* Train logistic regression, random forest, and XGBoost models on the training set.\n",
    "* Evaluate the performance of the models on the validation set using metrics such as accuracy, precision, recall, and F1 score.\n",
    "* Choose the best-performing model based on the evaluation results.\n",
    "\n",
    "**Data drift monitoring**\n",
    "\n",
    "* Use deep checks to monitor for data drift in the input features and the target variable.\n",
    "* Check the model's performance on the validation set regularly to detect any model drift.\n",
    "\n",
    "**Inference pipeline**\n",
    "\n",
    "* Build an inference pipeline to predict churn for new data.\n",
    "* Handle cases where the label (churn) is not present in the input data.\n",
    "* Handle cases where the drift is detected by retraining the model with misclassified data.\n",
    "\n",
    "**Project Summary**\n",
    "\n",
    "* Summarize the results and draw insights from the model's predictions.\n",
    "* Provide recommendations for business actions based on the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0k5VMe9LRWn"
   },
   "source": [
    "## **Learning Outcomes**\n",
    "\n",
    "* Understanding the problem of customer churn and its impact on businesses.\n",
    "\n",
    "* Understanding the importance of data cleaning and preprocessing in building accurate machine learning models.\n",
    "* Learning how to handle missing values, outliers, and categorical variables using one-hot encoding and numerical variables using standard scaler.\n",
    "* Understanding different machine learning algorithms such as logistic regression, random forest, and XGBoost, and their pros and cons in predicting churn.\n",
    "* Learning how to evaluate machine learning models using metrics such as accuracy, precision, recall, and F1 score.\n",
    "* Understanding the importance of monitoring data drift in machine learning models and how to use deep checks to detect it.\n",
    "* Learning how to build an inference pipeline for predicting churn for new data.\n",
    "* Understanding the limitations of the model and the potential impact of false positives and false negatives.\n",
    "* Learning how to provide recommendations for business actions based on the model's predictions, such as targeted marketing campaigns and retention strategies.\n",
    "* Understanding the iterative nature of machine learning and the need for continuous improvement and retraining of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbdDx8JyLweb"
   },
   "source": [
    "## **Prerequisites**\n",
    "\n",
    "* Familiarity with Python programming language and libraries such as NumPy, Pandas, and Scikit-learn.\n",
    "\n",
    "* Basic knowledge of machine learning concepts such as supervised learning, linear regression.\n",
    "\n",
    "* Understanding of data preprocessing techniques such as handling missing values, outliers, and categorical variables.\n",
    "\n",
    "* Knowledge of Jupyter Notebook or any other Python IDE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPncgPDm1_jf"
   },
   "source": [
    "## **Execution Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6fvW78jMQip"
   },
   "source": [
    "<details>\n",
    "    <summary>Click here to view more</summary>\n",
    "\n",
    "\n",
    "### **Installation and Setup for IPYNB**\n",
    "\n",
    "For the best experience, please stay connected to the internet while executing this Project\n",
    "\n",
    "#### **Running an IPYNB on Google Colab**:\n",
    "\n",
    "* Open the [Google Colab website](https://colab.research.google.com/).\n",
    "* Click on the \"New Notebook\" button.\n",
    "* Click the \"File\" menu in the new notebook and choose \"Upload notebook.\"\n",
    "* Select the IPYNB file(notebooks/churn_modeling.ipynb) you want to upload.\n",
    "* Once the file is uploaded, click on the \"Runtime\" menu and choose \"Run all\" to execute all the cells in the notebook.\n",
    "* Alternatively, you can execute each cell individually by clicking the \"Play\" button next to the cell OR by pressing `\"shift” + \"enter\"`.\n",
    "* The Default version of Python that Colab uses currently is 3.8\n",
    "\n",
    "#### **Python setup steps for Local Machine**:\n",
    "* If you're using a local machine and do not have Python installed, follow these steps to set up Python:\n",
    "* Download and install the latest version of Python from the official Python website: https://www.python.org/downloads/.\n",
    "* Once the installation is complete, open a command prompt/terminal and type the following command to check if Python is installed `python –-version`\n",
    "* If Python is installed, the version number will be displayed.\n",
    "* This Project has been created using **Python version 3.8.10**\n",
    "\n",
    "#### **Setting up a Python Virtual Environment on Windows**\n",
    "\n",
    "* Open a command prompt by pressing `Windows Key + R`, type `cmd`, and press `Enter`.\n",
    "* Navigate to the directory where you want to create the virtual environment.\n",
    "* Install virtualenv by running the command in the command prompt `pip install virtualenv`\n",
    "* Create a new virtual environment by running the command `virtualenv env`\n",
    "* This will create a new directory called `env`, containing the virtual environment.\n",
    "* Activate the virtual environment by running the command `env\\Scripts\\activate`\n",
    "* You can now install packages and work on your project within this virtual environment.\n",
    "* To deactivate the virtual environment, simply run the command `deactivate`\n",
    "\n",
    "#### **Setting up a Python Virtual Environment on Mac**\n",
    "\n",
    "* Open the Terminal by pressing `Command + Spacebar`, type `Terminal`, and press `Enter`.\n",
    "* Navigate to the directory where you want to create the virtual environment.\n",
    "* Install virtualenv by running the following command in the terminal `pip install virtualenv`\n",
    "* Create a new virtual environment by running the following command `virtualenv env`\n",
    "* This will create a new directory called `env`, containing the virtual environment.\n",
    "* Activate the virtual environment by running the following command `source env/bin/activate`\n",
    "* You can now install packages and work on your project within this virtual environment.\n",
    "* To deactivate the virtual environment, simply run the following command `deactivate`\n",
    "\n",
    "#### **Setting up a Python Virtual Environment on Linux**\n",
    "\n",
    "* Open the Terminal by pressing `Ctrl + Alt + T`.\n",
    "* Navigate to the directory where you want to create the virtual environment.\n",
    "* Install `virtualenv` by running the following command in the terminal `sudo apt-get install python3-virtualenv`\n",
    "* Create a new virtual environment by running the following command `virtualenv -p python3 env`\n",
    "* This will create a new directory called `env`, containing the virtual environment.\n",
    "* Activate the virtual environment by running the following command `source env/bin/activate`\n",
    "* You can now install packages and work on your project within this virtual environment.\n",
    "* To deactivate the virtual environment, simply run the following command `deactivate`\n",
    "\n",
    "#### **Installing Jupyter with pip**\n",
    "If pip is installed on your local machine, you can install Jupyter. \n",
    "\n",
    "Here are the steps:\n",
    "* Open the command prompt (Windows) or terminal (Mac/Linux).\n",
    "* Install Jupyter with pip by running the following command `pip install jupyter`\n",
    "* Launch Jupyter Notebook by running the following command `jupyter notebook`\n",
    "\n",
    "#### **Installing Jupyter with Conda**\n",
    "\n",
    "* Download and install Anaconda from the official website: https://www.anaconda.com/products/individual.\n",
    "* Open the Anaconda Navigator App and launch Jupyter Notebook \n",
    "* Running IPYNB in Jupyter Notebook\n",
    "* Open Terminal / Command Prompt and Navigate to the notebooks directory using cd\n",
    "* Launch Jupyter Notebook by running the following command jupyter notebook\n",
    "* This will open a browser window displaying the Jupyter interface.\n",
    "* Click on the IPYNB file you want to open.\n",
    "* To execute all the cells in the notebook, click on the \"Cell\" menu and choose \"Run all\".\n",
    "* Alternatively, you can execute each cell individually by clicking on the \"Play\" button next to the cell.\n",
    "\n",
    "\n",
    "\n",
    "# Executing the project via Modular Code\n",
    "* Install the dependencies using the command, navigate to the **Telecom Machine Learning Project to Predict Customer Churn** directory where the requirements.txt file exists, and run ***pip install -r requirements.txt*** in the terminal/CMD.\n",
    "\n",
    "* Navigate to the src folder in the project directory using the cd command \n",
    "* Run the command python Engine.py in Terminal or run the Engine.py file in either VScode or PyCharm.\n",
    "\n",
    "![churn_project_structure.png](data:image/png;base64,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)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UB7mcZqH9Hsf"
   },
   "source": [
    "## **Important Libraries**\n",
    " \n",
    "* **pandas**: pandas is a fast, powerful, flexible, and easy-to-use open-source data analysis and manipulation tool built on top of the Python programming language. Refer to [documentation](https://pandas.pydata.org/) for more information.\n",
    " \n",
    "* **NumPy**: The fundamental package for scientific computing with Python. Fast and versatile, the NumPy vectorization, indexing, and broadcasting concepts are the de-facto standards of array computing today. NumPy offers comprehensive mathematical functions, random number generators, linear algebra routines, Fourier transforms, and more. Refer to [documentation](https://numpy.org/) for more information. pandas and NumPy are together used for most of the data analysis and manipulation in Python.\n",
    "\n",
    "* **Matplotlib**: Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. Refer to [documentation](https://matplotlib.org/) for more information.\n",
    " \n",
    "* **seaborn**: Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics. Refer to [documentation](https://seaborn.pydata.org/) for more information.\n",
    "\n",
    "* **scikit-learn**: Simple and efficient tools for predictive data analysis\n",
    "accessible to everybody and reusable in various contexts.\n",
    "It is built on NumPy, SciPy, and matplotlib to support machine learning in Python. Refer to [documentation](https://scikit-learn.org/stable/) for more information.\n",
    "\n",
    "* **Warnings**:The warnings library provides a way to handle warnings that are generated during program execution. Warnings are typically issued when there is a potential issue with code, but the code still runs without errors. The warnings module provides a way to catch these warnings and handle them in a way that is appropriate for the program. This can be especially useful when developing and debugging code, as warnings can help identify potential issues before they become errors.\n",
    "\n",
    "* **sys**:The sys library provides access to some system-specific parameters and functions. This library can be used to access system-level information, such as the command line arguments passed to the program, the version of the Python interpreter being used, and more. \n",
    "\n",
    "* **xgboost**: xgboost is an open-source machine learning library designed to be highly efficient, scalable, and portable. It is a gradient boosting algorithm that is used for supervised learning problems, including regression, classification, and ranking. Refer to [documentation](https://xgboost.readthedocs.io/en/stable/install.html) for more information.\n",
    "\n",
    "* **deepchecks**: deepchecks is an open-source library that is used for deep learning model debugging and monitoring. It provides a suite of tools for detecting and diagnosing common errors that can occur during the training and evaluation of deep learning models. Refer to [documentation](https://docs.deepchecks.com/stable/getting-started/welcome.html) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUdQ82V3M_-R"
   },
   "source": [
    "## **Package Requirements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2KU4tEjC15Jr"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8qAwTZ6tM-5k"
   },
   "outputs": [],
   "source": [
    "# !pip install numpy==1.24.2\n",
    "# !pip install pandas==1.5.3\n",
    "# !pip install matplotlib==3.7.0\n",
    "# !pip install seaborn==0.12.2\n",
    "# !pip install scikit_learn==1.2.2\n",
    "# !pip install xgboost==1.7.4\n",
    "# !pip install deepchecks==0.12.0\n",
    "# !pip install projectpro --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After package installation, kindly restart the runtime if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nbG5fh1i9Q3U"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-I2Xd4o4uKB"
   },
   "source": [
    "## **Data Reading from Different Sources**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ffu_ikbNanX"
   },
   "source": [
    "<details>\n",
    "    <summary>Click here to view more</summary>\n",
    "\n",
    "\n",
    "### **Files**\n",
    "\n",
    "In many cases, the data is stored in the local system. To read the data from the local system, specify the correct path and filename.\n",
    "\n",
    "### **CSV Format**\n",
    "Comma-separated values, also known as CSV, is a specific way to store data in a table structure format. The data used in this project is stored in a CSV file. Download the data for the project.\n",
    "\n",
    "\n",
    "Use following code to read data from csv file using pandas. \n",
    "```\n",
    "import pandas as pd\n",
    "csv_file_path= \"D:/ProjectPro/Telecom Machine Learning Project to Predict Customer Churn/data/Telecom_data.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "```\n",
    "With appropriate csv_file_path, pd.read_csv() function will read the data and store it in df variable.\n",
    " \n",
    "If you get *FileNotFoundError or No such file or directory*, try checking the path provided in the function. It's possible that python is not able to find the file or directory at a given location.\n",
    "\n",
    "\n",
    "### **Colab - CSV Format**\n",
    "\n",
    "```\n",
    "# mount the google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "csv_file_path= '/content/drive/MyDrive/project_pro/Telecom Machine Learning Project to Predict Customer Churn/Telecom_data.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "```\n",
    "\n",
    "### **AWS S3 - CSV**\n",
    "Use the S3 public link to read the CSV file directly into a pandas DataFrame\n",
    "```\n",
    "s3_link = 'https://s3.amazonaws.com/projex.dezyre.com/telecom-machine-learning-project-for-customer-churn/materials/Telecom_data.csv'\n",
    "df = pd.read_csv(s3_link)\n",
    "```\n",
    "\n",
    "### **Database**\n",
    "Most organizations store their data in databases such as MS SQL. Microsoft SQL Server (MS SQL) is a relational database management system developed by Microsoft. A BAK file in Microsoft SQL Server is a backup file that contains a copy of a SQL Server database at a specific point in time. It is essentially a binary representation of the database and includes all its data, tables, schema, indexes, stored procedures, and other objects.\n",
    "\n",
    "#### **Installing MS SQL Management Studio**\n",
    "To install Microsoft SQL Server Management Studio, you can follow these steps:\n",
    "\n",
    "* Go to the Microsoft SQL Server Downloads page (https://www.microsoft.com/en-us/sql-server/sql-server-downloads) and click on the \"Download now\" button for the version of SQL Server Management Studio that you want to install.\n",
    "* Follow the instructions on the screen to download the installation file to your computer.\n",
    "* Once the download is complete, locate the installation file and double-click on it to start the installation process.\n",
    "\n",
    "\n",
    "#### **Restore a BAK file in MS SQL**\n",
    "\n",
    "* Open SQL Server Management Studio and connect to the SQL Server instance to which you want to upload the BAK file.\n",
    "* Right-click on the Databases folder in the Object Explorer pane and select \"Restore Database...\" from the context menu.\n",
    "* In the \"Restore Database\" dialog box, select the \"Device\" option under the \"Source\" section.\n",
    "* Click on the \"...\" button to open the \"Select backup devices\" dialog box.\n",
    "In the \"Select backup devices\" dialog box, click on the \"Add\" button to add the BAK file that you want to upload.\n",
    "* In the \"Locate Backup File\" dialog box, browse to the location where the BAK file is stored in the Telecom Data Analysis Project to Improve Service Quality directory under the ‘data’ folder, select the file, and click on the \"OK\" button.\n",
    "* Back in the \"Select backup devices\" dialog box, the BAK file you added should now be listed under \"Backup media:\".\n",
    "* Click on the \"OK\" button to close the \"Select backup devices\" dialog box.\n",
    "In the \"Restore Database\" dialog box, you should see the BAK file listed in the \"Backup sets to restore\" section.\n",
    "* By default, the original database name and file locations from the BAK file will be used. If you want to restore the database with a different name or to a different location, you can modify the \"To database\" and \"Restore as\" options under the \"General\" section.\n",
    "* Click the \"Options\" tab for additional restore options.\n",
    "* If you want to overwrite an existing database with the same name, you can select the \"Overwrite the existing database (WITH REPLACE)\" option under the \"Restore options\" section.\n",
    "* Click on the \"OK\" button to start the restore process.\n",
    "* Once the restore process is complete, you should see a message indicating that the restore was successful.\n",
    "\n",
    "#### **Read Data from DB to Python**\n",
    " The data can be accessed by secret credentials, which will be in the following format.\n",
    "```\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "connection = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};\\\n",
    "                       SERVER=server_name;\\\n",
    "                       PORT=1433;\\\n",
    "                       DATABASE=database_name;\\\n",
    "                       UID=admin;\\\n",
    "                       PWD=password')\n",
    "```\n",
    "#### **Steps to install ODBC driver**\n",
    "* Go to the Microsoft Download Center page for the ODBC Driver 17 for SQL Server: https://www.microsoft.com/en-us/download/details.aspx?id=56567\n",
    "* Select the download button that corresponds to the operating system you are using.\n",
    "* Select the language you want to use for the installer, then click the download button.\n",
    "* Once the download is complete, run the installer.\n",
    "* Accept the license terms, then select the features you want to install.\n",
    "Choose a location to install the driver, or use the default location.\n",
    "* Complete the installation process by following the instructions provided by the installer.\n",
    "* Once the installation is complete, you can use the ODBC Driver 17 for SQL Server to connect to SQL Server databases from applications that support ODBC connectivity.\n",
    "\n",
    "#### **Query to read the data into Pandas**\n",
    "\n",
    "```\n",
    "\n",
    "query = '''select * from Processed_month_1_data\n",
    "           UNION ALL\n",
    "            select * from Processed_month_2_data\n",
    "            UNION ALL\n",
    "            select * from Processed_month_3_data\n",
    "            UNION ALL\n",
    "            select * from Processed_month_4_data\n",
    "            UNION ALL\n",
    "            select * from Processed_month_5_data\n",
    "            UNION ALL\n",
    "            select * from Processed_month_6_data\n",
    "            UNION ALL\n",
    "            select * from Processed_month_7_data\n",
    "            UNION ALL\n",
    "            select * from Processed_month_8_data\n",
    "            UNION ALL\n",
    "            select * from Processed_month_9_data\n",
    "            UNION ALL\n",
    "            select * from Processed_month_10_data\n",
    "            UNION ALL\n",
    "            select * from Processed_month_11_data\n",
    "            UNION ALL\n",
    "            select * from Processed_month_12_data\n",
    "            UNION ALL\n",
    "            select * from Processed_month_13_data\n",
    "            UNION ALL\n",
    "            select * from Processed_month_14_data'''\n",
    "processed_data = pd.read_sql(query,connection)\n",
    "processed_data.head()\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0cGA-Er4Z-2i"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5WNyymnhZ-2i"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-izhMtyrZ-2j"
   },
   "outputs": [
    {
     "ename": "IncompleteRead",
     "evalue": "IncompleteRead(141207025 bytes read, 163464798 more expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10652\\587380586.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Read data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://s3.amazonaws.com/projex.dezyre.com/telecom-machine-learning-project-for-customer-churn/materials/Telecom_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\Telco\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Telco\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 577\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Telco\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Telco\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"b\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1661\u001b[1;33m             self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Telco\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m     \u001b[1;31m# open URLs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m     ioargs = _get_filepath_or_buffer(\n\u001b[0m\u001b[0;32m    717\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Telco\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    371\u001b[0m                 \u001b[1;31m# Override compression based on Content-Encoding header\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m                 \u001b[0mcompression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"method\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"gzip\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m             \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m         return IOArgs(\n\u001b[0;32m    375\u001b[0m             \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Telco\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    478\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 480\u001b[1;33m                     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    481\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_close_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Telco\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mamt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 631\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIncompleteRead\u001b[0m: IncompleteRead(141207025 bytes read, 163464798 more expected)"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "df=pd.read_csv('https://s3.amazonaws.com/projex.dezyre.com/telecom-machine-learning-project-for-customer-churn/materials/Telecom_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBC_yfBlN3mc"
   },
   "source": [
    "## **Data Exploration**\n",
    "\n",
    "Data exploration is a critical step in the data analysis process, where you examine the dataset to gain a preliminary understanding of the data, detect patterns, and identify potential issues that may need further investigation. Data exploration is important because it helps to provide a solid foundation for subsequent data analysis tasks, hypothesis testing and data visualization.\n",
    "\n",
    "Data exploration is also important because it can help you to identify an appropriate approach for analyzing the data.\n",
    "\n",
    "Here are the various functions that help us explore and understand the data.\n",
    "\n",
    "* Shape: Shape is used to identify the dimensions of the dataset. It gives the number of rows and columns present in the dataset. Knowing the dimensions of the dataset is important to understand the amount of data available for analysis and to determine the feasibility of different methods of analysis.\n",
    "\n",
    "* Head: The head function is used to display the top five rows of the dataset. It helps us to understand the structure and organization of the dataset. This function gives an idea of what data is present in the dataset, what the column headers are, and how the data is organized.\n",
    "\n",
    "* Tail: The tail function is used to display the bottom five rows of the dataset. It provides the same information as the head function but for the bottom rows. The tail function is particularly useful when dealing with large datasets, as it can be time-consuming to scroll through all the rows.\n",
    "\n",
    "* Describe: The describe function provides a summary of the numerical columns in the dataset. It includes the count, mean, standard deviation, minimum, and maximum values, as well as the quartiles. It helps to understand the distribution of the data, the presence of any outliers, and potential issues that can affect the model's accuracy.\n",
    "\n",
    "* Isnull: The isnull function is used to identify missing values in the dataset. It returns a Boolean value for each cell, indicating whether it is null or not. This function is useful to identify the presence of missing data, which can be problematic for regression analysis.\n",
    "\n",
    "* Dropna: The dropna function is used to remove rows or columns with missing data. It is used to remove any observations or variables with missing data, which can lead to biased results in the regression analysis. The dropna function is used after identifying the missing data with the isnull function.\n",
    "\n",
    "* Columns: The .columns method is a built-in function that is used to display the column names of a pandas DataFrame or Series. It returns an array-like object that contains the names of the columns in the order in which they appear in the original DataFrame or Series. It can be used to obtain a quick overview of the variables in a dataset and their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Fqx0MsIZ-2j",
    "outputId": "14d978e0-75fa-4237-b17a-3c8d16c57c06"
   },
   "outputs": [],
   "source": [
    "# Check the shape of the Dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMm321OAZ-2k",
    "outputId": "254a55b1-2d79-4906-8ca8-78a837b0685c"
   },
   "outputs": [],
   "source": [
    "# Check the Information of the Dataframe, datatypes and non-null counts\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uyb0dMULZ-2k",
    "outputId": "b1062bd9-8e17-4fc6-c663-5fb75902c6f0"
   },
   "outputs": [],
   "source": [
    "# Checking the names of the columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXTBcfTrORWl"
   },
   "source": [
    "## **Data Dictionary**\n",
    "\n",
    "\n",
    "\n",
    "| Column name\t | Description|\n",
    "| ----- | ----- |\n",
    "| Customer ID\t | Unique identifier for each customer |\n",
    "| Month | Calendar Month- 1:12 | \n",
    "| Month of Joining |\tCalender Month -1:14, Month for which the data is captured|\n",
    "| zip_code |\tZip Code|\n",
    "|Gender |\tGender|\n",
    "| Age |\tAge(Years)|\n",
    "| Married |\tMarital Status |\n",
    "|Dependents | Dependents - Binary |\n",
    "| Number of Dependents |\tNumber of Dependents|\n",
    "|Location ID |\tLocation ID|\n",
    "|Service ID\t |Service ID|\n",
    "|state|\tState|\n",
    "|county\t|County|\n",
    "|timezone\t|Timezone|\n",
    "|area_codes|\tArea Code|\n",
    "|country\t|Country|\n",
    "|latitude|\tLatitude|\n",
    "|longitude\t|Longitude|\n",
    "|arpu|\tAverage revenue per user|\n",
    "|roam_ic\t|Roaming incoming calls in minutes|\n",
    "|roam_og\t|Roaming outgoing calls in minutes|\n",
    "|loc_og_t2t|\tLocal outgoing calls within same network in minutes|\n",
    "|loc_og_t2m\t|Local outgoing calls outside network in minutes(outside same + partner network)|\n",
    "|loc_og_t2f|\tLocal outgoing calls with Partner network in minutes|\n",
    "|loc_og_t2c\t|Local outgoing calls with Call Center in minutes|\n",
    "|std_og_t2t|\tSTD outgoing calls within same network in minutes|\n",
    "|std_og_t2m|\tSTD outgoing calls outside network in minutes(outside same + partner network)|\n",
    "|std_og_t2f|\tSTD outgoing calls with Partner network in minutes|\n",
    "|std_og_t2c\t|STD outgoing calls with Call Center in minutes|\n",
    "|isd_og|\tISD Outgoing calls|\n",
    "|spl_og\t|Special Outgoing calls|\n",
    "|og_others|\tOther Outgoing Calls|\n",
    "|loc_ic_t2t|\tLocal incoming calls within same network in minutes|\n",
    "|loc_ic_t2m|\tLocal incoming calls outside network in minutes(outside same + partner network)|\n",
    "|loc_ic_t2f\t|Local incoming calls with Partner network in minutes|\n",
    "|std_ic_t2t\t|STD incoming calls within same network in minutes|\n",
    "|std_ic_t2m\t|STD incoming calls outside network in minutes(outside same + partner network)|\n",
    "|std_ic_t2f|\tSTD incoming calls with Partner network in minutes|\n",
    "|std_ic_t2o|\tSTD incoming calls operators other networks in minutes|\n",
    "|spl_ic|\tSpecial Incoming calls in minutes|\n",
    "|isd_ic|\tISD Incoming calls in minutes|\n",
    "|ic_others|\tOther Incoming Calls|\n",
    "|total_rech_amt|\tTotal Recharge Amount in Local Currency|\n",
    "|total_rech_data|\tTotal Recharge Amount for Data in Local Currency\n",
    "|vol_4g|\t4G Internet Used in GB|\n",
    "|vol_5g|\t5G Internet used in GB|\n",
    "|arpu_5g|\tAverage revenue per user over 5G network|\n",
    "|arpu_4g|\tAverage revenue per user over 4G network|\n",
    "|night_pck_user|\tIs Night Pack User(Specific Scheme)|\n",
    "|fb_user|\tSocial Networking scheme|\n",
    "|aug_vbc_5g|\tVolume Based cost for 5G network (outside the scheme paid based on extra usage)|\n",
    "|offer|\tOffer Given to User|\n",
    "|Referred a Friend|\tReferred a Friend : Binary|\n",
    "|Number of Referrals|\tNumber of Referrals|\n",
    "|Phone Service|\tPhone Service: Binary|\n",
    "|Multiple Lines|\tMultiple Lines for phone service: Binary|\n",
    "|Internet Service|\tInternet Service: Binary|\n",
    "|Internet Type|\tInternet Type|\n",
    "|Streaming Data Consumption|\tStreaming Data Consumption|\n",
    "|Online Security|\tOnline Security|\n",
    "|Online Backup|\tOnline Backup|\n",
    "|Device Protection Plan|\tDevice Protection Plan|\n",
    "|Premium Tech Support|\tPremium Tech Support|\n",
    "|Streaming TV|\tStreaming TV|\n",
    "|Streaming Movies|\tStreaming Movies|\n",
    "|Streaming Music|\tStreaming Music|\n",
    "|Unlimited Data|\tUnlimited Data|\n",
    "|Payment Method|\tPayment Method|\n",
    "|Status ID|\tStatus ID|\n",
    "|Satisfaction Score|\tSatisfaction Score|\n",
    "|Churn Category|\tChurn Category|\n",
    "|Churn Reason|\tChurn Reason|\n",
    "|Customer Status|\tCustomer Status|\n",
    "|Churn Value|\tBinary Churn Value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYI9w9ZQZ-2k",
    "outputId": "c86cad26-af70-4576-a9dc-8a57e304bc82"
   },
   "outputs": [],
   "source": [
    "# Null values sum\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_P-B9TxjZ-2l",
    "outputId": "a1dca556-b0c6-4292-edb2-5ee1f7293325"
   },
   "outputs": [],
   "source": [
    "# Null values in total recharge data\n",
    "df['total_rech_data'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFPE07KUZ-2l",
    "outputId": "09b90153-2724-4faf-ffd7-d9540cf7de48"
   },
   "outputs": [],
   "source": [
    "# Null values in Internet Type\n",
    "df['Internet Type'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20Xrd0RHZ-2l",
    "outputId": "0d715b22-e433-4087-f308-49c741ac242e"
   },
   "outputs": [],
   "source": [
    "# Missing value percentage\n",
    "df['total_rech_data'].isna().sum()/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmWVmQQXZ-2l"
   },
   "source": [
    "**Observation:**\n",
    "\n",
    "*  These missing values may represent customers who have not recharged their account or have recharged but the information has not been recorded.\n",
    "\n",
    "* It is possible that customers with missing recharge data are those who received free data service, and therefore did not need to recharge their account. Alternatively, it is possible that the missing values are due to technical issues, such as data recording errors or system failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vvV6-BuZZ-2m",
    "outputId": "dedf9d49-79e2-42d3-d983-3f767b915de8"
   },
   "outputs": [],
   "source": [
    "# Checking the value counts of Internet Service where total recharge data was null\n",
    "df[df['total_rech_data'].isna()]['Internet Service'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR2RpuGKZ-2m"
   },
   "source": [
    "**Observation**:\n",
    "\n",
    "* It turns out that all customers with missing recharge data have opted for internet service, the next step could be to check if they have used it or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBgsjUoPZ-2m",
    "outputId": "aa6246b9-4066-4422-b758-25c283ef8546"
   },
   "outputs": [],
   "source": [
    "# Let's check unlimited data column\n",
    "df[(df['total_rech_data'].isna())]['Unlimited Data'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVj27H0xZ-2m",
    "outputId": "15ef3fdf-b711-499d-d52f-db9e2d51b477"
   },
   "outputs": [],
   "source": [
    "# Lets check Average Revenue for 4g and 5g\n",
    "df[(df['total_rech_data'].isna())][['arpu_4g','arpu_5g']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z--ECzlXZ-2m"
   },
   "source": [
    "**Observation**:\n",
    "\n",
    "* We can fill the missing values in the total_rech_data column with 0 when the arpu (Average Revenue Per User) is not applicable. This is because the arpu is a measure of the revenue generated per user, and if it is not applicable, it may indicate that the user is not generating any revenue for the company. In such cases, it is reasonable to assume that the total recharge amount is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HKsHAQQcZ-2n",
    "outputId": "8c982b59-e751-4ed1-8e8f-ca437e26f0af"
   },
   "outputs": [],
   "source": [
    "# Check the value counts of ARPU 4g and 5g\n",
    "df[['arpu_4g','arpu_5g']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHibU4lcZ-2n"
   },
   "outputs": [],
   "source": [
    "# Replacing all values of total recharge data= 0 where arpu 4g and 5g are not applicable\n",
    "df.loc[(df['arpu_4g']=='Not Applicable') | (df['arpu_5g']=='Not Applicable'),'total_rech_data']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ZywhbWfZ-2n",
    "outputId": "66d9b527-8b00-46ce-b36b-bd25ba1e8ded"
   },
   "outputs": [],
   "source": [
    "# Missing value percentage\n",
    "df['total_rech_data'].isna().sum()/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvS-GOYdZ-2n"
   },
   "source": [
    "We cannot fill other values with 0 because they have some ARPU to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xivTAuUZ-2n",
    "outputId": "db7a1b71-ca02-478f-eb06-751e1df76303"
   },
   "outputs": [],
   "source": [
    "# Calculate the mean of 'total_rech_data' where either 'arpu_4g' or 'arpu_5g' is not equal to 'Not Applicable'\n",
    "df.loc[(df['arpu_4g']!='Not Applicable') | (df['arpu_5g']!='Not Applicable'),'total_rech_data'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5NNUYX8Z-2n"
   },
   "source": [
    "With this mean, we will fill the NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePxg16ZkZ-2n"
   },
   "outputs": [],
   "source": [
    "# Fill NaN values in 'total_rech_data' with the mean of 'total_rech_data' where either 'arpu_4g' or 'arpu_5g' is not equal to 'Not Applicable'\n",
    "df['total_rech_data']=df['total_rech_data'].fillna(df.loc[(df['arpu_4g']!='Not Applicable') | (df['arpu_5g']!='Not Applicable'),'total_rech_data'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2y8-hQOgZ-2n",
    "outputId": "73731371-748c-4dd8-a9f0-dab769c9d4e0"
   },
   "outputs": [],
   "source": [
    "# Check the value counts for Internet Type\n",
    "df['Internet Type'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VojpiRlLZ-2o",
    "outputId": "423ee788-6a6a-447d-fa27-14efb280d6ce"
   },
   "outputs": [],
   "source": [
    "# Check value counts for Internet Service where Internet Type is null\n",
    "df[df['Internet Type'].isna()]['Internet Service'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDvmPBoeZ-2o"
   },
   "source": [
    "All null values in Internet Type does not have Internet Service. Let's fill these null values with Not Applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YjmE5bcbZ-2o"
   },
   "outputs": [],
   "source": [
    "# Filling Null values in Internet Type \n",
    "df['Internet Type']=df['Internet Type'].fillna('Not Applicable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BoezoPLWZ-2o",
    "outputId": "9efd88a7-4612-40ab-eaa0-24a7635e19ec"
   },
   "outputs": [],
   "source": [
    "# Shape of the dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjO5_qGIZ-2o"
   },
   "outputs": [],
   "source": [
    "# Insert a new column named 'total_recharge' before the last column in the dataframe \n",
    "# The values of 'total_recharge' are the sum of 'total_rech_amt' and 'total_rech_data'\n",
    "df.insert(loc=df.shape[1]-1,column='total_recharge',value=df['total_rech_amt']+df['total_rech_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkQYshrQZ-2o",
    "outputId": "4940c1b4-7a6a-407a-d9a3-b8e76325466f"
   },
   "outputs": [],
   "source": [
    "# Checking percent of missing values in columns\n",
    "df_missing_columns = (round(((df.isnull().sum()/len(df.index))*100),2).to_frame('null')).sort_values('null', ascending=False)\n",
    "df_missing_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHK6wY6OZ-2o"
   },
   "source": [
    "Let's drop some unnecessary columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRayCzDDZ-2o"
   },
   "outputs": [],
   "source": [
    "# Dropping columns\n",
    "df=df.drop(columns=['night_pck_user', 'fb_user','Churn Category','Churn Reason', 'Customer Status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCVlehSCZ-2o",
    "outputId": "1603cc56-12b2-45d4-f538-b6f7bd8c3d34"
   },
   "outputs": [],
   "source": [
    "# Checking churn %\n",
    "round(100*(df['Churn Value'].mean()),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3d99fGw_Z-2p",
    "outputId": "21c70fea-cdc9-41e0-be0b-e260bcd7244c"
   },
   "outputs": [],
   "source": [
    "# Number of unique latitudes\n",
    "df['latitude'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cHkBum8Z-2p",
    "outputId": "3575ab63-3f7b-4235-8e2f-79301e2a16de"
   },
   "outputs": [],
   "source": [
    "# Number of unique longitudes\n",
    "df['longitude'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Mp5w1niZ-2p"
   },
   "source": [
    "Replace 'Not Applicable' with 0 in both 'arpu_4g' and 'arpu_5g'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74bP-j3tZ-2p"
   },
   "outputs": [],
   "source": [
    "# Replace 'Not Applicable' with 0 in 'arpu_4g'\n",
    "df['arpu_4g'] = df['arpu_4g'].replace('Not Applicable', 0)\n",
    "\n",
    "# Replace 'Not Applicable' with 0 in 'arpu_5g'\n",
    "df['arpu_5g'] = df['arpu_5g'].replace('Not Applicable', 0)\n",
    "\n",
    "# Convert 'arpu_4g' to float data type\n",
    "df['arpu_4g'] = df['arpu_4g'].astype(float)\n",
    "\n",
    "# Convert 'arpu_5g' to float data type\n",
    "df['arpu_5g'] = df['arpu_5g'].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NypxsZ4JZ-2p",
    "outputId": "2ae94d49-c660-4465-d701-d51b5d0931fa"
   },
   "outputs": [],
   "source": [
    "# Check the data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HS3WicKZ-2p"
   },
   "outputs": [],
   "source": [
    "# Note: We are keeping customer location-based attributes aside for now\n",
    "location_att=['zip_code''state', 'county', 'timezone', 'area_codes', 'country','latitude','longitude']\n",
    "\n",
    "# List of categorical columns\n",
    "categorical_cols=['Gender',\n",
    "       'Married', 'Dependents',\n",
    "       'offer','Referred a Friend', 'Phone Service',\n",
    "       'Multiple Lines', 'Internet Service', 'Internet Type',\n",
    "        'Online Security', 'Online Backup',\n",
    "       'Device Protection Plan', 'Premium Tech Support', 'Streaming TV',\n",
    "       'Streaming Movies', 'Streaming Music', 'Unlimited Data',\n",
    "       'Payment Method']\n",
    "\n",
    "# List of continuous columns\n",
    "cts_cols=['Age','Number of Dependents',\n",
    "       'roam_ic', 'roam_og', 'loc_og_t2t',\n",
    "       'loc_og_t2m', 'loc_og_t2f', 'loc_og_t2c', 'std_og_t2t', 'std_og_t2m',\n",
    "       'std_og_t2f', 'std_og_t2c', 'isd_og', 'spl_og', 'og_others',\n",
    "       'loc_ic_t2t', 'loc_ic_t2m', 'loc_ic_t2f', 'std_ic_t2t', 'std_ic_t2m',\n",
    "       'std_ic_t2f', 'std_ic_t2o', 'spl_ic', 'isd_ic', 'ic_others',\n",
    "       'total_rech_amt', 'total_rech_data', 'vol_4g', 'vol_5g', 'arpu_5g',\n",
    "       'arpu_4g', 'arpu', 'aug_vbc_5g', 'Number of Referrals','Satisfaction Score',\n",
    "       'Streaming Data Consumption']   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfLU8h3Z15Jz"
   },
   "source": [
    "### **Outlier Detection**:\n",
    "\n",
    "Outlier detection is a critical data analysis technique that involves identifying and removing data points that are significantly different from the rest of the data. Outliers are data points that lie far away from the rest of the data, and they can significantly influence the statistical analysis and machine learning models' performance. Therefore, identifying and removing outliers is essential to ensure accurate and reliable data analysis results.\n",
    "\n",
    "There are two main approaches for outlier detection: parametric and non-parametric.\n",
    "\n",
    "* Parametric Methods:\n",
    "Parametric methods assume that the data follows a specific distribution, such as a normal distribution. In this approach, outliers are identified by calculating the distance of each data point from the mean of the distribution in terms of the number of standard deviations. Data points that are beyond a certain number of standard deviations (usually three or more) are considered as outliers.\n",
    "\n",
    "One common parametric method is the Z-score method, which calculates the distance of each data point from the mean in terms of standard deviations.\n",
    "Parametric methods can be useful when the data follows a known distribution, but they may not be effective when the data is not normally distributed.\n",
    "\n",
    "* Non-Parametric Methods:\n",
    "Non-parametric methods do not assume any specific distribution of the data. Instead, they rely on the rank or order of the data points. In this approach, outliers are identified by comparing the values of each data point with the values of other data points. Data points that are significantly different from other data points are considered as outliers.\n",
    "\n",
    "Quantiles are an important concept in non-parametric outlier detection methods. They represent values that divide a dataset into equal-sized parts, such as quarters or thirds. The most commonly used quantiles are the median (which divides the data into two equal parts), the first quartile (which divides the data into the lowest 25% and the highest 75%), and the third quartile (which divides the data into the lowest 75% and the highest 25%).\n",
    "\n",
    "The interquartile range (IQR) is another important concept related to quantiles. It is defined as the difference between the third and first quartiles and represents the middle 50% of the data. The IQR can be used to identify outliers by defining a range (known as the Tukey's fence) beyond which any data points are considered outliers.\n",
    "Non-parametric methods can be useful when the data is not normally distributed or when the distribution is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LqaJC1SzZ-2p"
   },
   "outputs": [],
   "source": [
    "# Create an empty dataframe with columns as cts_cols and index as quantiles\n",
    "quantile_df=pd.DataFrame(columns=cts_cols,index=[0.1,0.25,0.5,0.75,0.8,0.9,0.95,0.97,0.99])\n",
    "\n",
    "# for each column in cts_cols, calculate the corresponding quantiles and store them in the quantile_df\n",
    "for col in cts_cols:\n",
    "   quantile_df[col]=df[col].quantile([0.1,0.25,0.5,0.75,0.8,0.9,0.95,0.97,0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFRSmmv015Jz"
   },
   "source": [
    "By calculating quantiles for each continuous variable in the dataset, we are trying to get an idea about the spread and distribution of the data. Specifically, we are interested in identifying potential outliers in the data.\n",
    "\n",
    "Quantiles divide a distribution into equal proportions. For instance, the 0.25 quantile is the value below which 25% of the observations fall and the 0.75 quantile is the value below which 75% of the observations fall. By calculating quantiles at various levels, we can get a better understanding of the distribution of the data and identify any observations that are too far away from the rest of the data.\n",
    "\n",
    "These quantiles can be used as thresholds to identify potential outliers in the data. Observations with values beyond these thresholds can be considered as potential outliers and further investigation can be carried out to determine if they are true outliers or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fH6bWCegZ-2p",
    "outputId": "8ed6f768-73a7-4c99-c71c-fe22ca8abdd3"
   },
   "outputs": [],
   "source": [
    "# Let's check out the quantiles df\n",
    "quantile_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqnLtZs1Z-2p"
   },
   "source": [
    "Outliers were detected in the variables vol_5g, arpu_4g, and arpu_5g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6NQZsd3Z-2q",
    "outputId": "38bb54e5-ed40-4a05-f29b-655ae1544e25"
   },
   "outputs": [],
   "source": [
    "# Checking further\n",
    "df['arpu_4g'].quantile([0.75,0.8,0.9,0.95,0.97,0.99,0.999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bcx5-kVNZ-2q",
    "outputId": "025b06d2-46ae-45c2-e63b-40e810bb416e"
   },
   "outputs": [],
   "source": [
    "# Calculate the proportion of rows in the DataFrame where the value in the 'arpu_4g' column is equal to 254687\n",
    "df[df['arpu_4g']==254687].shape[0]/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJjw0lS5Z-2q",
    "outputId": "1db27add-bcf1-4a9c-d84d-a1c77fd25f44"
   },
   "outputs": [],
   "source": [
    "# Let's check it out\n",
    "df[df['arpu_4g']==254687]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzzuy2bvZ-2q"
   },
   "source": [
    "Let's see what is the value of 'total_rech_data' for these observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJ8kQmwSZ-2q",
    "outputId": "d3a243b3-42d2-493b-bb96-56205bcb39b5"
   },
   "outputs": [],
   "source": [
    "# Get the value counts of 'total_rech_data' for observations where the value in the 'arpu_4g' column is equal to 254687\n",
    "df[df['arpu_4g']==254687]['total_rech_data'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4c454tIZ-2q"
   },
   "source": [
    "Now, since the recharge amount is 0 and there is no ARPU, let's replace it with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akCqNl15Z-2q"
   },
   "outputs": [],
   "source": [
    "# Replace the outlier value 254687 in the 'arpu_4g' column of the dataframe 'df' with 0.\n",
    "df['arpu_4g']=df['arpu_4g'].replace(254687,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2m3EgcRZ-2q",
    "outputId": "3204f340-6a72-487b-9858-e5f427e386a0"
   },
   "outputs": [],
   "source": [
    "# Checking further\n",
    "df['arpu_4g'].quantile([0.75,0.8,0.9,0.95,0.97,0.99,0.999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Crbcd8PaZ-2q",
    "outputId": "46ab4a55-fca0-4856-b020-4aae00dd8f39"
   },
   "outputs": [],
   "source": [
    "# Filter by 'arpu_4g' value of 87978 and count unique values in 'total_rech_data' column\n",
    "df[df['arpu_4g']==87978]['total_rech_data'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9jocWPh15J1"
   },
   "source": [
    "All rows in the dataframe with an 'arpu_4g' value of 87978 have 0 value in the 'total_rech_data' column, indicating that these are likely outliers. Therefore, we have decided to replace the 'arpu_4g' value for these rows with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuOjiovGZ-2q"
   },
   "outputs": [],
   "source": [
    "# Replace the values with 0\n",
    "df['arpu_4g']=df['arpu_4g'].replace(87978,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7aizZWaZ-2q",
    "outputId": "df37019b-2fd1-4893-c1e0-90fe814f778e"
   },
   "outputs": [],
   "source": [
    "# Checking the quantiles again\n",
    "df['arpu_4g'].quantile([0.75,0.8,0.9,0.95,0.97,0.99,0.999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJp0H0BRZ-2r",
    "outputId": "c8944f7e-bad5-4350-d31c-1865a16518ce"
   },
   "outputs": [],
   "source": [
    "# Check the churn value for this ARPU\n",
    "df[df['arpu_4g']>8000]['Churn Value'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrZP-iq5Z-2r"
   },
   "source": [
    "**Observation:**\n",
    "\n",
    " * A higher ARPU suggests that a business is generating more revenue per user, which can be a positive sign for the business's profitability. However, a high ARPU can also imply churn, or the rate at which customers are leaving the business.\n",
    "\n",
    "* There are a few reasons why a high ARPU may imply churn. First, if a business is charging a high price for its services, it may attract a customer base that is more price-sensitive and likely to switch to a competitor if they find a better deal. This could result in a higher churn rate for the business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwB9svs2Z-2r",
    "outputId": "b3a34dfc-1ab0-45bd-e2a8-309ce4410416"
   },
   "outputs": [],
   "source": [
    "# Check the value counts of total recharge data at outlying values\n",
    "df[df['arpu_5g']==254687]['total_rech_data'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvnOLN9tZ-2r",
    "outputId": "90a82a8b-493d-4aa9-e99d-9ba7a700cf3f"
   },
   "outputs": [],
   "source": [
    "# Check the value counts of total recharge data at outlying values\n",
    "df[df['arpu_5g']==87978]['total_rech_data'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWUCA-0QZ-2r"
   },
   "outputs": [],
   "source": [
    "# Replacing the values with 0 where total recharge data is 0\n",
    "df['arpu_5g']=df['arpu_5g'].replace([87978,254687],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZ2G0_CgZ-2r",
    "outputId": "b3a9fe9c-cd7b-4380-8382-f24202b15506"
   },
   "outputs": [],
   "source": [
    "# Check the quantiles of ARPU 5G\n",
    "df['arpu_5g'].quantile([0.75,0.8,0.9,0.95,0.97,0.99,0.999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "288nXt4mZ-2r",
    "outputId": "f8c7d42f-d79d-4a2e-ce81-241662b06e73"
   },
   "outputs": [],
   "source": [
    "# Check the quantiles of Volume of 5G data\n",
    "df['vol_5g'].quantile([0.75,0.8,0.9,0.95,0.97,0.98,0.99,0.999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jk5WDhO4Z-2r",
    "outputId": "966f2b53-c1e2-4e62-b4ca-93f6e36514da"
   },
   "outputs": [],
   "source": [
    "# Lets see the recharge data value\n",
    "df[df['vol_5g']>=87978]['total_rech_data'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnX_gEMTZ-2r",
    "outputId": "8a09bdf3-d41b-433e-9526-a9b4bdbe13d7"
   },
   "outputs": [],
   "source": [
    "# Proportion of these values\n",
    "df[df['vol_5g']>=87978]['total_rech_data'].value_counts()/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GufyTVuZ-2s"
   },
   "source": [
    "**Observation**:\n",
    "\n",
    "There is a presence of 2% outliers in vol 5g, where the values are very high, but their total recharge data is 0. We will fill these outliers with 0, and below are some possible reasons why this could be:\n",
    "\n",
    "* Data recording error: It is possible that there was an error in recording the recharge data for these outliers, leading to an incorrect value of 0. In this case, it would make sense to fill the outliers with 0, as this is likely the correct value.\n",
    "\n",
    "* Promotions or bonuses: Another possibility is that these customers received promotions or bonuses that allowed them to use the service without recharging, leading to a total recharge data of 0. However, these customers may still be using the service heavily, leading to the high values in vol 5g. In this case, filling the outliers with 0 would make sense as it accurately reflects the lack of recharge data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnSUoP5cZ-2s"
   },
   "outputs": [],
   "source": [
    "# Replace the outlier values\n",
    "df['vol_5g']=df['vol_5g'].replace([87978,254687],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yc4VgnXjZ-2s",
    "outputId": "d31adc30-66e5-4ba9-e5b9-c8e00886eb66"
   },
   "outputs": [],
   "source": [
    "# Unique months\n",
    "df['Month'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyuox--FZ-2s",
    "outputId": "f46f2627-7512-4313-f160-ee23d12b5b19"
   },
   "outputs": [],
   "source": [
    "# Unique months of joining\n",
    "df['Month of Joining'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiZ5l8yUZ-2s"
   },
   "source": [
    "We will get 4 quarters in month of joining!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-iiHq7o15J3"
   },
   "outputs": [],
   "source": [
    "# # Save Processed data\n",
    "# df.to_csv('../data/processed/processed_churn_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CY0wR2W15J3"
   },
   "source": [
    "### **Quarterly Churn Analysis**\n",
    "\n",
    "Quarterly churn analysis is a process of analyzing the rate at which customers are leaving a business or discontinuing their services over a period of three months. The analysis is usually done by calculating the churn rate, which is the percentage of customers who have stopped using the service during the quarter.\n",
    "\n",
    "* Timeliness: Quarterly churn analysis and prediction allows for a timely assessment of customer retention and churn rates. By conducting this analysis regularly, businesses can identify any changes in customer behavior and take necessary actions to address them in a timely manner.\n",
    "\n",
    "* Evaluation of strategies: Conducting churn analysis and prediction on a quarterly basis enables businesses to evaluate the effectiveness of their customer retention strategies. If the churn rate has increased, the business can evaluate the strategies implemented in the previous quarter and determine whether they were effective or not. This will allow them to adjust their strategies and improve their customer retention efforts.\n",
    "\n",
    "* Financial impact: Churn has a significant financial impact on businesses. By conducting quarterly churn analysis and prediction, businesses can identify areas where they are losing revenue and take steps to prevent further losses. This will help them to maintain financial stability and growth.\n",
    "\n",
    "* Customer insights: Quarterly churn analysis and prediction can also provide valuable insights into customer behavior and preferences. By analyzing customer behavior and reasons for churn, businesses can identify patterns and trends that will help them to improve their services and retain customers in the future.\n",
    "\n",
    "* Benchmarking: Conducting quarterly churn analysis and prediction allows businesses to benchmark their performance against industry standards and competitors. This will help them to identify areas where they are performing well and areas where they need to improve to stay competitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bixkWnVAZ-2s"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Define a function to map a month to its corresponding quarter\n",
    "def map_month_to_quarter(month):\n",
    "    if math.isnan(month): # Handle NaN values if present\n",
    "        return None\n",
    "    quarter = math.ceil(month / 3)\n",
    "    return quarter\n",
    "\n",
    "# Insert a new column called 'Quarter of Joining' in the DataFrame 'df' and populate it with the quarter corresponding to the 'Month of Joining' column\n",
    "df.insert(loc=1,column='Quarter of Joining',value=df['Month of Joining'].apply(lambda x: map_month_to_quarter(x)))\n",
    "\n",
    "# Insert a new column called 'Quarter' in the DataFrame 'df'and populate it with the quarter corresponding to the 'Month' column\n",
    "df.insert(loc=1,column='Quarter',value= df['Month'].apply(lambda x: map_month_to_quarter(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwJBQcGLZ-2t"
   },
   "outputs": [],
   "source": [
    "# Remove duplicate rows in the DataFrame 'df' based on the 'Customer ID', 'Quarter', and 'Quarter of Joining' columns and keep only the last occurrence of each set of duplicates\n",
    "telco=df.drop_duplicates(subset=['Customer ID','Quarter','Quarter of Joining'],keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8GvS05j15J3"
   },
   "source": [
    "The 'train_data' DataFrame contains the data of customers who joined in the first quarter and were active in the first quarter. This dataset is used for training the churn prediction model.\n",
    "\n",
    "The 'test_data' DataFrame contains the data of customers who joined in the first quarter and were active in the second quarter. This dataset is used for testing the accuracy of the churn prediction model.\n",
    "\n",
    "The 'prediction_data' DataFrame contains the data of customers who joined in the second quarter and were active in the second quarter. This dataset is used for predicting the churn of customers who joined in the second quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-lrocBCZ-2t"
   },
   "outputs": [],
   "source": [
    "# Filter 1 and 2 quarter wise data\n",
    "train_data=telco[(telco['Quarter of Joining']==1)&(telco['Quarter']==1)]\n",
    "test_data=telco[(telco['Quarter of Joining']==1)&(telco['Quarter']==2)]\n",
    "prediction_data=telco[(telco['Quarter of Joining']==2)&(telco['Quarter']==2)]\n",
    "save_point(\"fcTel2\")\n",
    "#note that we have not used alot of data which we will use for feedback loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8RVRmNxZ-2t"
   },
   "source": [
    "### **Data Preprocessing and Leakage**\n",
    "\n",
    "Data leakage is a situation where information from the test or prediction data is inadvertently used during the training process of a machine learning model. This can occur when information from the test or prediction data is leaked into the training data, and the model uses this information to improve its performance during the training process.\n",
    "\n",
    "Data leakage can occur during the preprocessing phase of machine learning when information from the test or prediction data is used to preprocess the training data, inadvertently leaking information from the test or prediction data into the training data.\n",
    "\n",
    "For example, consider a scenario where the preprocessing step involves imputing missing values in the dataset. If the missing values are imputed using the mean or median values of the entire dataset, including the test and prediction data, then the imputed values in the training data may be influenced by the values in the test and prediction data. This can lead to data leakage, as the model may learn to recognize patterns in the test and prediction data during the training process, leading to overfitting and poor generalization performance.\n",
    "\n",
    "\n",
    "To avoid data leakage, it's important to perform the data preprocessing steps on the training data only, and then apply the same preprocessing steps to the test and prediction data separately. This ensures that the test and prediction data remain unseen by the model during the training process, and helps to prevent overfitting and improve the accuracy of the model.\n",
    "\n",
    "In the context of this problem, we performed all data preprocessing steps together for the sake of simplicity, which could potentially lead to data leakage. However, in real-world scenarios, it's important to treat the test and prediction data separately and apply the necessary preprocessing steps separately, based on the characteristics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8O8pPwBlZ-2t",
    "outputId": "9ae89c5f-53fc-4df1-facf-b46eeede3f9e"
   },
   "outputs": [],
   "source": [
    "# Unique counts of quarter and month of joining\n",
    "telco[['Quarter','Quarter of Joining']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2U_wBCfdZ-2t",
    "outputId": "c9e19662-78b2-41c4-c950-852bc3acef6e"
   },
   "outputs": [],
   "source": [
    "# Checking the shape of the data\n",
    "train_data.shape,test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeGRNSUDZ-2t",
    "outputId": "0d902fb8-33e8-49eb-f0cb-7f5086b7f0f6"
   },
   "outputs": [],
   "source": [
    "# Normalizing value counts and checking the churn rate in 1st quarter or the training data\n",
    "train_data['Churn Value'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pn1taM8KZ-2t",
    "outputId": "34dbb65c-ae76-42e3-d1d2-9f588feab3bd"
   },
   "outputs": [],
   "source": [
    "# List of columns in Train data\n",
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSt8tvLCZ-2t"
   },
   "outputs": [],
   "source": [
    "# Let's drop unnecessary columns\n",
    "drop_cols=['Customer ID', 'Quarter', 'Quarter of Joining', 'Month',\n",
    "       'Month of Joining', 'zip_code','Location ID', 'Service ID',\n",
    "       'state', 'county', 'timezone', 'area_codes', 'country', 'latitude',\n",
    "       'longitude','Status ID']\n",
    "\n",
    "train_data=train_data.drop(columns=drop_cols)\n",
    "test_data=test_data.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4s2wqOZOZ-2t",
    "outputId": "45e7cbb7-459f-44d7-9735-34ebb8e13e45"
   },
   "outputs": [],
   "source": [
    "# Columns\n",
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OiHvfZ-aZ-2u"
   },
   "outputs": [],
   "source": [
    "# Splitting the train data into features and label\n",
    "X_train=train_data[train_data.columns[:-1]]\n",
    "y_train=train_data[train_data.columns[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tV0KZAHSZ-2u"
   },
   "outputs": [],
   "source": [
    "# Splitting the test data into features and label\n",
    "X_test=test_data[test_data.columns[:-1]]\n",
    "y_test=test_data[test_data.columns[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWI8OHHwZ-2u",
    "outputId": "dc0b50fa-837b-4e11-e599-52e352b712e8"
   },
   "outputs": [],
   "source": [
    "# % churn value\n",
    "y_train.mean(),y_test.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OyZfNYj15J5"
   },
   "source": [
    "### **Transforming Variables**\n",
    "\n",
    "Transforming variables is an important step in the data preprocessing pipeline of machine learning, as it helps to convert the data into a format that is suitable for analysis and modeling. There are several ways to transform variables, depending on the type and nature of the data.\n",
    "\n",
    "Categorical variables, for example, are variables that take on discrete values from a finite set of categories, such as colors, gender, or occupation. One common way to transform categorical variables is through one-hot encoding. One-hot encoding involves creating a new binary variable for each category in the original variable, where the value is 1 if the observation belongs to that category and 0 otherwise. This approach is useful when the categories have no natural order or ranking.\n",
    "\n",
    "Another way to transform categorical variables is through label encoding. Label encoding involves assigning a unique integer value to each category in the variable. This approach is useful when the categories have a natural order or ranking, such as low, medium, and high.\n",
    "\n",
    "Numerical variables, on the other hand, are variables that take on continuous or discrete numerical values, such as age, income, or number of children. One common way to transform numerical variables is through standardization or normalization. Standardization involves scaling the variable to have a mean of 0 and a standard deviation of 1, while normalization involves scaling the variable to have values between 0 and 1.\n",
    "\n",
    "\n",
    "Normalization:\n",
    "\n",
    "$x_{norm} = \\frac{x - x_{min}}{x_{max} - x_{min}}$\n",
    "\n",
    "where $x$ is the original value, $x_{min}$ and $x_{max}$ are the minimum and maximum values in the range of $x$.\n",
    "\n",
    "Standardization:\n",
    "\n",
    "$x_{std} = \\frac{x - \\mu}{\\sigma}$\n",
    "\n",
    "where $x$ is the original value, $\\mu$ is the mean of the variable, and $\\sigma$ is the standard deviation of the variable. The standardized variable $x_{std}$ has a mean of 0 and a standard deviation of 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kg6yEz-pZ-2u"
   },
   "outputs": [],
   "source": [
    "# Importing Standard Scaler and One hot encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7sLu8xVZ-2u"
   },
   "outputs": [],
   "source": [
    "#fit encoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "# train\n",
    "encoder.fit(X_train[categorical_cols])\n",
    "encoded_features = list(encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "X_train[encoded_features] = encoder.transform(X_train[categorical_cols])\n",
    "# test\n",
    "X_test[encoded_features] = encoder.transform(X_test[categorical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLyqWrSX15J5"
   },
   "source": [
    "##### **Did you know - I**\n",
    "\n",
    "Notice that we fit the encoder on the training set, but only transform the test set. This ensures that only the categories found in the training set are one hot encoded (which prevents **data leakage** - when information outside the training set is used to build the model). \n",
    "\n",
    "A supervised algorithm's goal is to generate predictions about data that the algorithm has never seen before. As a result, while training an algorithm to predict validation and test splits, practitioners must ensure that the algorithm does not obtain access to knowledge about such splits inadvertently. By encoding the labels all at once before dividing, you indirectly indicate that you already know what are the possible classes or numeric ranges we are going to see in the future. Depending on the definition, this could be defined as data leaking, because you can deduce information that isn't in the training set.\n",
    "\n",
    "Notice we will use fit and transform on the training set and just transform on the test set which essentially means that the one hot encoder object is trained or fitted by seeing the values of just the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gr9hmgqZ-2u"
   },
   "outputs": [],
   "source": [
    "# Dump the encoder to use in transforming test data\n",
    "# dump(encoder, open('../data/output/encoder.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouPOkzzUZ-2u",
    "outputId": "7830e5db-25d6-40e6-a1ea-6b1686bcc187"
   },
   "outputs": [],
   "source": [
    "# Shape\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5oLmG73vZ-2u"
   },
   "outputs": [],
   "source": [
    "# drop original features\n",
    "X_train=X_train.drop(categorical_cols,axis=1)\n",
    "X_test=X_test.drop(categorical_cols,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iH1IbeEGZ-2u",
    "outputId": "54444f85-c9dd-4c0a-8b72-f69f65db130a"
   },
   "outputs": [],
   "source": [
    "# Check again\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZ3r_X7gZ-2v"
   },
   "outputs": [],
   "source": [
    "# Instantiate scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale Separate Columns\n",
    "# train\n",
    "X_train[cts_cols]  = scaler.fit_transform(X_train[cts_cols]) \n",
    "# test\n",
    "X_test[cts_cols]  = scaler.transform(X_test[cts_cols])\n",
    "preserve(\"fcTel2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_S5Rq7RZ-2v"
   },
   "outputs": [],
   "source": [
    "# Dump the scaler to use in transforming test data\n",
    "# dump(scaler, open('../data/output/scaler.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWNGtUnFO9Xb"
   },
   "source": [
    "## **Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjAY-cCpPi_S"
   },
   "source": [
    "### **Supervised Machine Learning**\n",
    "\n",
    "Supervised learning is a type of machine learning where the algorithm learns from labeled data. In other words, the data used to train the algorithm includes input variables and corresponding output variables. The algorithm learns to predict the output variable based on the input variables. Supervised learning can be further divided into two categories: regression and classification.\n",
    "\n",
    "* **Regression** is a type of supervised learning where the algorithm learns to predict a continuous output variable. In other words, the output variable is a numerical value. Examples of regression problems include predicting housing prices, stock prices, or the amount of rainfall in a particular area.\n",
    "\n",
    "* **Classification**, on the other hand, is a type of supervised learning where the algorithm learns to predict a discrete output variable. In other words, the output variable is a category or label. Examples of classification problems include predicting whether an email is spam or not, whether a tumor is malignant or benign, or whether a customer is likely to churn or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_z-jF_g9QPWu"
   },
   "source": [
    "### **Logistic Regression**\n",
    "\n",
    "\n",
    "\n",
    "Logistic regression is a type of machine learning algorithm used for classification problems where we need to predict if something belongs to one category or another. For example, we can use it to predict if a customer will churn or not.\n",
    "\n",
    "The algorithm works by analyzing the relationship between the input variables (such as customer demographics and usage patterns) and the binary output variable (such as churn or no churn). It then estimates the probability of the output variable using a logistic function, which outputs a value between 0 and 1.\n",
    "\n",
    "Logistic regression is actually a type of classification algorithm, but it is called \"logistic regression\" because it uses a logistic function to model the probability of the binary output variable.\n",
    "\n",
    "The term \"regression\" comes from the fact that the logistic regression model is based on a linear combination of the input variables and their associated weights, which is similar to linear regression. However, in linear regression, we predict a continuous output variable, while in logistic regression, we predict a probability of belonging to a particular class.\n",
    "\n",
    "In other words, logistic regression is a regression algorithm that is used for classification problems. The logistic function transforms the output of the regression equation into a probability value between 0 and 1, which can then be used to classify the input variable into one of two categories.\n",
    "\n",
    "Let's see how!!\n",
    "\n",
    "The logistic regression model is based on the logistic function, which maps any real-valued input to a value between 0 and 1. The logistic function is defined as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "sigmoid(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\end{equation}\n",
    "\n",
    "where $z$ is a linear combination of the input variables and their associated weights. In other words, we calculate $z$ as follows:\n",
    "\n",
    "$$z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3$$\n",
    "\n",
    "where $\\beta_0$ is the intercept term and $\\beta_1$, $\\beta_2$, and $\\beta_3$ are the weights associated with the input variables $x_1$, $x_2$, and $x_3$, respectively.\n",
    "\n",
    "The logistic regression model then predicts the probability of the binary outcome (in our example, whether a customer will churn or not) as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "P(y=1|x) = sigmoid(z)\n",
    "\\end{equation}\n",
    "\n",
    "where $y$ is the binary outcome, $x$ is the input variable vector, and $sigmoid(z)$ is the logistic function.\n",
    "\n",
    "To train the logistic regression model, we use a dataset of labeled examples. Each example includes a set of input variables and the corresponding binary outcome. The model is trained by minimizing the cross-entropy loss function, which is defined as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "L(y,\\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\n",
    "\\end{equation}\n",
    "\n",
    "where $y$ is the binary outcome, $\\hat{y}$ is the predicted probability, $N$ is the number of examples, and $\\log$ is the natural logarithm.\n",
    "\n",
    "To minimize the cross-entropy loss function, we use an optimization algorithm such as gradient descent. The gradient of the loss function with respect to each weight is computed using the chain rule of differentiation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial w_j} = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}i - y_i) x{ij}\n",
    "\\end{equation}\n",
    "\n",
    "where $x_{ij}$ is the $j$th input variable of the $i$th example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iD3z6y3ASLM4"
   },
   "source": [
    "### **Logistic Regression: Mathematical Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3eTRp4LFRicM"
   },
   "source": [
    "Suppose we have the following dataset with three input variables (customer age, monthly bill amount, and number of customer service calls) and a binary output variable (1 for churn and 0 for no churn):\n",
    "\n",
    "\n",
    "We can use logistic regression to build a model that predicts the probability of churn based on these input variables. The logistic function that we use is:\n",
    "\n",
    "$$P(y=1|x) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "where $y$ is the output variable (churn), $x$ is the input variable (age, monthly bill amount, and number of customer service calls), and $z$ is the linear combination of the input variables and their associated weights:\n",
    "\n",
    "$$z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3$$\n",
    "\n",
    "where $\\beta_0$ is the intercept term and $\\beta_1$, $\\beta_2$, and $\\beta_3$ are the weights associated with the input variables $x_1$, $x_2$, and $x_3$, respectively.\n",
    "\n",
    "To train the model, we start with some initial values for the weights and use a training algorithm to adjust the weights iteratively until we minimize the error between the predicted probability and the actual output. The training algorithm typically uses a gradient descent approach to update the weights in the direction that minimizes the loss function.\n",
    "\n",
    "Once the model is trained, we can use it to predict the probability of churn for a new customer. For example, suppose we want to predict the probability of churn for a customer who is 40 years old, has a monthly bill amount of 180, and has made 2 customer service calls. Using the logistic function and the weights learned during training, we can calculate the probability as:\n",
    "\n",
    "$$P(y=1|x) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3)}}$$\n",
    "\n",
    "Let's say the weights learned during training are $\\beta_0 = -2$, $\\beta_1 = 0.05$, $\\beta_2 = 0.01$, and $\\beta_3 = 0.8$. Then we can plug in the values for the new customer and get:\n",
    "\n",
    "$$P(y=1|x) = \\frac{1}{1+e^{-(-2 + 0.05\\times 40 + 0.01\\times 180 + 0.8\\times 2)}} \\approx 0.69$$\n",
    "\n",
    "So, the model predicts that there is a 69% probability that this customer will churn. We can use this probability to classify the customer as churn or no churn, depending on a threshold that we set (e.g., if the probability is above 0.5, we classify the customer as churn).\n",
    "\n",
    "This is a simple example, but it illustrates how logistic regression uses the logistic function and linear combination of input variables to predict the probability of a binary output variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ojrZMI6XO8O"
   },
   "source": [
    "## **Decision Trees**\n",
    "\n",
    "**Decision Trees in Classification**\n",
    "\n",
    "Decision trees are a type of supervised learning algorithm that can be used for classification as well as regression problems. They are widely used in machine learning because they are easy to understand and interpret, and can handle both categorical and numerical data. The idea behind decision trees is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "\n",
    "The decision tree starts with a single node, called the root node, which represents the entire dataset. The root node is then split into several child nodes based on the value of a chosen feature. The process of selecting the best feature and splitting the nodes is repeated recursively for each child node until a stopping criterion is reached. This results in a tree-like structure that represents the decision rules learned from the data.\n",
    "\n",
    "Each node in the decision tree represents a decision or a test of a feature value, and each branch represents the possible outcomes of that decision. The leaves of the tree represent the final decision or the class label assigned to the input data.\n",
    "\n",
    "**Splitting Criteria**\n",
    "\n",
    "To build a decision tree, we need a measure that determines how to split the data at each node. The splitting criterion is chosen based on the type of data and the nature of the problem. The most common splitting criteria are:\n",
    "\n",
    "* Gini index: measures the impurity of a set of labels. It calculates the probability of misclassifying a randomly chosen element from the set, and is used to minimize misclassification errors.\n",
    "* Information gain: measures the reduction in entropy (uncertainty) after a split. It is used to maximize the information gain in each split.\n",
    "* Chi-square: measures the difference between observed and expected frequencies of the classes. It is used to minimize the deviation between the observed and expected class distribution.\n",
    "\n",
    "**Overfitting in Decision Trees**\n",
    "\n",
    "One of the main challenges in building decision trees is overfitting. Overfitting occurs when the tree is too complex and fits the training data too well, resulting in poor performance on new and unseen data. This can be addressed by pruning the tree or limiting its depth, or by using ensemble methods such as bagging and boosting.\n",
    "\n",
    "**Ensemble Methods**\n",
    "\n",
    "Ensemble methods are techniques that combine multiple models to improve performance and reduce overfitting. The two most common ensemble methods used with decision trees are:\n",
    "\n",
    "* Bagging (Bootstrap Aggregating): involves training multiple decision trees on different subsets of the training data and then combining their predictions by averaging or voting. This reduces the variance and improves the stability of the model.\n",
    "* Boosting: involves training multiple decision trees sequentially, where each subsequent tree focuses on the misclassified examples of the previous tree. This reduces the bias and improves the accuracy of the model.\n",
    "\n",
    "Decision trees are powerful tools for classification problems that provide a clear and interpretable representation of the decision rules learned from the data. The choice of splitting criterion, stopping criterion, and ensemble method can have a significant impact on the performance and generalization of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wddhu8xMYRA5"
   },
   "source": [
    "### **Bagging**\n",
    "\n",
    "\n",
    "\n",
    "Bagging is an ensemble learning technique that aims to decrease the variance of a single estimator by combining the predictions from multiple learners. The basic idea behind bagging is to generate multiple versions of the training dataset through random sampling with replacement, and then train a separate classifier for each sampled dataset. The predictions from these individual classifiers are then combined using averaging or voting to obtain a final prediction.\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "Suppose we have a training set D of size n, and we want to train a classifier using bagging. Here are the steps involved:\n",
    "\n",
    "* Create k different bootstrap samples from D, each of size n.\n",
    "* Train a classifier on each bootstrap sample.\n",
    "* When making predictions on a new data point, take the average or majority vote of the predictions from each of the k classifiers.\n",
    "\n",
    "\n",
    "**Mathematical Explanation:**\n",
    "\n",
    "Suppose we have a binary classification problem with classes -1 and 1. Let's also assume that we have a training set D of size n, and we want to train a decision tree classifier using bagging.\n",
    "\n",
    "**Bootstrap Sample**: For each of the k classifiers, we create a bootstrap sample of size n by sampling with replacement from D. This means that each bootstrap sample may contain duplicates of some instances and may also miss some instances from the original dataset. Let's denote the i-th bootstrap sample as D_i.\n",
    "\n",
    "**Train a Classifier**: We train a decision tree classifier T_i on each bootstrap sample D_i. This gives us k classifiers T_1, T_2, ..., T_k.\n",
    "\n",
    "**Combine Predictions**: To make a prediction on a new data point x, we take the majority vote of the predictions from each of the k classifiers. \n",
    "\n",
    "The idea behind bagging is that the variance of the prediction error decreases as k increases. This is because each classifier has a chance to explore a different part of the feature space due to the random sampling with replacement, and the final prediction is a combination of these diverse classifiers.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ut9Sa3NrZWqi"
   },
   "source": [
    "### **Random Forest**\n",
    "\n",
    "\n",
    "\n",
    "Random Forest is an ensemble learning algorithm that builds a large number of decision trees and combines them to make a final prediction. It is a type of bagging method, where multiple decision trees are trained on random subsets of the training data and features. The algorithm then averages the predictions of these individual trees to produce a final prediction. Random Forest is particularly useful for handling high-dimensional data and for avoiding overfitting.\n",
    "\n",
    "**Algorithm of Random Forest**\n",
    "\n",
    "The algorithm of Random Forest can be summarized in the following steps:\n",
    "\n",
    "* Start by randomly selecting a subset of the training data, with replacement. This subset is called the bootstrap sample.\n",
    "\n",
    "* Next, randomly select a subset of features from the full feature set.\n",
    "\n",
    "* Build a decision tree using the bootstrap sample and the selected subset of features. At each node of the tree, select the best feature and split the data based on the selected feature.\n",
    "\n",
    "* Repeat steps 1-3 to build multiple trees.\n",
    "\n",
    "* Finally, combine the predictions of all trees to make a final prediction. For classification, this is usually done by taking a majority vote of the predicted classes. For regression, this is usually done by taking the average of the predicted values.\n",
    "\n",
    "\n",
    "**Mathematics Behind Random Forest**\n",
    "\n",
    "The mathematics behind Random Forest involves the use of decision trees and the bootstrap sampling technique. Decision trees are constructed using a recursive binary partitioning algorithm that splits the data based on the values of the selected features. At each node, the algorithm chooses the feature and the split point that maximizes the information gain. Information gain measures the reduction in entropy or impurity of the target variable after the split. The goal is to minimize the impurity of the subsets after each split.\n",
    "\n",
    "Bootstrap sampling is a statistical technique that involves randomly sampling the data with replacement to create multiple subsets. These subsets are used to train individual decision trees. By using bootstrap samples, the algorithm can generate multiple versions of the same dataset with slightly different distributions. This introduces randomness into the training process, which helps to reduce overfitting.\n",
    "\n",
    "\n",
    "\n",
    "**Difference between Bagging and Random Forest**\n",
    "\n",
    "Bagging and Random Forest are both ensemble learning algorithms that involve training multiple models on random subsets of the data. The main difference between the two is the way the individual models are trained.\n",
    "\n",
    "Bagging involves training multiple models using the bootstrap sampling technique, but each model uses the same set of features. This can lead to correlated predictions, which reduces the variance but not necessarily the bias of the model.\n",
    "\n",
    "Random Forest, on the other hand, involves training multiple models using the bootstrap sampling technique, but each model uses a randomly selected subset of features. This introduces additional randomness into the model and helps to reduce the correlation between individual predictions. Random Forest can achieve better performance than Bagging, especially when dealing with high-dimensional data or noisy features. In simpler terms it uses subsets of observations as well as features.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F27Y1y-YbNJA"
   },
   "source": [
    "### **Boosting**\n",
    "\n",
    "Boosting is a machine learning algorithm that works by combining several weak models (also known as base learners) into a strong model. The goal of boosting is to reduce the bias and variance of the base learners by iteratively adding new models to the ensemble that focus on correcting the errors made by the previous models. In other words, the boosting algorithm tries to learn from the mistakes of the previous models and improve the overall accuracy of the ensemble.\n",
    "\n",
    "Boosting works by assigning higher weights to the data points that the previous models misclassified, and lower weights to the ones that were classified correctly. This ensures that the new model focuses more on the difficult data points that the previous models struggled with, and less on the ones that were already well-classified. As a result, the new model is more specialized and can improve the accuracy of the ensemble.\n",
    "\n",
    "There are several types of boosting algorithms, including AdaBoost (Adaptive Boosting), Gradient Boosting, and XGBoost (Extreme Gradient Boosting). Each of these algorithms has its own approach to assigning weights to the data points and building the new models, but they all share the fundamental idea of iteratively improving the accuracy of the ensemble by combining weak models into a strong one. Boosting is a powerful algorithm that has been shown to achieve state-of-the-art results in many machine learning tasks, such as image classification, natural language processing, and recommender systems.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Difference between Bagging and Boosting**\n",
    "\n",
    "\n",
    "It's important to remember that boosting is a generic method, not a specific model, in order to comprehend it. Boosting involves specifying a weak model, such as regression or decision trees, and then improving it. In Ensemble Learning, the primary difference between Bagging and Boosting is that in bagging, weak learners are trained in simultaneously, but in boosting, they are trained sequentially. This means that each new model iteration increases the weights of the prior model's misclassified data. This redistribution of weights aids the algorithm in determining which parameters it should focus on in order to increase its performance.\n",
    "\n",
    "Both the Ensemble techniques are used in a different way as well.  Bagging methods, for example, are often used on poor learners who have large variance and low bias such as decision trees because they tend to overfit, whereas boosting methods are employed when there is low variance and high bias. While bagging can help prevent overfitting, boosting methods are more vulnerable to it because of a simple fact they continue to build on weak learners and continue to minimize error. This can lead to overfitting on the training data but specifying a decent number of models to be generated or hyperparameter tuning,  regularization can help in this case, if overfitting encountered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5QhbpJ3iIOI"
   },
   "source": [
    "### **Gradient Boosting**\n",
    "\n",
    "The primary idea behind this technique is to develop models in a sequential manner, with each model attempting to reduce the mistakes of the previous model.The additive model, loss function, and a weak learner are the three fundamental components of Gradient Boosting.\n",
    "\n",
    "The method provides a direct interpretation of boosting in terms of numerical optimization of the loss function using Gradient Descent. We employ Gradient Boosting Regressor when the target column is continuous, and Gradient Boosting Classifier when the task is a classification problem. The \"Loss function\" is the only difference between the two. The goal is to use gradient descent to reduce this loss function by adding weak learners. Because it is based on loss functions, for regression problems, Mean squared error (MSE) will be used, and  for classification problems, log-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lS5fSN6SjEQ1"
   },
   "source": [
    "### **XG Boost**\n",
    "\n",
    "\n",
    "XGBoost is a variant of gradient boosting, which is a popular ensemble learning technique that works by iteratively adding new models to an ensemble, each model attempting to correct the errors made by the previous models. In each iteration, the algorithm calculates the negative gradient of the loss function with respect to the current prediction, and fits a new model to the residual errors. The new model is then added to the ensemble, and the algorithm repeats this process until the desired number of models is reached.\n",
    "\n",
    "In XGBoost, the objective function is used to measure the difference between the predicted values and the true labels. The objective function is a sum of the loss function and the regularization term, where the latter prevents overfitting and encourages the model to be simple.\n",
    "\n",
    "\n",
    "\n",
    "Suppose we have a dataset with three features, x1, x2, and x3, and we want to predict a binary outcome, y. We decide to use decision trees as our weak learners. We start by training a decision tree on the entire dataset. However, this decision tree may not be able to capture the complex relationships between the features and the outcome, and it may be overfitting the training data.\n",
    "\n",
    "To improve upon the first decision tree, we can use XGBoost. Here's how:\n",
    "\n",
    "* Initialize the model: We start by initializing the XGBoost model with default hyperparameters. This model will be a simple decision tree with a single split.\n",
    "\n",
    "* Make predictions: We use this model to make predictions on the training data. We compare these predictions to the true labels and calculate the residuals, which are the differences between the predicted values and the true labels.\n",
    "\n",
    "* Fit a new tree: We then fit a new decision tree to the residuals. This tree will be a weak learner, as it is only modeling the errors of the previous model.\n",
    "\n",
    "* Combine the models: We add the new tree to the previous model to create a new ensemble. This new ensemble consists of the previous model plus the new tree.\n",
    "\n",
    "* Repeat: We repeat steps 2-4 for a specified number of iterations, adding a new tree to the ensemble each time.\n",
    "\n",
    "* Predictions: To make predictions on new data, we combine the predictions of all the trees in the ensemble.\n",
    "\n",
    "The key idea behind XGBoost is that it improves upon the predictions of the weak learners by focusing on the misclassified data points. By fitting a new tree to the residuals, XGBoost can correct the errors of the previous model and improve its overall accuracy. Additionally, XGBoost uses regularization to prevent overfitting and to improve generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqg9v6lo15J7"
   },
   "source": [
    "### **Classification Evaluation Metrics**\n",
    "\n",
    "Classification evaluation metrics are used to evaluate the performance of a machine learning model that is trained for classification tasks. Some of the commonly used classification evaluation metrics are F1 score, recall score, confusion matrix, and ROC AUC score. Here's an overview of each of these metrics:\n",
    "\n",
    "**F1 score**: The F1 score is a metric that combines the precision and recall of a model into a single value. It is calculated as the harmonic mean of precision and recall, and is expressed as a value between 0 and 1, where 1 indicates perfect precision and recall. \n",
    "F1 score is the harmonic mean of precision and recall. It is calculated as follows:\n",
    "$$ F1 = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}} $$\n",
    "where precision is the number of true positives divided by the sum of true positives and false positives, and recall is the number of true positives divided by the sum of true positives and false negatives.\n",
    "\n",
    "**Recall**: Use the recall score when the cost of false negatives (i.e., missing instances of a class) is high. For example, in a medical diagnosis problem, the cost of missing a positive case may be high, so recall would be a more appropriate metric.\n",
    "Recall score (also known as sensitivity) is the number of true positives divided by the sum of true positives and false negatives. It is given by the following formula:\n",
    "$$ Recall = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "**Precision**: Precision is another important classification evaluation metric, which is defined as the ratio of true positives to the total predicted positives. It measures the accuracy of positive predictions made by the classifier, i.e., the proportion of positive identifications that were actually correct.\n",
    "The formula for precision is:\n",
    "$$ precision = \\frac{true\\ positive}{true\\ positive + false\\ positive} $$\n",
    "where true positive refers to the cases where the model correctly predicted the positive class, and false positive refers to the cases where the model incorrectly predicted the positive class.\n",
    "Precision is useful when the cost of false positives is high, such as in medical diagnosis or fraud detection, where a false positive can have serious consequences. In such cases, a higher precision indicates that the model is better at identifying true positives and minimizing false positives.\n",
    "\n",
    "**Confusion Matrix**:\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model. It compares the predicted labels with the true labels and counts the number of true positives, false positives, true negatives, and false negatives. Here is an example of a confusion matrix:\n",
    "\n",
    "|          | Actual Positive | Actual Negative |\n",
    "|----------|----------------|----------------|\n",
    "| Predicted Positive | True Positive (TP) | False Positive (FP) |\n",
    "| Predicted Negative | False Negative (FN) | True Negative (TN) |\n",
    "\n",
    "​\n",
    " \n",
    "\n",
    "\n",
    "**ROC AUC Score**:\n",
    "ROC AUC (Receiver Operating Characteristic Area Under the Curve) score is a measure of how well a classifier is able to distinguish between positive and negative classes. It is calculated as the area under the ROC curve. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. TPR is the number of true positives divided by the sum of true positives and false negatives, and FPR is the number of false positives divided by the sum of false positives and true negatives.\n",
    "$$ ROC\\ AUC\\ Score = \\int_0^1 TPR(FPR^{-1}(t)) dt $$\n",
    "where $FPR^{-1}$ is the inverse of the FPR function.\n",
    "\n",
    "**When to use which**:\n",
    "\n",
    "The choice of evaluation metric depends on the specific requirements of the business problem. Here are some general guidelines:\n",
    "\n",
    "* F1 score: Use the F1 score when the class distribution is imbalanced, and when both precision and recall are equally important.\n",
    "\n",
    "* Recall score: Use the recall score when the cost of false negatives (i.e., missing instances of a class) is high. For example, in a medical diagnosis problem, the cost of missing a positive case may be high, so recall would be a more appropriate metric.\n",
    "\n",
    "* Precision: Precision is useful when the cost of false positives is high, such as in medical diagnosis or fraud detection, where a false positive can have serious consequences. In such cases, a higher precision indicates that the model is better at identifying true positives and minimizing false positives.\n",
    "\n",
    "* Confusion matrix: The confusion matrix is a versatile tool that can be used to visualize the performance of a model across different classes. It can be useful for identifying specific areas of the model that need improvement.\n",
    "\n",
    "* ROC AUC score: Use the ROC AUC score when the ability to distinguish between positive and negative classes is important. For example, in a credit scoring problem, the ability to distinguish between good and bad credit risks is crucial.\n",
    "\n",
    "Importance with respect to the business problem:\n",
    "\n",
    "The importance of each evaluation metric varies depending on the business problem. For example, in a spam detection problem, precision may be more important than recall, since false positives (i.e., classifying a non-spam email as spam) may annoy users, while false negatives (i.e., missing a spam email) may not be as harmful. On the other hand, in a disease diagnosis problem, recall may be more important than precision, since missing a positive case (i.e., a false negative) could have serious consequences. Therefore, it is important to choose the evaluation metric that is most relevant to the specific business problem at hand.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5t3L7dR15J8"
   },
   "source": [
    "### **Importance of Evaluation Metrics in Churn**\n",
    "\n",
    "Churn is a critical business problem for many companies because losing customers can have a significant impact on revenue and profitability. Therefore, it is essential to have accurate and reliable churn prediction models that can identify customers who are at risk of leaving.\n",
    "\n",
    "Evaluation metrics such as F1-score, recall, and ROC-AUC score provide insight into the performance of churn prediction models. By analyzing these metrics, businesses can determine how well their models are performing and make informed decisions about how to improve them. For example, if the F1-score is low, it may indicate that the model is not accurately identifying customers who are likely to churn. This may prompt businesses to re-evaluate their feature selection, hyperparameter tuning, or even their data collection processes.\n",
    "\n",
    "Moreover, evaluation metrics also help businesses make trade-offs between different prediction models. For example, in some cases, a company may prioritize recall over precision because they want to identify as many at-risk customers as possible, even if it means some false positives. On the other hand, in other cases, a company may prioritize precision over recall because they want to avoid incorrectly flagging customers as at-risk and taking unnecessary retention actions.\n",
    "\n",
    "Therefore, evaluation metrics play a crucial role in helping businesses optimize their churn prediction models to minimize churn and retain valuable customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nHsMiK115J8"
   },
   "source": [
    "#### **Did you know - II**\n",
    "\n",
    "Did you know that imbalanced data is a common problem in many real-world applications, including fraud detection, disease diagnosis, and churn prediction? In fact, in some cases, the minority class can be as small as 1% of the total dataset. \n",
    "\n",
    "Imbalanced data refers to a situation where the classes in a classification problem are not represented equally in the dataset. In other words, one class may have significantly fewer examples than the other class(es). For example, in a binary classification problem, the minority class may account for only 10% of the dataset, while the majority class accounts for 90%. This can make it challenging to build accurate machine learning models, as traditional algorithms tend to favor the majority class and may fail to capture the patterns in the minority class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XaCXWrHoZ-2v"
   },
   "outputs": [],
   "source": [
    "# Importing training libraries and evaluation metrics\n",
    "from sklearn.metrics import f1_score, recall_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObhvEqGv15J8"
   },
   "source": [
    "This function evaluate_models() is written to evaluate the performance of various classification models on a given dataset. The function takes a machine learning model, training and testing data as inputs and returns evaluation metrics such as F1 Score, Recall Score, Confusion Matrix and Area Under Curve (AUC) for both the training and testing datasets.\n",
    "\n",
    "The function is useful for comparing the performance of different classification models and selecting the one that performs the best for a given problem. The output of the function is stored in a pandas DataFrame for comparison and analysis purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAXhpTUEZ-2v"
   },
   "outputs": [],
   "source": [
    "# function modelling\n",
    "#Columns needed to compare metrics\n",
    "comparison_columns = ['Model_Name', 'Train_F1score', 'Train_Recall', 'Test_F1score', 'Test_Recall']\n",
    "\n",
    "comparison_df = pd.DataFrame()\n",
    "\n",
    "def evaluate_models(model_name, model_defined_var, X_train, y_train, X_test, y_test):\n",
    "  ''' This function predicts and evaluates various models for clasification'''\n",
    "  \n",
    "  # train predictions\n",
    "  y_train_pred = model_defined_var.predict(X_train)\n",
    "  # train performance\n",
    "  train_f1_score = f1_score(y_train,y_train_pred)\n",
    "  train_recall = recall_score(y_train, y_train_pred)\n",
    "\n",
    "  # test predictions\n",
    "  y_pred = model_defined_var.predict(X_test)\n",
    "  # test performance\n",
    "  test_f1_score = f1_score(y_test,y_pred)\n",
    "  test_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "  # Printing performance\n",
    "  print(\"Train Results\")\n",
    "  print(f'F1 Score: {train_f1_score}')\n",
    "  print(f'Recall Score: {train_recall}')\n",
    "  print(f'Confusion Matrix: \\n{confusion_matrix(y_train, y_train_pred)}')\n",
    "  print(f'Area Under Curve: {roc_auc_score(y_train, y_train_pred)}')\n",
    "\n",
    "  print(\" \")\n",
    "\n",
    "  print(\"Test Results\")\n",
    "  print(f'F1 Score: {test_f1_score}')\n",
    "  print(f'Recall Score: {test_recall}')\n",
    "  print(f'Confusion Matrix: \\n{confusion_matrix(y_test, y_pred)}')\n",
    "  print(f'Area Under Curve: {roc_auc_score(y_test, y_pred)}')\n",
    "\n",
    "  \n",
    "  #Saving our results\n",
    "  global comparison_columns\n",
    "\n",
    "  metric_scores = [model_name, train_f1_score, train_recall, test_f1_score, test_recall]\n",
    "  final_dict = dict(zip(comparison_columns,metric_scores))\n",
    "\n",
    "  return final_dict\n",
    "\n",
    "\n",
    "#function to create the comparison table\n",
    "final_list = []\n",
    "def add_dic_to_final_df(final_dict):\n",
    "  global final_list\n",
    "  final_list.append(final_dict)\n",
    "  global comparison_df\n",
    "  comparison_df = pd.DataFrame(final_list, columns= comparison_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7-ur5w915J8"
   },
   "source": [
    "The above code defines two functions for evaluating machine learning models for classification. The first function is evaluate_models() which takes a model name, a defined machine learning model variable, training and testing data, and evaluates the model's performance using the F1 score, recall score, confusion matrix, and area under the curve (AUC) score. It then prints the results of the model's performance on the training and testing datasets. Finally, it creates a dictionary of the evaluation metrics for the model and returns it.\n",
    "\n",
    "The second function is add_dic_to_final_df() which takes the dictionary returned from the evaluate_models() function and appends it to a list of all models evaluated. It then creates a pandas DataFrame from the list and returns it. The DataFrame contains the evaluation metrics for all the models evaluated so far, including the model name, training F1 score, training recall score, testing F1 score, and testing recall score.\n",
    "\n",
    "The comparison_columns variable is a list of the column names for the comparison_df DataFrame. It is used to ensure that the DataFrame columns are always in the correct order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mC-rzryIZ-2v",
    "outputId": "bdc40228-71ad-40f0-ec44-32badd58abfc"
   },
   "outputs": [],
   "source": [
    "# Churn in training data\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kULbYj1WZ-2v",
    "outputId": "8b6621e4-2d2a-436c-96ff-5429d14710fa"
   },
   "outputs": [],
   "source": [
    "# Churn in test data\n",
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJQRfdRO15J8"
   },
   "source": [
    "In machine learning models, when we encounter imbalanced classes where one class may have significantly fewer samples than another, models predict with a bias to the majority class. In such cases, using the class frequencies as weights can improve the performance of the model.\n",
    "\n",
    "For example, in a binary classification problem where we are trying to predict churn (i.e., customers who leave the service), the churn rate may be only 10% of the total customers. In this case, we can use the churn rate as weights to give more importance to the minority class during the model training process.\n",
    "\n",
    "To do this, we can calculate the churn rate (e.g., by dividing the number of churned customers by the total number of customers) and use it as a weight in the loss function during model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBTMjBgSZ-2v"
   },
   "outputs": [],
   "source": [
    "# Let's calculate the churn rate for data and store it as dict\n",
    "w=y_train.value_counts(normalize=True).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8l1pyBzZ-2v",
    "outputId": "22035971-3d60-4a09-ce75-3bf8a221e1c9"
   },
   "outputs": [],
   "source": [
    "# Weights\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zso3lJtgZ-2v",
    "outputId": "490d286d-b3f4-4433-d28d-4ac1bb1377b4"
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "lg2 = LogisticRegression(random_state=13, class_weight=w)\n",
    "# fit it\n",
    "lg2.fit(X_train,y_train)\n",
    "\n",
    "model_snapshot(\"fcTel2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32uRyLlRZ-2v",
    "outputId": "5895d80b-d068-437f-a84f-c27b2af6be1a"
   },
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "logistic_results = evaluate_models(\"Logistic Regression\", lg2, X_train, y_train, X_test, y_test)\n",
    "add_dic_to_final_df(logistic_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_x9tisdZ-2w",
    "outputId": "74c3ad02-43b1-4991-8b47-e36a01f217ba"
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "random_f = RandomForestClassifier(n_estimators=20, class_weight=w, random_state=7)\n",
    "random_f.fit(X_train, y_train)\n",
    "\n",
    "randomf_results = evaluate_models(\"Random Forest\", random_f, X_train, y_train, X_test, y_test)\n",
    "add_dic_to_final_df(randomf_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHIxg_Lq15J9"
   },
   "source": [
    "### **XG Boost Training**\n",
    "\n",
    "XGBoost is a popular gradient boosting library used for building supervised machine learning models. It is designed to be efficient, scalable, and flexible. XGBoost provides an efficient implementation of gradient boosting algorithms and is widely used in various machine learning competitions.\n",
    "\n",
    "In XGBoost, the D matrix is a data structure that stores the input data and provides efficient access to it during model training. The D matrix is essentially a wrapper around the input data, which is typically stored as a two-dimensional NumPy array or a Pandas DataFrame. The D matrix adds some additional features to the input data that make it easier to use with XGBoost.\n",
    "\n",
    "The D matrix provides a few benefits over using the raw input data directly. First, it allows for efficient access to the input data during model training, which is critical for large datasets. Second, it provides some additional functionality, such as the ability to handle missing values and to split the data into training and validation sets. Finally, it simplifies the process of passing data to the XGBoost model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0oToz7ZZ-2w",
    "outputId": "4f95287b-8c92-4ff0-9dae-d3d602b2f4a0"
   },
   "outputs": [],
   "source": [
    "# import xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Convert training and test sets to DMatrix\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Train initial model\n",
    "params = {'objective': 'multi:softmax', 'num_class': 2}\n",
    "num_rounds = 30\n",
    "xgbmodel = xgb.train(params, dtrain, num_rounds)\n",
    "model_snapshot(\"fcTel2\")\n",
    "\n",
    "\n",
    "xgb_results = evaluate_models(\"XGB\", xgbmodel, dtrain, y_train, dtest, y_test)\n",
    "add_dic_to_final_df(xgb_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujXnaqb4Z-2w",
    "outputId": "2d201a8c-58d8-4662-dfb3-3ec84743097c"
   },
   "outputs": [],
   "source": [
    "# Let's see the comparison df\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Yl8FyOwZ-2w"
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# xgbmodel.save_model('../models/xgb_base.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "572xabmveMsQ"
   },
   "source": [
    "## **Data Drift Monitoring**\n",
    "\n",
    "\n",
    "### **So Why Is Drift So Important?**\n",
    "\n",
    "Machine learning models are meant to predict on unseen data, based on previous known data. If the data or the relationships between features and the target label have changed, our model’s performance may degrade.\n",
    "\n",
    "Detecting drift is an important warning sign that our model may be not as accurate on newer data (compared to the training data), and that it should be adjusted or retrained on different data. In production environments, detecting drift (and other measures derived from drift, such as model confidence) is often the only way to know that our model performance is deteriorating, as in many cases the label is unknown for some time after the prediction is made.\n",
    "\n",
    "It is important to note that not all changes in data represent drift. For example, periodic changes in data due to daily, weekly or seasonal changes are usually not considered drift, as they are often present in the training data.\n",
    "\n",
    "![img.png](https://docs.deepchecks.com/stable/_images/patterns-of-concept-drift.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **Which Types of Drift Are There?**\n",
    "In machine learning, we usually refer to 2 types of drift:\n",
    "\n",
    "**Data Drift**\n",
    "\n",
    "Data drift is any change in the distribution of the data.\n",
    "\n",
    "For example, in a dataset predicting a person’s income, the target (income) is highly correlated with high level of education (advanced academic degrees). A government plan to help people of lower social-economic status to get higher education, would create a data drift that changes how the data distributes. However, this will not change the relation between a person’s salary and their level of education, as these new graduates will be able to work in better paying professions.\n",
    "\n",
    "**Concept Drift**\n",
    "\n",
    "Concept drift is a change in the underlying relation between the data and the label.\n",
    "\n",
    "Continuing the example of predicting income using the level of education, let’s assume that a change in the job market (for example, the rise of high-tech companies) caused drift in the data: suddenly, job experience became more significant for programming jobs than a degree in computer science. Now, the relation between the level of education and the income has changed - and a person’s salary can’t be predicted from their level of education as accurately as it was on previous data.\n",
    "\n",
    "Concept drift will almost always require some changes to the model, usually by retraining of the model on newer data.\n",
    "\n",
    "![img.png](https://docs.deepchecks.com/stable/_images/types-of-drift.png)\n",
    "\n",
    "\n",
    "Model Drift and Concept Drift are essentially the same thing. They both refer to the phenomenon where the underlying data generating process changes over time, leading to a degradation in the performance of a machine learning model.\n",
    "\n",
    "When a model is trained on a dataset and then deployed to make predictions in the real world, it assumes that the statistical properties of the data remain constant over time. However, this is often not the case in practice. For example, in the context of natural language processing, the meanings of words can shift over time, rendering a language model trained on historical data less effective on current data.\n",
    "\n",
    "When the statistical properties of the data change over time, it can lead to a situation where the model's predictions become less accurate, even if the model itself remains unchanged. This is what is known as model drift or concept drift. To mitigate this issue, techniques such as online learning or retraining the model on fresh data can be used.\n",
    "\n",
    "\n",
    "\n",
    "### **What Can You Do in Case of Drift?**\n",
    "\n",
    "![img.png](https://docs.deepchecks.com/stable/_images/deepchecks_label_drift.png)\n",
    "When suspecting drift in your data, you must first understand what changed in the data - were it the features, the labels, or maybe just the predictions. In deepchecks, we show a drift score for each feature, starting with your most important features, giving you an idea of the severity of your drift, even if you’re not still sure of its source.\n",
    "\n",
    "It is recommended to manually explore your data and try to understand the root cause of your changes, in order to estimate the effect of the change on your model’s performance.\n",
    "\n",
    "**Retrain Your Model**\n",
    "\n",
    "If you have either kind of drift, retraining your model on new data that better represents the current distribution, is the most straight-forward solution. However, this solution may require additional resources such as manual labeling of new data, or might not be possible if labels on the newer data are not available yet.\n",
    "\n",
    "Retraining is usually necessary in cases of concept drift. However, retraining may still be of use even for other cases, such as data drift that caused a change in the label’s distribution, but not in the ability to predict the label from the data. In this cas, retraining the model with the correct distribution of the label can improve the model’s performance (this is not relevant when the training dataset is sampled so labels are evenly distributed).\n",
    "\n",
    "\n",
    "Reference: [Deepchecks Documentation](https://docs.deepchecks.com/stable/user-guide/general/drift_guide.html#what-is-distribution-drift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EpX231PYZ-2w"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import traceback\n",
    "\n",
    "from deepchecks.tabular import Dataset\n",
    "from deepchecks.tabular import Suite\n",
    "from deepchecks.tabular.checks import WholeDatasetDrift, DataDuplicates, NewLabelTrainTest, TrainTestFeatureDrift, TrainTestLabelDrift\n",
    "from deepchecks.tabular.checks import FeatureLabelCorrelation, FeatureLabelCorrelationChange, ConflictingLabels, OutlierSampleDetection \n",
    "from deepchecks.tabular.checks import WeakSegmentsPerformance, RocReport, ConfusionMatrixReport, TrainTestPredictionDrift, CalibrationScore, BoostingOverfit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FsPZPOfpZ-2x"
   },
   "outputs": [],
   "source": [
    "# Define categorical and continuous columns\n",
    "pred_cat_cols=[\n",
    "       'Gender_Female', 'Gender_Male', 'Gender_Not Specified', 'Gender_Other',\n",
    "       'Married_No', 'Married_Not Specified', 'Married_Yes', 'Dependents_No',\n",
    "       'Dependents_Not Specified', 'Dependents_Yes', 'offer_A', 'offer_B',\n",
    "       'offer_C', 'offer_D', 'offer_E', 'offer_F', 'offer_G', 'offer_H',\n",
    "       'offer_I', 'offer_J', 'offer_No Offer', 'Referred a Friend_No',\n",
    "       'Referred a Friend_Yes', 'Phone Service_No', 'Phone Service_Yes',\n",
    "       'Multiple Lines_No', 'Multiple Lines_None', 'Multiple Lines_Yes',\n",
    "       'Internet Service_No', 'Internet Service_Yes', 'Internet Type_Cable',\n",
    "       'Internet Type_DSL', 'Internet Type_Fiber Optic', 'Internet Type_None',\n",
    "       'Internet Type_Not Applicable', 'Online Security_No',\n",
    "       'Online Security_Yes', 'Online Backup_No', 'Online Backup_Yes',\n",
    "       'Device Protection Plan_No', 'Device Protection Plan_Yes',\n",
    "       'Premium Tech Support_No', 'Premium Tech Support_Yes',\n",
    "       'Streaming TV_No', 'Streaming TV_Yes', 'Streaming Movies_No',\n",
    "       'Streaming Movies_Yes', 'Streaming Music_No', 'Streaming Music_Yes',\n",
    "       'Unlimited Data_No', 'Unlimited Data_None', 'Unlimited Data_Yes',\n",
    "       'Payment Method_Bank Withdrawal', 'Payment Method_Credit Card',\n",
    "       'Payment Method_Wallet Balance']\n",
    "\n",
    "pred_cts_cols=['Age', 'Number of Dependents', 'roam_ic', 'roam_og', 'loc_og_t2t',\n",
    "       'loc_og_t2m', 'loc_og_t2f', 'loc_og_t2c', 'std_og_t2t', 'std_og_t2m',\n",
    "       'std_og_t2f', 'std_og_t2c', 'isd_og', 'spl_og', 'og_others',\n",
    "       'loc_ic_t2t', 'loc_ic_t2m', 'loc_ic_t2f', 'std_ic_t2t', 'std_ic_t2m',\n",
    "       'std_ic_t2f', 'std_ic_t2o', 'spl_ic', 'isd_ic', 'ic_others',\n",
    "       'total_rech_amt', 'total_rech_data', 'vol_4g', 'vol_5g', 'arpu_5g',\n",
    "       'arpu_4g', 'arpu', 'aug_vbc_5g', 'Number of Referrals',\n",
    "       'Streaming Data Consumption', 'Satisfaction Score', 'total_recharge']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdfyIpeV15J-"
   },
   "source": [
    "The below code defines a function called check_data_drift() that checks for data drifts between two datasets, ref_df and cur_df, based on a set of predictors. The function uses the dataduit library to create two datasets, ref_dataset and cur_dataset, based on the reference and current dataframes, respectively. The features and cat_features parameters are set for each dataset based on the ref_features, cur_features, ref_cat_features, and cur_cat_features lists, which are generated based on the intersection of predictors and the columns of the two dataframes.\n",
    "\n",
    "The function then creates a suite object, which contains two tests for data drift: WholeDatasetDrift() and TrainTestFeatureDrift(). The WholeDatasetDrift() test checks for overall drift in the entire dataset, while the TrainTestFeatureDrift() test checks for drift in specific features between the reference and current datasets. The add_condition_overall_drift_value_less_than() and add_condition_drift_score_less_than() methods set the threshold for acceptable drift to 0.2 and 0.1, respectively.\n",
    "\n",
    "The suite is then run using the reference and current datasets as train_dataset and test_dataset, respectively, and the results are stored in an r object. If any checks did not run or did not pass, the retrain variable is set to True, indicating that the model may need to be retrained. Finally, the function saves the results of the data drift analysis as an HTML report in the Output directory with a filename based on the job_id parameter.\n",
    "\n",
    "The function returns a dictionary with two keys: report, which contains the r object with the results of the data drift analysis, and retrain, which is a boolean value indicating whether the model needs to be retrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSdh_6o2Z-2x"
   },
   "outputs": [],
   "source": [
    "def check_data_drift(ref_df:pd.DataFrame, cur_df:pd.DataFrame, predictors:list, job_id:str):\n",
    "    \"\"\"\n",
    "    Check for data drifts between two datasets and decide whether to retrain the model. \n",
    "    A report will be saved in the results directory.\n",
    "    :param ref_df: Reference dataset\n",
    "    :param cur_df: Current dataset\n",
    "    :param predictors: Predictors to check for drifts\n",
    "    :param target: Target variable to check for drifts\n",
    "    :param job_id: Job ID\n",
    "    :return: boolean\n",
    "    \"\"\"\n",
    "    ref_features = [col for col in predictors if col in ref_df.columns]\n",
    "    cur_features = [col for col in predictors if col in cur_df.columns]\n",
    "    ref_cat_features = [col for col in pred_cat_cols if col in ref_df.columns]\n",
    "    cur_cat_features = [col for col in pred_cat_cols if col in cur_df.columns]\n",
    "    ref_dataset = Dataset(ref_df,  features=ref_features, cat_features=ref_cat_features)\n",
    "    cur_dataset = Dataset(cur_df, features=cur_features, cat_features=cur_cat_features)\n",
    "    \n",
    "    suite = Suite(\"data drift\",\n",
    "        WholeDatasetDrift().add_condition_overall_drift_value_less_than(0.2), #0.2 \n",
    "        TrainTestFeatureDrift().add_condition_drift_score_less_than(0.2), #0.1   \n",
    "        )\n",
    "    r = suite.run(train_dataset=ref_dataset, test_dataset=cur_dataset)\n",
    "    retrain = (len(r.get_not_ran_checks())>0) or (len(r.get_not_passed_checks())>0)\n",
    "    \n",
    "    # try:\n",
    "    #     r.save_as_html(f\"../reports/{job_id}_data_drift_report.html\")\n",
    "    #     print(\"[INFO] Data drift report saved as {}\".format(f\"{job_id}_data_drift_report.html\"))\n",
    "    # except Exception as e:\n",
    "    #     print(f\"[WARNING][DRIFTS.check_DATA_DRIFT] {traceback.format_exc()}\")\n",
    "    return {\"report\": r, \"retrain\": retrain}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__Oh0gxVZ-2x"
   },
   "outputs": [],
   "source": [
    "# Defining the preprocessing steps for test data\n",
    "def preprocess_steps(data):\n",
    "    df=data.copy()\n",
    "    drop_cols=['Customer ID', 'Quarter', 'Quarter of Joining', 'Month',\n",
    "       'Month of Joining', 'zip_code','Location ID', 'Service ID',\n",
    "       'state', 'county', 'timezone', 'area_codes', 'country', 'latitude',\n",
    "       'longitude','Status ID']\n",
    "    df=df.drop(columns=drop_cols)\n",
    "    processed_data=df.copy()\n",
    "    processed_data[encoded_features] = encoder.transform(processed_data[categorical_cols])\n",
    "    processed_data=processed_data.drop(categorical_cols,axis=1)\n",
    "    processed_data[cts_cols]  = scaler.transform(processed_data[cts_cols]) \n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AdLiM35qZ-2x"
   },
   "outputs": [],
   "source": [
    "# Creating a copy of train for reference\n",
    "ref_check_data=X_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMnZm0MdZ-2y"
   },
   "source": [
    "### **Inference Pipeline with and without Label Availability**\n",
    "\n",
    "In real-world scenarios, we often need to deploy machine learning models to make predictions on new data. In order to make accurate predictions, the input data must be preprocessed and checked for any data drift.\n",
    "\n",
    "However, there are two cases of inference pipeline, depending on whether the label is available or not.\n",
    "\n",
    "Case 1: Label is not available\n",
    "In this case, we cannot check the model drift and retrain the model, as we do not have the true label values for the new data. Therefore, we need to use the previously trained model to make predictions on the new data.\n",
    "\n",
    "In this case, the inference pipeline consists of two steps:\n",
    "\n",
    "* Preprocessing the new data using the same preprocessing steps that were used for the training data.\n",
    "* Using the previously trained model to make predictions on the preprocessed new data.\n",
    "* Even though we cannot retrain the model in this case, we can still check for any data drift in the new data compared to the training data. This can help us identify if there are any significant changes in the data distribution that may affect the accuracy of the predictions.\n",
    "\n",
    "Case 2: Label is available\n",
    "In this case, we can not only preprocess the new data and check for any data drift, but we can also check for model drift and decide whether to retrain the model.\n",
    "\n",
    "In this case, the inference pipeline consists of three steps:\n",
    "\n",
    "* Preprocessing the new data using the same preprocessing steps that were used for the training data.\n",
    "* Checking for any data drift between the new data and the training data. If the data drift is significant, we may need to retrain the model.\n",
    "* Using the previously trained or retrained model to make predictions on the preprocessed new data.\n",
    "\n",
    "The reason why we check for data drift and model drift is that the performance of a machine learning model depends on the assumption that the training data and the new data are drawn from the same distribution. If there is significant data drift or model drift, then the model may not perform well on the new data, and we may need to retrain the model.\n",
    "\n",
    "The inference pipeline for machine learning models is a crucial step in deploying the models in real-world scenarios. By pre-processing the new data, checking for data drift and model drift, and retraining the model when necessary, we can ensure that the model is accurate and performs well on the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5W9uUrowZ-2x"
   },
   "outputs": [],
   "source": [
    "def inference_pipeline(inference_data,reference_data,job_id,predictors_cols):\n",
    "    #write data cleaning steps if necessary\n",
    "\n",
    "    #data preprocessing\n",
    "    clean_inf_data=preprocess_steps(inference_data)\n",
    "\n",
    "    #data drift\n",
    "    data_drift=check_data_drift(ref_df=reference_data, cur_df=clean_inf_data, predictors=predictors_cols,  job_id=job_id)\n",
    "    print(f\"Data Drift Retrain: {data_drift['retrain']}\")\n",
    "\n",
    "    return data_drift\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4gbSSj9Z-2y",
    "outputId": "87c79d4e-fa5e-49f6-fbe0-5da4a99be8c7"
   },
   "outputs": [],
   "source": [
    "model_snapshot(\"fcTel2\")\n",
    "d1_drift=inference_pipeline(inference_data=prediction_data[prediction_data.columns[:-1]],reference_data=ref_check_data,job_id='1cbhja2',predictors_cols=pred_cat_cols+pred_cts_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_7DsLU6Z-2y"
   },
   "source": [
    "No data drift!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oW7LH9zQZ-2y"
   },
   "outputs": [],
   "source": [
    "# Taking prediction data for testing\n",
    "pred_processed_data=preprocess_steps(prediction_data[prediction_data.columns[:-1]])\n",
    "d_pred_processed_data = xgb.DMatrix(pred_processed_data)\n",
    "predictions = xgbmodel.predict(d_pred_processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EO1T4DxKZ-2y"
   },
   "source": [
    "Lets compare predictions and actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0I6G4UfZ-2y"
   },
   "outputs": [],
   "source": [
    "# Saving the actual labels\n",
    "pred_label=prediction_data['Churn Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u45hgTcUZ-2y",
    "outputId": "acf231e2-5fc7-4884-dd6f-1f0a6c827b72"
   },
   "outputs": [],
   "source": [
    "print(f'Confusion Matrix: \\n{confusion_matrix(pred_label, predictions)}')\n",
    "print(f'Area Under Curve: {roc_auc_score(pred_label, predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8RlvR_yZ-2y"
   },
   "source": [
    "**Observation:**\n",
    "\n",
    "* The confusion matrix shows that the model correctly predicted 21449 instances of non-churn and 1486 instances of churn. However, it incorrectly predicted 2079 instances of churn and 618 instances of non-churn.\n",
    "\n",
    "* The area under the curve (AUC) is 0.8089, which indicates that the model has a moderate level of accuracy in distinguishing between churn and non-churn customers.\n",
    "\n",
    "* False negatives occur when the model predicts that a customer will not churn, but in reality, the customer does churn. In this case, the model has 618 false negatives, which means that it predicted 618 customers to be non-churners, but they actually churned. This is a concern because it means that the model is not able to accurately identify all of the customers who are at risk of churning, and this could result in missed opportunities to retain these customers.\n",
    "\n",
    "* False positives occur when the model predicts that a customer will churn, but in reality, the customer does not churn. In this case, the model has 2079 false positives, which means that it predicted 2079 customers to be churners, but they actually did not churn. This is also a concern because it could result in unnecessary retention efforts being directed towards customers who are not at risk of churning, which could be a waste of resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGHXLy6-Z-2z"
   },
   "source": [
    "Let's create a function to check data drift when label is available!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LH5RjXzYZ-2z"
   },
   "outputs": [],
   "source": [
    "def check_data_drift_with_label(ref_df:pd.DataFrame, cur_df:pd.DataFrame, target:str, predictors:list, job_id:str):\n",
    "    \"\"\"\n",
    "    Check for data drifts between two datasets and decide whether to retrain the model. \n",
    "    A report will be saved in the results directory.\n",
    "    :param ref_df: Reference dataset\n",
    "    :param cur_df: Current dataset\n",
    "    :param predictors: Predictors to check for drifts\n",
    "    :param target: Target variable to check for drifts\n",
    "    :param job_id: Job ID\n",
    "    :return: boolean\n",
    "    \"\"\"\n",
    "    ref_features = [col for col in predictors if col in ref_df.columns]\n",
    "    cur_features = [col for col in predictors if col in cur_df.columns]\n",
    "    ref_cat_features = [col for col in pred_cat_cols if col in ref_df.columns]\n",
    "    cur_cat_features = [col for col in pred_cat_cols if col in cur_df.columns]\n",
    "    ref_dataset = Dataset(ref_df, label=target, features=ref_features, cat_features=ref_cat_features)\n",
    "    cur_dataset = Dataset(cur_df, label=target,features=cur_features, cat_features=cur_cat_features)\n",
    "    \n",
    "    suite = Suite(\"data drift\",\n",
    "        NewLabelTrainTest(),\n",
    "        WholeDatasetDrift().add_condition_overall_drift_value_less_than(0.2), \n",
    "        FeatureLabelCorrelationChange().add_condition_feature_pps_difference_less_than(0.2), \n",
    "        TrainTestFeatureDrift().add_condition_drift_score_less_than(0.2), \n",
    "        TrainTestLabelDrift(balance_classes=True).add_condition_drift_score_less_than(0.4) \n",
    "    )\n",
    "    r = suite.run(train_dataset=ref_dataset, test_dataset=cur_dataset)\n",
    "    retrain = (len(r.get_not_ran_checks())>0) or (len(r.get_not_passed_checks())>0)\n",
    "    \n",
    "    # try:\n",
    "    #     r.save_as_html(f\"../reports/{job_id}_data_drift_report.html\")\n",
    "    #     print(\"[INFO] Data drift report saved as {}\".format(f\"{job_id}_data_drift_report.html\"))\n",
    "    # except Exception as e:\n",
    "    #     print(f\"[WARNING][DRIFTS.check_DATA_DRIFT] {traceback.format_exc()}\")\n",
    "    return {\"report\": r, \"retrain\": retrain}\n",
    "\n",
    "def check_model_drift(model,pred_data,label):\n",
    "    dmatrix = xgb.DMatrix(pred_data)\n",
    "    label_pred=model.predict(dmatrix)\n",
    "    test_f1_score = f1_score(label,label_pred)\n",
    "    test_recall = recall_score(label, label_pred)\n",
    "\n",
    "    print(\"\\n Test Results\")\n",
    "    print(f'F1 Score: {test_f1_score}')\n",
    "    print(f'Recall Score: {test_recall}')\n",
    "    print(f'Confusion Matrix: \\n{confusion_matrix(label, label_pred)}')\n",
    "    print(f'Area Under Curve: {roc_auc_score(label, label_pred)}')\n",
    "\n",
    "    #condition for model retraining according to business\n",
    "    model_retrain= (test_recall<0.80) or (test_f1_score<0.35)\n",
    "    print(f\"\\n Model Drift Retrain: {model_retrain}\")\n",
    "    return model_retrain,label_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbZuYOoJZ-2z"
   },
   "outputs": [],
   "source": [
    "def inference_pipeline_with_label(inference_data,reference_data,job_id,trained_model,target_col_name,target_value,predictors_cols):\n",
    "    # write data cleaning steps if necessary\n",
    "\n",
    "    # data preprocessing\n",
    "    clean_inf_data=preprocess_steps(inference_data)\n",
    "    clean_inf_data[target_col_name]=target_value\n",
    "\n",
    "    # data drift\n",
    "    data_drift=check_data_drift_with_label(ref_df=reference_data, cur_df=clean_inf_data, predictors=predictors_cols, target=target_col_name, job_id=job_id)\n",
    "    print(f\"Data Drift Retrain: {data_drift['retrain']}\")\n",
    "\n",
    "    # model drift\n",
    "    model_retrain,predictions=check_model_drift(model=trained_model,pred_data=clean_inf_data.drop(columns=target_col_name),label=target_value)\n",
    "\n",
    "    return  data_drift,model_retrain,predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xeRIVFEhZ-2z"
   },
   "outputs": [],
   "source": [
    "# actual values\n",
    "label_check_data=X_train.copy()\n",
    "label_check_data['Churn Value']=y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6u2dvFK6Z-20",
    "outputId": "3e69750a-7f37-436b-a02a-5a0f8d1b7eb0"
   },
   "outputs": [],
   "source": [
    "# Report of data drift with label\n",
    "d2_drift,model_retrain,pred=inference_pipeline_with_label(inference_data=prediction_data[prediction_data.columns[:-1]],reference_data=label_check_data,job_id='1njkwna',trained_model=xgbmodel,predictors_cols=pred_cat_cols+pred_cts_cols,target_col_name='Churn Value',target_value=prediction_data['Churn Value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MONYJU8W15KB"
   },
   "source": [
    "Here we would have to retrain the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPDMNIRN15KC"
   },
   "source": [
    "### **Model Retraining**\n",
    "\n",
    "Model retraining is a process of updating a machine learning model using newly available data. This is done to ensure that the model remains accurate and relevant to the changing data distribution. In the given code, the model retraining is performed when there is a mismatch between the actual target value and the predicted value. In such cases, the misclassified data points are collected and added back to the training data, and the model is retrained on the combined data.\n",
    "\n",
    "In the next section, when there is data drift, the misclassified observations are used as feedback for the model to update its parameters. The feedback data is concatenated with the original training data, preprocessed, and used to train a new model. The new model is trained for a specific number of rounds and then used for prediction.\n",
    "\n",
    "To judge if the retrained model should go ahead in production, we need to evaluate its performance on a validation set or a holdout test set. We can calculate various performance metrics such as accuracy, precision, recall, F1 score, and area under the ROC curve (AUC-ROC) to compare the performance of the new model with the old model.\n",
    "\n",
    "If the performance of the new model is better than the old model, we can deploy it in production. However, we need to be cautious of overfitting to the feedback data. To avoid overfitting, we can use techniques such as regularization, early stopping, and cross-validation while training the new model. We can also monitor the model's performance on the production data and retrain it periodically to ensure its accuracy and robustness.\n",
    "\n",
    "Finally, it is crucial to monitor the model's performance over time and update it as needed. This is because the data distribution may change over time, leading to a degradation in model performance. Therefore, regular retraining and monitoring of the model's performance are essential to ensure its continued relevance and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ml2eg_pXZ-20"
   },
   "outputs": [],
   "source": [
    "# Collect wrongly predicted observations and put it back as feedback for train\n",
    "\n",
    "params = {'objective': 'multi:softmax', 'num_class': 2}\n",
    "retrain_rounds=100\n",
    "# Retrain or partial retrain depending on data drift\n",
    "\n",
    "if d2_drift['retrain']:\n",
    "    clean_prediction_data=preprocess_steps(prediction_data[prediction_data.columns[:-1]])\n",
    "    drift_train=pd.concat([X_train,clean_prediction_data],ignore_index=True)\n",
    "    drift_label=pd.concat([y_train,prediction_data['Churn Value']],ignore_index=True)\n",
    "\n",
    "    xgbmodel = xgb.train(params, xgb.DMatrix(drift_train, label=drift_label), num_boost_round=num_rounds)\n",
    "\n",
    "\n",
    "elif model_retrain:\n",
    "    misclassified = prediction_data['Churn Value'] != pred\n",
    "    feedback_X = prediction_data[misclassified][prediction_data.columns[:-1]]\n",
    "    feedback_y = prediction_data[misclassified]['Churn Value']\n",
    "    \n",
    "    # Preprocess the combined training data\n",
    "    feedback_processed = preprocess_steps(feedback_X)\n",
    "\n",
    "     # Append misclassified feedback data to original training data\n",
    "    X_train_all = pd.concat([X_train, feedback_processed], ignore_index=True)\n",
    "    y_train_all = pd.concat([y_train, feedback_y], ignore_index=True)\n",
    "    \n",
    "    # Retrain the model on the combined training data\n",
    "    # xgb_retrained = xgb.train(params, xgb.DMatrix(X_train_all, label=y_train_all), \n",
    "    #                      xgb_model='../models/xgb_base.model', num_boost_round=retrain_rounds)\n",
    "    xgb_retrained = xgb.train(params, xgb.DMatrix(X_train_all, label=y_train_all), \n",
    "                         xgb_model=xgbmodel, num_boost_round=retrain_rounds)\n",
    "\n",
    "save_point(\"fcTel2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlpnfZ_m15KC",
    "outputId": "28a04969-3dea-4f0b-fb0c-3bd36c893ba7"
   },
   "outputs": [],
   "source": [
    "# Check after retraining\n",
    "d3_drift,model2_retrain,pred2=inference_pipeline_with_label(inference_data=prediction_data[prediction_data.columns[:-1]],reference_data=label_check_data,job_id='6378njkwna',trained_model=xgb_retrained,predictors_cols=pred_cat_cols+pred_cts_cols,target_col_name='Churn Value',target_value=prediction_data['Churn Value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a better result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0b14ECPu15KC"
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# xgb_retrained.save_model('../models/xgb_retrained.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3x7Ts8ajl4J"
   },
   "source": [
    "## **Project Summary**\n",
    "\n",
    "In conclusion, we have developed a machine learning model to predict churn in a telecommunications company. We followed a systematic approach that involved data exploration, preprocessing, model training, and data drift monitoring. We evaluated three models: logistic regression, random forest, and XGBoost, and selected the best-performing model based on its evaluation metrics.\n",
    "\n",
    "We also monitored the data drift and the model's performance regularly to ensure the model's stability over time.\n",
    "\n",
    "Our inference pipeline allows us to make predictions for new data and handle cases where the label (churn) is not present in the input data or where drift is detected.\n",
    "\n",
    "It is important to note that our primary goal for this project was not just to build an accurate churn prediction model, but also to demonstrate the implementation of a feedback loop in machine learning. Through continuous monitoring of the model's performance and data drift, we can provide timely feedback and improve the model's accuracy and stability.\n",
    "\n",
    "By implementing a feedback loop, we can ensure that our model remains relevant and effective, even as the underlying data and business environment change. This is crucial for businesses operating in a dynamic and ever-evolving landscape.\n",
    "\n",
    "Based on our model's predictions, we can draw insights into which customers are most likely to churn and why. We can also provide recommendations for business actions.\n",
    "\n",
    "**Recommendations**:\n",
    "\n",
    "\n",
    "* The company should focus on improving its customer support and services to reduce churn rates and increase customer satisfaction. This can include providing better support channels such as live chat or 24/7 support, improving response times, and training customer service representatives on effective communication skills.\n",
    "\n",
    "* The company should use the demographic and usage patterns of its customers to create targeted marketing campaigns that are more likely to resonate with specific customer segments. This can involve analyzing customer data to identify trends and preferences and tailoring marketing messages accordingly.\n",
    "\n",
    "* The company should consider offering incentives to customers who refer friends to use their services, as this can increase customer loyalty and bring in new customers. This can include referral bonuses, discounts, or other rewards for both the referring customer and the new customer.\n",
    "\n",
    "* The company should explore ways to increase revenue from customers who generate less revenue, such as by offering targeted promotions or service upgrades. For example, the company could offer a premium service plan with additional features or benefits that are attractive to customers who may be more price-sensitive.\n",
    "\n",
    "* The company should continue to monitor its customer satisfaction score and take action to improve it. This can involve conducting regular surveys or using social media monitoring tools to gather feedback and address any issues or concerns that customers may have.\n",
    "\n",
    "\n",
    "\n",
    "Overall, our approach demonstrates the potential of machine learning for predicting churn and its usefulness for informing business decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vO88CQjs15KD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "9054e5812adb29eebbcd6b680e8ef1afc4fe6e00a75ff130e735bd95b5b32301"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
