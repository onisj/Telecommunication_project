{"cells":[{"cell_type":"markdown","metadata":{"id":"TD4mUs2O5nn4"},"source":["# **BUILDING AN OFFER RECOMMENDER SYSTEM FOR TELECOMMUNICATIONS PRODUCTS/SERVICES**\n"]},{"cell_type":"markdown","metadata":{"id":"DKUGxhlbo74G"},"source":["# **Introduction**\n","\n","In the dynamic landscape of the telecom industry, offer recommendation plays a pivotal role by tailoring personalized suggestions to customers, leveraging insights gleaned from their usage habits, demographics, and pertinent data points. This strategic practice is instrumental in enhancing customer satisfaction and driving revenue growth. Employing sophisticated algorithms and harnessing the power of big data, telecom enterprises can furnish customers with pertinent and valuable offers, fostering heightened loyalty and facilitating sustained business expansion.\n","![image.png](https://ierp.ai/wp-content/uploads/2021/07/telecom-discount-recommendation.jpeg)\n","Credit: rawpixel.com\n"]},{"cell_type":"markdown","metadata":{"id":"6-AGRDjswx3C"},"source":["### **The Business Implications of Offer Recommendation**\n","\n","\n","* **Enhanced Customer Satisfaction:** Offer recommendation empowers telecom enterprises to gain deeper insights into customer preferences, enabling the delivery of pertinent and valuable offers. Consequently, this fosters heightened levels of customer satisfaction.\n","\n","* **Revenue Growth:** Through the provision of tailored offers, telecom companies can drive revenue growth by encouraging increased customer spending while mitigating churn, thereby bolstering overall financial performance.\n","\n","* **Fostering Customer Loyalty:** Personalized offers not only enhance the customer experience but also nurture enduring relationships between telecom providers and their clientele. This, in turn, cultivates heightened levels of customer loyalty, pivotal for sustained business success.\n"]},{"cell_type":"markdown","metadata":{"id":"c6HTiYsXpKJ7"},"source":["## **Potential Solutions**\n","\n","\n","### **Approaches to Offer Recommendation**\n","\n","\n","Developing an offer recommendation system encompasses various methodologies, with the optimal choice contingent upon factors such as available data, system complexity, and computational resources. Below are several approaches:\n","\n","1. **Rule-Based Systems:** Rule-based systems recommend offers based on predefined directives. While simple to implement and interpret, they may lack accuracy and adaptability. Ideal for specific business directives, such as utilizing Product A to promote Product B as part of a marketing strategy.\n","\n","2. **Collaborative Filtering:** This method recommends offers by analyzing the preferences of similar customers. By identifying customer similarities, it suggests offers suited to comparable customer profiles.\n","\n","3. **Content-Based Filtering:** Here, the system constructs a model of customer preferences based on offer features they've engaged with, subsequently recommending similar offers. While effective with rich feature data, it may face challenges with the cold start problem, particularly in suggesting offers to new customers.\n","\n","4. **Hybrid Systems:** Hybrid systems amalgamate multiple approaches to capitalize on their respective strengths and offset weaknesses. For instance, employing collaborative filtering for existing customer interactions and content-based filtering for new customer recommendations.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Mhjzt133qQr8"},"source":["### **Assumptions**\n","\n","As I embark on this analysis, I operate under the following assumptions:\n","\n","* I assume the existence of multiple offers available within the system.\n","\n","* Furthermore, I assume that, currently, each customer can only have one offer activated.\n","\n","* Within the dataset, I understand that the column \"offer\" denotes the offer extended to the customer, while the churn status column indicates whether the customer churned or not.\n","\n","* In cases where a customer possesses an offer but lacks a churn status, I will interpret this as the customer presently utilizing that offer."]},{"cell_type":"markdown","metadata":{"id":"CI7ubDgiEIjP"},"source":["## **My Approach**\n","\n","\n","In approaching this challenge, I adopt an unsupervised learning framework. This entails that within the dataset, there isn't a definitive means to ascertain the correctness or incorrectness of our actions.\n","\n","In practical application, validating this algorithm would necessitate testing it with actual customers to gauge its efficacy in reducing churn.\n","\n","Based on our established assumptions regarding offers, I will construct a collaborative-filtering system centered around the individual user. To simplify, the approach can be outlined as follows:\n","\n","1. Develop an algorithm to discern the n-most similar customers to customer A.\n","\n","2. Utilize a churn-rate methodology to ascertain the most successful offer among these similar customers.\n","\n","3. Select the most successful offer to present to customer A.\n","\n","\n","Training of this algorithm will be conducted on a subset of the customer dataset who have received offers (labeled A, B, C, D, E, F, G, H, I, or J) in their 'offer' field. The intention is to subsequently apply this learned model to the 'No Offer' group."]},{"cell_type":"markdown","metadata":{"id":"EqlkWqtaySzO"},"source":["## **Package Requirements**\n","\n","Avoid installing the libraries in the next cell if you have previously run the requirements.txt file:\n","\n","``` python\n","    pip install -r requirements.txt\n","```\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"aD5rK2Sn9sht","executionInfo":{"status":"ok","timestamp":1713223560956,"user_tz":-60,"elapsed":3,"user":{"displayName":"Segun John Oni","userId":"17240615915142463897"}}},"outputs":[],"source":["# # Installing packages\n","# !pip install numpy==1.22.4\n","# !pip install pandas==1.3.5\n","# !pip install scikit-learn==1.2.1\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"J0X_4ajp9shv","executionInfo":{"status":"ok","timestamp":1713223595938,"user_tz":-60,"elapsed":1691,"user":{"displayName":"Segun John Oni","userId":"17240615915142463897"}}},"outputs":[],"source":["# Import Libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n","\n","# Suppress all warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"eC9-7gaYGfau","executionInfo":{"status":"ok","timestamp":1713223622735,"user_tz":-60,"elapsed":355,"user":{"displayName":"Segun John Oni","userId":"17240615915142463897"}}},"outputs":[],"source":["# Configuring display options for improved data viewing with large datasets\n","pd.set_option('display.max_columns', 200)  # Set maximum displayed columns to 200\n","pd.set_option('display.max_rows', 300)     # Set maximum displayed rows to 300\n"]},{"cell_type":"code","source":["# Works if I'm working on Google Colab, else it passes on anaconda.\n","try:\n","    # Attempting to import the drive module from google.colab\n","    from google.colab import drive\n","\n","    # Mounting Google Drive\n","    drive.mount('/content/drive')\n","    print(\"Google Drive mounted successfully!\")\n","except Exception as e:\n","    # Handling the case when the import fails, likely due to working outside of Google Colab environment\n","    print(\"Error occurred while mounting Google Drive:\", e)\n","    pass\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AscxxuskFg6o","executionInfo":{"status":"ok","timestamp":1713223682936,"user_tz":-60,"elapsed":43930,"user":{"displayName":"Segun John Oni","userId":"17240615915142463897"}},"outputId":"e231b7c2-e08d-4bb1-e271-c9db3235fc36"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Google Drive mounted successfully!\n"]}]},{"cell_type":"markdown","metadata":{"id":"pbB4yB5VmnqF"},"source":["## **Data Exploration**\n"]},{"cell_type":"code","source":["# Reading data from a CSV file\n","try:\n","    # Reading data from a CSV file on local machine\n","    df = pd.read_csv('../data/processed/advanced_telecom_data.csv')\n","    print(\"CSV file read successfully from local machine!\")\n","except FileNotFoundError:\n","    try:\n","        # Reading data from a CSV file hosted on AWS S3\n","        df = pd.read_csv('/content/drive/Othercomputers/MyLaptop/git_repos/projectpro/Projects/MachineLearning/Telecommunication_project/data/processed/advanced_telecom_data.csv')\n","        print(\"CSV file read successfully from  Google Drive!\")\n","    except Exception as e:\n","        print(\"An error occurred while reading the CSV file:\", e)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SGYBn0D2GLCW","executionInfo":{"status":"ok","timestamp":1713223872109,"user_tz":-60,"elapsed":23845,"user":{"displayName":"Segun John Oni","userId":"17240615915142463897"}},"outputId":"ede2f12c-aa26-4acf-f500-daa52e739d93"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["CSV file read successfully from  Google Drive!\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pKsSttGTz84y"},"outputs":[],"source":["# use s3_link if you are connected with internet\n","s3_link='https://s3.amazonaws.com/projex.dezyre.com/recommender-system-for-telecom-products/materials/Telecom_data.csv'\n","df = pd.read_csv(s3_link)\n","\n","## if not use following and provide the path accordingly\n","# csv_file_path='../data/Telecom_data.csv'\n","# df = pd.read_csv(csv_file_path)"]},{"cell_type":"markdown","metadata":{"id":"sCiAPaLzGIU1"},"source":["Lets look at the first 10 records from the dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A48l1uFgyMVq"},"outputs":[],"source":["df.head(10)"]},{"cell_type":"markdown","metadata":{"id":"lOCmKZUWVlYV"},"source":["When we inspect the dataframe, we notice the same customer appears more than once, in different months.\n","\n","We have 2 options here:\n","1. Create a consolidated frame where each customer is in 1 line;\n","2. Use the different moments in time of each customer as a datapoint;\n","\n","The latter approach gives us more data points in theory, because in different months customers could have behaved differently. We will follow with approach number 2. However, we need to keep this in mind as we calculate similarity scores."]},{"cell_type":"markdown","metadata":{"id":"VvmWYEFMppHZ"},"source":["# **Exploratory Data Analysis**"]},{"cell_type":"markdown","metadata":{"id":"BQhYLW2YE-tA"},"source":["## **Data Exploration**\n","\n","Data exploration is a critical step in the data analysis process, where you examine the dataset to gain a preliminary understanding of the data, detect patterns, and identify potential issues that may need further investigation. Data exploration is important because it helps to provide a solid foundation for subsequent data analysis tasks, hypothesis testing and data visualization.\n","\n","Data exploration is also important because it can help you to identify an appropriate approach for analyzing the data.\n","\n","Here are the various functions that help us explore and understand the data.\n","\n","* Shape: Shape is used to identify the dimensions of the dataset. It gives the number of rows and columns present in the dataset. Knowing the dimensions of the dataset is important to understand the amount of data available for analysis and to determine the feasibility of different methods of analysis.\n","\n","* Head: The head function is used to display the top five rows of the dataset. It helps us to understand the structure and organization of the dataset. This function gives an idea of what data is present in the dataset, what the column headers are, and how the data is organized.\n","\n","* Tail: The tail function is used to display the bottom five rows of the dataset. It provides the same information as the head function but for the bottom rows. The tail function is particularly useful when dealing with large datasets, as it can be time-consuming to scroll through all the rows.\n","\n","* Describe: The describe function provides a summary of the numerical columns in the dataset. It includes the count, mean, standard deviation, minimum, and maximum values, as well as the quartiles. It helps to understand the distribution of the data, the presence of any outliers, and potential issues that can affect the model's accuracy.\n","\n","* Isnull: The isnull function is used to identify missing values in the dataset. It returns a Boolean value for each cell, indicating whether it is null or not. This function is useful to identify the presence of missing data, which can be problematic for regression analysis.\n","\n","* Dropna: The dropna function is used to remove rows or columns with missing data. It is used to remove any observations or variables with missing data, which can lead to biased results in the regression analysis. The dropna function is used after identifying the missing data with the isnull function.\n","\n","* Columns: The .columns method is a built-in function that is used to display the column names of a pandas DataFrame or Series. It returns an array-like object that contains the names of the columns in the order in which they appear in the original DataFrame or Series. It can be used to obtain a quick overview of the variables in a dataset and their names."]},{"cell_type":"markdown","metadata":{"id":"xHGerr8zYgVM"},"source":["**Note**\n","\n","This Project is part of the \"telecom data\" cluster of projects, targeting the telecom industry. It is recommended to go through the \"[Exploratory Data Analysis](https://www.projectpro.io/data-science-use-cases/telecom-data-analysis-project )\" notebook before executing this project to understand the Data better.\n"]},{"cell_type":"markdown","metadata":{"id":"bS4gsX0KKPn-"},"source":["### **What can we learn from the data?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NG4nDrmOHU0s"},"outputs":[],"source":["# Check the shape of the Dataframe\n","df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u_0O85GRJRDT"},"outputs":[],"source":["# Checking the names of the columns\n","df.columns"]},{"cell_type":"markdown","metadata":{"id":"vZPhb0YX8W_m"},"source":["## **Data Dictionary**\n","\n","\n","\n","| Column name\t | Description|\n","| ----- | ----- |\n","| Customer ID\t | Unique identifier for each customer |\n","| Month | Calendar Month- 1:12 |\n","| Month of Joining |\tCalender Month -1:14, Month for which the data is captured|\n","| zip_code |\tZip Code|\n","|Gender |\tGender|\n","| Age |\tAge(Years)|\n","| Married |\tMarital Status |\n","|Dependents | Dependents - Binary |\n","| Number of Dependents |\tNumber of Dependents|\n","|Location ID |\tLocation ID|\n","|Service ID\t |Service ID|\n","|state|\tState|\n","|county\t|County|\n","|timezone\t|Timezone|\n","|area_codes|\tArea Code|\n","|country\t|Country|\n","|latitude|\tLatitude|\n","|longitude\t|Longitude|\n","|arpu|\tAverage revenue per user|\n","|roam_ic\t|Roaming incoming calls in minutes|\n","|roam_og\t|Roaming outgoing calls in minutes|\n","|loc_og_t2t|\tLocal outgoing calls within same network in minutes|\n","|loc_og_t2m\t|Local outgoing calls outside network in minutes(outside same + partner network)|\n","|loc_og_t2f|\tLocal outgoing calls with Partner network in minutes|\n","|loc_og_t2c\t|Local outgoing calls with Call Center in minutes|\n","|std_og_t2t|\tSTD outgoing calls within same network in minutes|\n","|std_og_t2m|\tSTD outgoing calls outside network in minutes(outside same + partner network)|\n","|std_og_t2f|\tSTD outgoing calls with Partner network in minutes|\n","|std_og_t2c\t|STD outgoing calls with Call Center in minutes|\n","|isd_og|\tISD Outgoing calls|\n","|spl_og\t|Special Outgoing calls|\n","|og_others|\tOther Outgoing Calls|\n","|loc_ic_t2t|\tLocal incoming calls within same network in minutes|\n","|loc_ic_t2m|\tLocal incoming calls outside network in minutes(outside same + partner network)|\n","|loc_ic_t2f\t|Local incoming calls with Partner network in minutes|\n","|std_ic_t2t\t|STD incoming calls within same network in minutes|\n","|std_ic_t2m\t|STD incoming calls outside network in minutes(outside same + partner network)|\n","|std_ic_t2f|\tSTD incoming calls with Partner network in minutes|\n","|std_ic_t2o|\tSTD incoming calls operators other networks in minutes|\n","|spl_ic|\tSpecial Incoming calls in minutes|\n","|isd_ic|\tISD Incoming calls in minutes|\n","|ic_others|\tOther Incoming Calls|\n","|total_rech_amt|\tTotal Recharge Amount in Local Currency|\n","|total_rech_data|\tTotal Recharge Amount for Data in Local Currency\n","|vol_4g|\t4G Internet Used in GB|\n","|vol_5g|\t5G Internet used in GB|\n","|arpu_5g|\tAverage revenue per user over 5G network|\n","|arpu_4g|\tAverage revenue per user over 4G network|\n","|night_pck_user|\tIs Night Pack User(Specific Scheme)|\n","|fb_user|\tSocial Networking scheme|\n","|aug_vbc_5g|\tVolume Based cost for 5G network (outside the scheme paid based on extra usage)|\n","|offer|\tOffer Given to User|\n","|Referred a Friend|\tReferred a Friend : Binary|\n","|Number of Referrals|\tNumber of Referrals|\n","|Phone Service|\tPhone Service: Binary|\n","|Multiple Lines|\tMultiple Lines for phone service: Binary|\n","|Internet Service|\tInternet Service: Binary|\n","|Internet Type|\tInternet Type|\n","|Streaming Data Consumption|\tStreaming Data Consumption|\n","|Online Security|\tOnline Security|\n","|Online Backup|\tOnline Backup|\n","|Device Protection Plan|\tDevice Protection Plan|\n","|Premium Tech Support|\tPremium Tech Support|\n","|Streaming TV|\tStreaming TV|\n","|Streaming Movies|\tStreaming Movies|\n","|Streaming Music|\tStreaming Music|\n","|Unlimited Data|\tUnlimited Data|\n","|Payment Method|\tPayment Method|\n","|Status ID|\tStatus ID|\n","|Satisfaction Score|\tSatisfaction Score|\n","|Churn Category|\tChurn Category|\n","|Churn Reason|\tChurn Reason|\n","|Customer Status|\tCustomer Status|\n","|Churn Value|\tBinary Churn Value\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hTP5K9uaHlxJ"},"outputs":[],"source":["# Check the Information of the Dataframe, datatypes and non-null counts\n","df.info()"]},{"cell_type":"markdown","metadata":{"id":"z8VSPNHoHZqo"},"source":["**Observation:**\n","* we can see some null values present in this data. We will check about this later\n","* arpu_4g and arpu_5g should be in float/int datatype but its object"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-EXpuNmrSFbp"},"outputs":[],"source":["df['arpu_4g'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-_f4xXijSSIs"},"outputs":[],"source":["df['arpu_5g'].unique()"]},{"cell_type":"markdown","metadata":{"id":"9GEm3otXST4D"},"source":["It seems both the columns have value as Not Applicable resulting in the different datatype than expected. We will deal with this later."]},{"cell_type":"markdown","metadata":{"id":"tKdYVxKPLEIG"},"source":["Lets see how many different offers we have"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"99IYIPfVLPMt"},"outputs":[],"source":["df['offer'].unique()"]},{"cell_type":"markdown","metadata":{"id":"oRmSPUhcLJYL"},"source":["So there are 10 different offers available."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YmHeFi1E0JGy"},"outputs":[],"source":["preserve(\"fcTel3\")\n","\n","# Taking a look at the offer distribution\n","dfg = df.groupby('offer').agg({'Customer ID':'count'}).reset_index()\n","dfg['% Total'] = dfg['Customer ID']/dfg['Customer ID'].sum() #this creates a % of total column\n","dfg['% Total'] = dfg['% Total'].apply(lambda x: '{:.2%}'.format(x)) #this function simply formats the column to %\n","dfg #this displays the dataframe"]},{"cell_type":"markdown","metadata":{"id":"biDeooU4Ly-N"},"source":["**Observation**\n","* The Offers seems to be evenly distributed amongst customers\n","* There are about 76% users who did not receive any offer from the company\n"]},{"cell_type":"markdown","metadata":{"id":"AAv8WcGBMKPl"},"source":["**Think about it**\n","\n","* Is there any way to check impact of offers on churn?\n","* How many customer churned as they were not given any offer?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z6hg8ehVLhSm"},"outputs":[],"source":["# Taking a look at the offer distribution and churn distribution\n","dfg2 = df.groupby(['offer','Customer Status']).agg({'Customer ID':'count'}).reset_index()\n","pivoted_dfg2 = dfg2.pivot(index='offer', columns='Customer Status', values='Customer ID')\n","pivoted_dfg2 = pivoted_dfg2.reset_index()\n","pivoted_dfg2['Churn Rate'] = pivoted_dfg2['Churned']/(pivoted_dfg2['Churned'] + pivoted_dfg2['Stayed'])\n","pivoted_dfg2['Churn Rate'] = pivoted_dfg2['Churn Rate'].apply(lambda x: '{:.2%}'.format(x)) #this function simply formats the column to %\n","pivoted_dfg2"]},{"cell_type":"markdown","metadata":{"id":"f1cJKnw-LwDy"},"source":["**Observations**\n","\n","* churn rate seems to be similar amongst customers regardless of the offer they received -> this tells us that maybe offers are not being tailored enough to groups\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jMyYmwM8Lmc8"},"outputs":[],"source":["# Taking a look at the churn category\n","dfg2 = df.groupby(['Churn Category',]).agg({'Customer ID':'count'}).reset_index()\n","dfg2['% Total'] = dfg2['Customer ID']/dfg2['Customer ID'].sum() #this creates a % of total column\n","dfg2['% Total'] = dfg2['% Total'].apply(lambda x: '{:.2%}'.format(x)) #this function simply formats the column to %\n","dfg2 #this displays the dataframe"]},{"cell_type":"markdown","metadata":{"id":"SruZKKyXQKf7"},"source":["**Observations**\n","\n","* The Churn Category for Competitor, Dissatisfaction, Price, Support have higher customers\n","* We can give them specific offers which may lead them to stay rather than churning\n"]},{"cell_type":"markdown","metadata":{"id":"jXUOLcU7Mu1B"},"source":["**Think about it**\n","\n","* Is there any better way to recommend offers to customers which can impact less churn rate in future?\n","* It would be interesting to see what happens if we tailor offers by looking at churn probability for each customer(May be a problem to solve in our next project)"]},{"cell_type":"markdown","metadata":{"id":"wQ4Wz4J3Q9wj"},"source":["# **Data Processing**"]},{"cell_type":"markdown","metadata":{"id":"oRmVyw-kRN0Q"},"source":["## **Missing Value Detection and Imputation**"]},{"cell_type":"markdown","metadata":{"id":"MRb18EtfRAAO"},"source":["We previously saw there are some missing values in the data. Lets have a look into that now."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fZIGeBwUHluM"},"outputs":[],"source":["data_pipeline(\"fcTel3\")\n","# Creating a missing value df with the null values of our original dataframe\n","percent_missing = df.isna().sum() * 100 / len(df)\n","missing_value_df = pd.DataFrame({'column_name': df.columns,\n","                                 'percent_missing': percent_missing.values})\n","\n","#sorting the dataframe by percent missing value\n","missing_value_df.sort_values(by='percent_missing',ascending=False)"]},{"cell_type":"markdown","metadata":{"id":"4V6oKEhHIhPV"},"source":["**Observation**\n","\n","\n","\n","*  Columns 'fb_user' and 'night_pck_user' have more 50% missing value. We will simply drop this from our dataframe\n","* According to data dictionary 'Internet Type' and 'total_rech_data' seems to correlated.\n","*   We need to check for columns 'Internet Type' and 'total_rech_data' and impute missing values if possible\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3RoxWkum9siI"},"outputs":[],"source":["#dropping the variables with more than 50% null values\n","df=df.drop(columns=['fb_user','night_pck_user'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VzfT_M0pIYZD"},"outputs":[],"source":["# Null values in total recharge data\n","df['total_rech_data'].isna().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lX-tRqJBIYSh"},"outputs":[],"source":["# Null values in Internet Type\n","df['Internet Type'].isna().sum()"]},{"cell_type":"markdown","metadata":{"id":"rXB_Rw-8JH8u"},"source":["**Observation:**\n","\n","*  These missing values may represent customers who have not recharged their account or have recharged but the information has not been recorded.\n","\n","* It is possible that customers with missing recharge data are those who received free data service, and therefore did not need to recharge their account. Alternatively, it is possible that the missing values are due to technical issues, such as data recording errors or system failures."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"95Ys5Vp_JJW6"},"outputs":[],"source":["# Checking the value counts of Internet Service where total recharge data was null\n","df[df['total_rech_data'].isna()]['Internet Service'].value_counts(dropna=False)"]},{"cell_type":"markdown","metadata":{"id":"xXQF7lyhJc-K"},"source":["**Observation**:\n","\n","* It turns out that all customers with missing recharge data have opted for internet service, the next step could be to check if they have used it or not."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZGkYQNQzJJUN"},"outputs":[],"source":["# Let's check unlimited data column\n","df[(df['total_rech_data'].isna())]['Unlimited Data'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v3LJDZD2JJRO"},"outputs":[],"source":["# Lets check Average Revenue for 4g and 5g when there is no recharge for data\n","df[(df['total_rech_data'].isna())][['arpu_4g','arpu_5g']].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"XP2-Ju-PJpCb"},"source":["**Observation**:\n","\n","* We can fill the missing values in the total_rech_data column with 0 when the arpu (Average Revenue Per User) is not applicable. This is because the arpu is a measure of the revenue generated per user, and if it is not applicable, it may indicate that the user is not generating any revenue for the company. In such cases, it is reasonable to assume that the total data recharge amount is 0\n","* It is advisable to check with the business before making this decision"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pDCOb9RbJJPI"},"outputs":[],"source":["# Replacing all values of total recharge data= 0 where arpu 4g and 5g are not applicable\n","df.loc[(df['arpu_4g']=='Not Applicable') | (df['arpu_5g']=='Not Applicable'),'total_rech_data']=0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l2rNS84dJJMN"},"outputs":[],"source":["# Missing value percentage after imputation\n","df['total_rech_data'].isna().sum()/df.shape[0]"]},{"cell_type":"markdown","metadata":{"id":"WlPcG1IDJ4LE"},"source":["We cannot fill other values with 0 because they have some ARPU to consider."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hROmJEL3J8ch"},"outputs":[],"source":["# Calculate the mean of 'total_rech_data' where either 'arpu_4g' or 'arpu_5g' is not equal to 'Not Applicable'\n","arpu_data_mean=df.loc[(df['arpu_4g']!='Not Applicable') | (df['arpu_5g']!='Not Applicable'),'total_rech_data'].mean()\n","arpu_data_mean"]},{"cell_type":"markdown","metadata":{"id":"puGtRYsZJ-Wo"},"source":["With this mean, we will fill the NaN values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"al2O9x9KJJJx"},"outputs":[],"source":["# Fill NaN values in 'total_rech_data' with the mean of 'total_rech_data' where either 'arpu_4g' or 'arpu_5g' is not equal to 'Not Applicable'\n","df['total_rech_data']=df['total_rech_data'].fillna(arpu_data_mean)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CTr0Ct-zJJHY"},"outputs":[],"source":["df['total_rech_data'].isna().sum()"]},{"cell_type":"markdown","metadata":{"id":"WrIm44_QKO0f"},"source":["There are no more missing values in the column ''total_rech_data'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q6onFrg3Kpyn"},"outputs":[],"source":["# Check the value counts for Internet Type\n","df['Internet Type'].value_counts(dropna=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MbAJ-PT4Kpvz"},"outputs":[],"source":["# Check value counts for Internet Service where Internet Type is null\n","df[df['Internet Type'].isna()]['Internet Service'].value_counts(dropna=False)"]},{"cell_type":"markdown","metadata":{"id":"A4jKCwzuKuF8"},"source":["All null values in Internet Type does not have Internet Service. Let's fill these null values with Not Applicable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KPM4symEKtm8"},"outputs":[],"source":["# Filling Null values in Internet Type\n","df['Internet Type']=df['Internet Type'].fillna('Not Applicable')"]},{"cell_type":"markdown","metadata":{"id":"kjdm6Ub4SimD"},"source":["Replace 'Not Applicable' with 0 in both 'arpu_4g' and 'arpu_5g and convert them to float"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vjIERGOFSgky"},"outputs":[],"source":["# Replace 'Not Applicable' with 0 in 'arpu_4g'\n","df['arpu_4g'] = df['arpu_4g'].replace('Not Applicable', 0)\n","\n","# Replace 'Not Applicable' with 0 in 'arpu_5g'\n","df['arpu_5g'] = df['arpu_5g'].replace('Not Applicable', 0)\n","\n","# Convert 'arpu_4g' to float data type\n","df['arpu_4g'] = df['arpu_4g'].astype(float)\n","\n","# Convert 'arpu_5g' to float data type\n","df['arpu_5g'] = df['arpu_5g'].astype(float)"]},{"cell_type":"markdown","metadata":{"id":"WHm8FJPERTT7"},"source":["## **Outlier Detection and Imputation**\n","\n","\n","Outlier detection is a critical data analysis technique that involves identifying and removing data points that are significantly different from the rest of the data. Outliers are data points that lie far away from the rest of the data, and they can significantly influence the statistical analysis and machine learning models' performance. Therefore, identifying and removing outliers is essential to ensure accurate and reliable data analysis results.\n","\n","There are two main approaches for outlier detection: parametric and non-parametric.\n","\n","* Parametric Methods:\n","Parametric methods assume that the data follows a specific distribution, such as a normal distribution. In this approach, outliers are identified by calculating the distance of each data point from the mean of the distribution in terms of the number of standard deviations. Data points that are beyond a certain number of standard deviations (usually three or more) are considered as outliers.\n","\n","One common parametric method is the Z-score method, which calculates the distance of each data point from the mean in terms of standard deviations.\n","Parametric methods can be useful when the data follows a known distribution, but they may not be effective when the data is not normally distributed.\n","\n","* Non-Parametric Methods:\n","Non-parametric methods do not assume any specific distribution of the data. Instead, they rely on the rank or order of the data points. In this approach, outliers are identified by comparing the values of each data point with the values of other data points. Data points that are significantly different from other data points are considered as outliers.\n","\n","Quantiles are an important concept in non-parametric outlier detection methods. They represent values that divide a dataset into equal-sized parts, such as quarters or thirds. The most commonly used quantiles are the median (which divides the data into two equal parts), the first quartile (which divides the data into the lowest 25% and the highest 75%), and the third quartile (which divides the data into the lowest 75% and the highest 25%).\n","\n","The interquartile range (IQR) is another important concept related to quantiles. It is defined as the difference between the third and first quartiles and represents the middle 50% of the data. The IQR can be used to identify outliers by defining a range (known as the Tukey's fence) beyond which any data points are considered outliers.\n","Non-parametric methods can be useful when the data is not normally distributed or when the distribution is unknown."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c7u9WbKWRp7L"},"outputs":[],"source":["# List of continuous columns\n","cts_cols=['Age','Number of Dependents',\n","       'roam_ic', 'roam_og', 'loc_og_t2t',\n","       'loc_og_t2m', 'loc_og_t2f', 'loc_og_t2c', 'std_og_t2t', 'std_og_t2m',\n","       'std_og_t2f', 'std_og_t2c', 'isd_og', 'spl_og', 'og_others',\n","       'loc_ic_t2t', 'loc_ic_t2m', 'loc_ic_t2f', 'std_ic_t2t', 'std_ic_t2m',\n","       'std_ic_t2f', 'std_ic_t2o', 'spl_ic', 'isd_ic', 'ic_others',\n","       'total_rech_amt', 'total_rech_data', 'vol_4g', 'vol_5g', 'arpu_5g',\n","       'arpu_4g', 'arpu', 'aug_vbc_5g', 'Number of Referrals','Satisfaction Score',\n","       'Streaming Data Consumption']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IrfdVPj5RHnT"},"outputs":[],"source":["data_pipeline(\"fcTel3\")\n","# Create an empty dataframe with columns as cts_cols and index as quantiles\n","quantile_df=pd.DataFrame(columns=cts_cols,index=[0.1,0.25,0.5,0.75,0.8,0.9,0.95,0.97,0.99])\n","\n","# for each column in cts_cols, calculate the corresponding quantiles and store them in the quantile_df\n","for col in cts_cols:\n","   quantile_df[col]=df[col].quantile([0.1,0.25,0.5,0.75,0.8,0.9,0.95,0.97,0.99])"]},{"cell_type":"markdown","metadata":{"id":"CXqbFZTwSrOd"},"source":["By calculating quantiles for each continuous variable in the dataset, we are trying to get an idea about the spread and distribution of the data. Specifically, we are interested in identifying potential outliers in the data.\n","\n","Quantiles divide a distribution into equal proportions. For instance, the 0.25 quantile is the value below which 25% of the observations fall and the 0.75 quantile is the value below which 75% of the observations fall. By calculating quantiles at various levels, we can get a better understanding of the distribution of the data and identify any observations that are too far away from the rest of the data.\n","\n","These quantiles can be used as thresholds to identify potential outliers in the data. Observations with values beyond these thresholds can be considered as potential outliers and further investigation can be carried out to determine if they are true outliers or not."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WdfbB-PbRHlv"},"outputs":[],"source":["# Let's check out the quantiles df\n","quantile_df"]},{"cell_type":"markdown","metadata":{"id":"mZokBV6xSyNq"},"source":["**Observation**\n","\n","The variables vol_5g, arpu_4g, and arpu_5g seems to have some abrupt values"]},{"cell_type":"markdown","metadata":{"id":"sj8qGM73TPgE"},"source":["**Note**\n","\n","We can inquire about this values with business. Remember to check out with business stakeholders before making any changes/imputations to data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3mjKNRn9RHi-"},"outputs":[],"source":["# Checking further\n","df['arpu_4g'].quantile([0.75,0.8,0.9,0.95,0.97,0.99,0.999])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-v-8bWJpRHgg"},"outputs":[],"source":["# Calculate the proportion of rows in the DataFrame where the value in the 'arpu_4g' column is equal to 254687\n","df[df['arpu_4g']==254687].shape[0]/df.shape[0]"]},{"cell_type":"markdown","metadata":{"id":"gUe-boM2S_pH"},"source":["Let's see what is the value of 'total_rech_data' for these observations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T0RlXGFYS9LK"},"outputs":[],"source":["# Get the value counts of 'total_rech_data' for observations where the value in the 'arpu_4g' column is equal to 254687\n","df[df['arpu_4g']==254687]['total_rech_data'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"Q9HRgLbHTCD8"},"source":["Now, since the recharge amount is 0 and there is no ARPU, let's replace it with 0."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FI_GKng9S9Hz"},"outputs":[],"source":["# Replace the outlier value 254687 in the 'arpu_4g' column of the dataframe 'df' with 0.\n","df['arpu_4g']=df['arpu_4g'].replace(254687,0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wcuXfrIAS9Ff"},"outputs":[],"source":["# Checking further\n","df['arpu_4g'].quantile([0.75,0.8,0.9,0.95,0.97,0.99,0.999])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tLvhMqrnS9AV"},"outputs":[],"source":["# Filter by 'arpu_4g' value of 87978 and count unique values in 'total_rech_data' column\n","df[df['arpu_4g']==87978]['total_rech_data'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"OOhQmUc0TJwO"},"source":["All rows in the dataframe with an 'arpu_4g' value of 87978 have 0 value in the 'total_rech_data' column, indicating that these are likely outliers. Therefore, we have decided to replace the 'arpu_4g' value for these rows with 0."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptW6IaZsS89i"},"outputs":[],"source":["# Replace the values with 0\n","df['arpu_4g']=df['arpu_4g'].replace(87978,0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AFAj5deaTmwp"},"outputs":[],"source":["# Checking the quantiles again\n","df['arpu_4g'].quantile([0.75,0.8,0.9,0.95,0.97,0.99,0.999])"]},{"cell_type":"markdown","metadata":{"id":"zMJJylafUPYk"},"source":["This seems to be fairly good now"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DzT5H-5wTmqx"},"outputs":[],"source":["# Get the value counts of 'total_rech_data' for observations where the value in the 'arpu_5g' column is equal to 254687\n","df[df['arpu_5g']==254687]['total_rech_data'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tSQYnL0ETmoC"},"outputs":[],"source":["# Get the value counts of 'total_rech_data' for observations where the value in the 'arpu_5g' column is equal to 87978\n","df[df['arpu_5g']==87978]['total_rech_data'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sk4DaNdDUJMM"},"outputs":[],"source":["# Replacing the values with 0 where total recharge data is 0\n","df['arpu_5g']=df['arpu_5g'].replace([87978,254687],0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n3AA2C60UJGE"},"outputs":[],"source":["# Check the quantiles of ARPU 5G\n","df['arpu_5g'].quantile([0.75,0.8,0.9,0.95,0.97,0.99,0.999])"]},{"cell_type":"markdown","metadata":{"id":"NEumGXDGUTLO"},"source":["This seems to be fairly good now"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g0vbAu3HUOSB"},"outputs":[],"source":["# Check the quantiles of Volume of 5G data\n","df['vol_5g'].quantile([0.75,0.8,0.9,0.95,0.97,0.98,0.99,0.999])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lJJgJkjGUOQA"},"outputs":[],"source":["# Lets see the recharge data value for vol_5g more than 87978\n","df[df['vol_5g']>=87978]['total_rech_data'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A2d0bSpdUOMt"},"outputs":[],"source":["# Proportion of these values\n","df[df['vol_5g']>=87978]['total_rech_data'].value_counts()/df.shape[0]"]},{"cell_type":"markdown","metadata":{"id":"KFKRydSZUjEp"},"source":["**Observation**:\n","\n","There is a presence of 2% outliers in vol 5g, where the values are very high, but their total recharge data is 0. We will fill these outliers with 0, and below are some possible reasons why this could be:\n","\n","* Data recording error: It is possible that there was an error in recording the recharge data for these outliers, leading to an incorrect value of 0. In this case, it would make sense to fill the outliers with 0, as this is likely the correct value.\n","\n","* Promotions or bonuses: Another possibility is that these customers received promotions or bonuses that allowed them to use the service without recharging, leading to a total recharge data of 0. However, these customers may still be using the service heavily, leading to the high values in vol 5g. In this case, filling the outliers with 0 would make sense as it accurately reflects the lack of recharge data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LxucXEUoUmr_"},"outputs":[],"source":["# Replace the outlier values\n","df['vol_5g']=df['vol_5g'].replace([87978,254687],0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IsFPYmVrUqK_"},"outputs":[],"source":["# Check the quantiles of Volume of 5G data\n","df['vol_5g'].quantile([0.75,0.8,0.9,0.95,0.97,0.98,0.99,0.999])"]},{"cell_type":"markdown","metadata":{"id":"BlT6-z7dUtQx"},"source":["It seems good now"]},{"cell_type":"markdown","metadata":{"id":"8b_Raj69YQ0O"},"source":["Lets store this processed data for further use."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Tx18u4gUo5b"},"outputs":[],"source":["#df.to_csv('processed_telecom_offer_data.csv',index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NYbdYxSK9sij","outputId":"5c545039-8473-4152-aebb-83025dec0e5b"},"outputs":[{"data":{"text/html":["\n","    <iframe \n","\t    src=\"https://docs.google.com/forms/d/e/1FAIpQLScs70rjp77cH8TmVpH7jFhas7dmlCWSKEnNJnooa8meVUSnZA/viewform?usp=sf_link\" \n","\t    width=\"100%\" \n","\t    height=\"1200px\" \n","\t    frameborder=\"0\" \n","\t    marginheight=\"0\" \n","\t    marginwidth=\"0\">\n","\t    Loading...\n","    </iframe>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["feedback()"]},{"cell_type":"markdown","metadata":{"id":"DyP1FgDpbLko"},"source":["### **Data Preprocessing and Leakage**\n","\n","Data leakage is a situation where information from the test or prediction data is inadvertently used during the training process of a machine learning model. This can occur when information from the test or prediction data is leaked into the training data, and the model uses this information to improve its performance during the training process.\n","\n","Data leakage can occur during the preprocessing phase of machine learning when information from the test or prediction data is used to preprocess the training data, inadvertently leaking information from the test or prediction data into the training data.\n","\n","For example, consider a scenario where the preprocessing step involves imputing missing values in the dataset. If the missing values are imputed using the mean or median values of the entire dataset, including the test and prediction data, then the imputed values in the training data may be influenced by the values in the test and prediction data. This can lead to data leakage, as the model may learn to recognize patterns in the test and prediction data during the training process, leading to overfitting and poor generalization performance.\n","\n","\n","To avoid data leakage, it's important to perform the data preprocessing steps on the training data only, and then apply the same preprocessing steps to the test and prediction data separately. This ensures that the test and prediction data remain unseen by the model during the training process, and helps to prevent overfitting and improve the accuracy of the model.\n","\n","In the context of this problem, we performed all data preprocessing steps together for the sake of simplicity, which could potentially lead to data leakage. However, in real-world scenarios, it's important to treat the test and prediction data separately and apply the necessary preprocessing steps separately, based on the characteristics of the data."]},{"cell_type":"markdown","metadata":{"id":"EToY-qUS0HrJ"},"source":["# **Feature engineering**"]},{"cell_type":"markdown","metadata":{"id":"lNwB8_b9KWct"},"source":["Let's start by doing some variable selection and transformation. For selection, at this stage, we are going to use some business judgement to stick to what is possible to work with all the variables."]},{"cell_type":"markdown","metadata":{"id":"7oyoa5Q1KAvb"},"source":["**1. Splitting the dataset into a training and production dataset:**\n","\n","- Training: part of the customers who received offers which will be used to train the model\n","- Production: customers who did not received offers to whom we'd like to then offer something"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4UL5TIn40MSb"},"outputs":[],"source":["# Let's split our dataframe in a training and production dataset:\n","def split_dataframe(data):\n","    train = data[data['offer']!='No Offer']\n","    production = data[data['offer']=='No Offer']\n","    return train, production"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E1H_nMpPRB8a"},"outputs":[],"source":["train, production = split_dataframe(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w2nZyUtpcW7z"},"outputs":[],"source":["train.shape,production.shape"]},{"cell_type":"markdown","metadata":{"id":"aa-h9Ubycusn"},"source":["**Think about it**\n","\n","Why we split the dataframe into such unusual technique?\n","\n","Here we are not dealing with traditional train test split method as we are building an unsupervised collaborative recommender system.\n","Now to make model learn we need to pass all the data with respect to offers."]},{"cell_type":"markdown","metadata":{"id":"OMFqjdxtZPil"},"source":["we are creating 2 dataframes for each train and production, that have the Customer ID as a join key. This will help us manipulate features, and also trace them back to a particular customer;\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"12Bgf2xBKhCT"},"outputs":[],"source":["decomposition(\"fcTel3\")\n","\n","#This help us identify the customer and the business outcomes\n","id_variables = ['Customer ID', 'Month','Month of Joining','offer','Churn Category',\n","       'Churn Reason', 'Customer Status', 'Churn Value']\n","\n","\n","#This helps us identify the different profiles of customers\n","selected_variables = ['Customer ID', 'Month', 'Month of Joining', 'Gender', 'Age',\n","                      'Married', 'Number of Dependents', 'area_codes','roam_ic', 'roam_og',\n","                      'loc_og_t2t','loc_og_t2m', 'loc_og_t2f', 'loc_og_t2c', 'std_og_t2t', 'std_og_t2m',\n","                      'std_og_t2f', 'std_og_t2c', 'isd_og', 'spl_og', 'og_others',\n","                      'loc_ic_t2t', 'loc_ic_t2m', 'loc_ic_t2f', 'std_ic_t2t', 'std_ic_t2m',\n","                      'std_ic_t2f', 'std_ic_t2o', 'spl_ic', 'isd_ic', 'ic_others',\n","                      'total_rech_amt', 'total_rech_data', 'vol_4g', 'vol_5g', 'arpu_5g',\n","                      'arpu_4g', 'arpu', 'aug_vbc_5g','Number of Referrals', 'Phone Service',\n","                      'Multiple Lines', 'Internet Service', 'Internet Type',\n","                      'Streaming Data Consumption', 'Online Security', 'Online Backup',\n","                      'Device Protection Plan', 'Premium Tech Support', 'Streaming TV',\n","                      'Streaming Movies', 'Streaming Music', 'Unlimited Data',\n","                      'Payment Method']\n","\n","train_id=train[id_variables]\n","train_feat=train[selected_variables]\n","\n","prod_id=production[id_variables]\n","prod_feat=production[selected_variables]"]},{"cell_type":"markdown","metadata":{"id":"F7e-I2BrO3Ao"},"source":["In the code above, what we have essentially eliminated are complex variables like latitude, longitude and timezone because they could be represented by the area_codes variable, that represents location."]},{"cell_type":"markdown","metadata":{"id":"diYfs8eGONWg"},"source":["\n","**2. Converting the Month of Joining into a customer tenure**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YSMiXWB0Oaz5"},"outputs":[],"source":["train_feat['tenure'] = train_feat['Month']- train_feat['Month of Joining']\n","train_feat['tenure'].describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VDKEAzRJdbx6"},"outputs":[],"source":["prod_feat['tenure'] = prod_feat['Month']- prod_feat['Month of Joining']\n","prod_feat['tenure'].describe()"]},{"cell_type":"markdown","metadata":{"id":"GAdakJ_RPRbO"},"source":["**3.Transforming Categorical Variables**\n","\n","Transforming variables is an important step in the data preprocessing pipeline of machine learning, as it helps to convert the data into a format that is suitable for analysis and modeling. There are several ways to transform variables, depending on the type and nature of the data.\n","\n","Categorical variables, for example, are variables that take on discrete values from a finite set of categories, such as colors, gender, or occupation. One common way to transform categorical variables is through one-hot encoding. One-hot encoding involves creating a new binary variable for each category in the original variable, where the value is 1 if the observation belongs to that category and 0 otherwise. This approach is useful when the categories have no natural order or ranking.\n","\n","Another way to transform categorical variables is through label encoding. Label encoding involves assigning a unique integer value to each category in the variable. This approach is useful when the categories have a natural order or ranking, such as low, medium, and high.\n","Transforming categorical features into numerical labels:\n","\n","**Note:** We are NOT using dummies here to minimize the explosion of columns because of the distance methods we are using.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AXLeAwaULY52"},"outputs":[],"source":["# Now we need to transform the features of the feature store.\n","def encode_categorical_features(train_df,prod_df):\n","    # Get a list of all categorical columns\n","    cat_columns = train_df.select_dtypes(include=['object', 'category']).columns.tolist()\n","\n","    # Encode each categorical column\n","    for col in cat_columns:\n","        le = LabelEncoder()\n","        train_df[col] = le.fit_transform(train_df[col])\n","        prod_df[col]= le.transform(prod_df[col])\n","    return train_df, prod_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ly7e8gphfb98"},"outputs":[],"source":["#excluding the customer ID so it doesn't get encoded\n","train_label_data=train_feat[train_feat.columns.difference(['Customer ID','Month','Month of Joining'])]\n","prod_label_data=prod_feat[prod_feat.columns.difference(['Customer ID','Month','Month of Joining'])]\n","train_feat_enc, prod_feat_enc = encode_categorical_features(train_label_data,prod_label_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"35B-v4IrfxkU"},"outputs":[],"source":["##bringing back the customer ids keys\n","train_feat_enc['Customer ID'] = train_feat['Customer ID'] #bringing back the customer id\n","train_feat_enc['Month'] = train_feat['Month'] #bringing back the Month\n","train_feat_enc['Month of Joining'] = train_feat['Month of Joining'] #bringing back the Month of joining\n","\n","prod_feat_enc['Customer ID'] = prod_feat['Customer ID'] #bringing back the customer id\n","prod_feat_enc['Month'] = prod_feat['Month'] #bringing back the Month\n","prod_feat_enc['Month of Joining'] = prod_feat['Month of Joining'] #bringing back the Month of joining\n"]},{"cell_type":"markdown","metadata":{"id":"SqacW8_WPauz"},"source":["Taking a look at the final list of variables\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9fIMkkRoPnEW"},"outputs":[],"source":["train_feat_enc.describe().transpose()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QuMGWq1ygzrL"},"outputs":[],"source":["train = pd.merge(train_feat_enc,train_id[['Customer ID','Month','Month of Joining','Churn Value','offer']],how = 'inner',on=['Customer ID','Month','Month of Joining'])\n","production = pd.merge(prod_feat_enc,prod_id[['Customer ID','Month','Month of Joining','Churn Value','offer']],how = 'inner',on=['Customer ID','Month','Month of Joining'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"koKNoG05hO85"},"outputs":[],"source":["## This help us check if we did not duplicate anything in the columns\n","\n","print(len(train))\n","print(len(production))"]},{"cell_type":"markdown","metadata":{"id":"2jkdQctthZpY"},"source":["We finally have prepared model ready data.\n","\n","Lets look into model building now."]},{"cell_type":"markdown","metadata":{"id":"CKYd-nsAStJD"},"source":["# **Model Building and Testing**"]},{"cell_type":"markdown","metadata":{"id":"STkrMrK_iT32"},"source":["## **Collaborative filtering**\n","\n","Collaborative filtering is a type of recommendation system that uses user feedback to make personalized recommendations for items. It works by finding similarities between users or items and using those similarities to make predictions about what a user might like or dislike.\n","\n","Collaborative filtering can be broken down into two main types: user-based and item-based. In user-based collaborative filtering, similarities are calculated between users based on their past interactions with items. In item-based collaborative filtering, similarities are calculated between items based on how often they are interacted with by the same users.\n","\n","Distance measures are commonly used in collaborative filtering to calculate similarities between users or items. Some common distance measures used in collaborative filtering include Manhattan distance, Euclidean distance, and cosine similarity.\n","\n","The basic idea behind using distance measures in collaborative filtering is that similar users or items will be close together in the feature space defined by the data. For example, if we are recommending movies to users based on their past movie ratings, we might represent each user as a vector of their ratings, with each rating corresponding to a different movie. We could then calculate the distance between two users' rating vectors using a distance measure like cosine similarity or Euclidean distance. Users who have similar ratings for the same movies will be closer together in this feature space and therefore will have a smaller distance between them.\n","\n","Once we have calculated similarities between users or items, we can use those similarities to make predictions about what a user might like or dislike. For example, if we have calculated the similarity between two users and we know that one of them likes a certain movie, we can predict that the other user might also like that movie based on their similarity.\n","\n","There are many variations of collaborative filtering that use different distance measures and algorithms for finding similarities between users or items. Some examples include k-nearest neighbors (k-NN), which uses the distances between users to find the k users who are most similar to a given user, and matrix factorization, which uses linear algebra techniques to decompose the user-item interaction matrix into lower-dimensional matrices that capture user and item characteristics.\n","\n","In summary, collaborative filtering is a type of recommendation system that uses user feedback to make personalized recommendations for items. It uses distance measures to calculate similarities between users or items, which are then used to make predictions about what a user might like or dislike."]},{"cell_type":"markdown","metadata":{"id":"G68SNKJHKa89"},"source":["## **Mathematical explanation for distance measure**\n","\n","Manhattan, cosine, and Euclidean distance are different distance metrics used in machine learning and data science.\n","\n","### **Manhattan distance:**\n","Manhattan distance, also known as taxicab distance or L1 distance, is a measure of the distance between two points in a n-dimensional space. It is called Manhattan distance because it is analogous to the distance a taxi would travel on the streets of Manhattan, where you can only move in straight lines along the grid.\n","The formula for Manhattan distance between two points P and Q in n-dimensional space is:\n","\n","\\begin{equation}\n","d(P,Q) = |x_1-y_1| + |x_2-y_2| + ... + |x_n-y_n|\n","\\end{equation}\n","\n","where $x_1, x_2, ..., x_n$ are the coordinates of point $P$ and $y_1, y_2, ..., y_n$ are the coordinates of point $Q$.\n"]},{"cell_type":"markdown","metadata":{"id":"sjOGx5iUMEoh"},"source":["### **Cosine similarity:**\n","Cosine similarity is a measure of the similarity between two non-zero vectors of an inner product space. It is the cosine of the angle between the two vectors and ranges from -1 to 1. A value of 1 indicates that the two vectors are identical, while a value of -1 indicates that they are completely dissimilar.\n","The formula for cosine similarity between two vectors A and B is:\n","\n","\\begin{equation}\n","\\text{cosine   similarity}(A, B) = \\frac{A * B }{ ||A|| * ||B||}\n","\\end{equation}\n","\n","\n","\n","where $A * B$ is the dot product of vectors A and B, and $||A||$ and $||B||$ are the magnitudes of vectors A and B, respectively."]},{"cell_type":"markdown","metadata":{"id":"ExX-n0gnMlxH"},"source":["###**Euclidean distance:**\n","Euclidean distance is a measure of the distance between two points in a n-dimensional space. It is called Euclidean distance because it is the distance between two points in a straight line, as defined by Euclidean geometry.\n","The formula for Euclidean distance between two points P and Q in n-dimensional space is:\n","\\begin{equation}\n","d(P,Q) = \\sqrt{(x_1-y_1)^2 + (x_2-y_2)^2 + ... + (x_n-y_n)^2}\n","\\end{equation}\n","where $x_1$, $x_2$, ..., $x_n$ are the coordinates of point P and $y_1, y_2, ..., y_n$ are the coordinates of point Q."]},{"cell_type":"markdown","metadata":{"id":"ayS52CUhi-KD"},"source":["If you want to learn more about collaborative filtering, here is a recommended [project](https://www.projectpro.io/project-use-case/collaborative-filtering-recommender-systems-project) for you."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9RBlrrI4hal0"},"outputs":[],"source":["def get_recommended_offers (df:pd.DataFrame, df_id:pd.DataFrame,customer_id:str,month:int,distance_func:str,n,minimal_threshold:float,max_offers_to_return:int):\n","    \"\"\"\n","    This function takes as parameters:\n","    1. the dataframe where we'll be getting our data\n","    2. the customer identifiers Customer Id and the Month we want to make an offer for\n","    3. the distance function we want to use to calculate similaries between customers (see explanation below on how to chose it)\n","    4. The number of other customers we want to base our recommendations on\n","    5. The minimal threshold of prevalence of a given offer, in the similar group of customers, for it to be considered for recommendation (see explanation below on how to chose it)\n","\n","    It returns:\n","    An array with the list of offers that we could recommend to this customer\n","    \"\"\"\n","\n","    # extract the feature vectors of all customers\n","    features = list(df.columns.difference(['Customer ID','Month','Month of Joining','offer']))\n","    X = df[features].values\n","\n","    # extract the feature vector of the given customer\n","    index = df[(df['Customer ID'] == customer_id) & (df['Month']==month)].index[0]\n","    x = X[index]\n","\n","    # compute the distances between the feature vectors\n","    if distance_func == 'euclidean':\n","      distances = euclidean_distances(X, x.reshape(1, -1)).flatten()\n","    elif distance_func == 'manhattan':\n","      distances = manhattan_distances(X, x.reshape(1, -1)).flatten()\n","    elif distance_func == 'cosine':\n","      distances = 1 - cosine_similarity(X, x.reshape(1, -1)).flatten()\n","    else:\n","      raise ValueError('Invalid distance function specified.')\n","\n","    # find the indices of the n customers with lowest distance\n","    most_similar_indices = distances.argsort()[:n]\n","\n","    # extract the customer data for the most similar customers\n","    similar_customers = df.iloc[most_similar_indices]\n","\n","    # merge with the id dataframe to select only the customers who did not churn\n","    similar_customers = pd.merge(similar_customers,df_id[['Customer ID','Month of Joining','Month','Churn Value']],on=['Customer ID','Month of Joining','Month','Churn Value'])\n","\n","    # select the customers that did not churn\n","    similar_customers = similar_customers[similar_customers['Churn Value']==0]\n","\n","    #count the top offers of the non-churned customers\n","    top_offers = similar_customers[['Customer ID','offer']].groupby(['offer']).agg({'Customer ID':'count'}).reset_index().sort_values(by = 'Customer ID', ascending = False)\n","    top_offers['perc_total'] = top_offers['Customer ID']/top_offers['Customer ID'].sum()\n","    top_offers_min = top_offers[top_offers['perc_total']>minimal_threshold].head(max_offers_to_return)\n","\n","    return top_offers_min['offer'].unique()"]},{"cell_type":"markdown","metadata":{"id":"JfjMQDBBrUWq"},"source":["#### **Function Overview**\n","\n","This function finds n similar customers with respect to the distance metric provided by user from training data corresponding to specific customer.\n","\n","It further finds the most occurring offers given to set of n similar customers and recommend the top 3 offers.\n","\n","\n","##### **Input Parameters**\n","1. df: The main DataFrame that contains all the data for all customers and their offers\n","\n","2. df_id: A DataFrame that contains only the customer ID, month of joining, and churn value and offer\n","\n","3. customer_id: The ID of the customer we want to recommend offers for\n","\n","4. month: The month we want to make the offer for\n","\n","5. distance_func: The distance function to use for finding similar customers\n","\n","6. n: The number of other customers we want to base our recommendations on\n","\n","7. minimal_threshold: The minimal threshold of prevalence of a given offer in the similar group of customers, for it to be considered for recommendation\n","\n","8. max_offers_to_return: The maximum number of offers to return\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bREZyZr2t2AS"},"source":["##### **Step by Step Explanation**\n","\n","* Extracts the feature vectors of all customers by removing the 'Customer ID', 'Month', 'Month of Joining', and 'offer' columns from the main DataFrame\n","\n","```\n","features = list(df.columns.difference(['Customer ID','Month','Month of Joining','offer']))\n","X = df[features].values\n","```\n","\n","* Extracts the feature vector of the given customer\n","\n","```\n","index = df[(df['Customer ID'] == customer_id) & (df['Month']==month)].index[0]\n","x = X[index]\n","```\n","\n","* Computes the distances between the feature vectors of the given customer and all other customers using the specified distance function\n","\n","```\n","if distance_func == 'euclidean':\n","    distances = euclidean_distances(X, x.reshape(1, -1)).flatten()\n","elif distance_func == 'manhattan':\n","    distances = manhattan_distances(X, x.reshape(1, -1)).flatten()\n","elif distance_func == 'cosine':\n","     distances = 1 - cosine_similarity(X, x.reshape(1, -1)).flatten()\n","else:\n","    raise ValueError('Invalid distance function specified.')\n","```\n","\n","* Finds the indices of the n customers with the lowest distance to the given customer\n","\n","```\n","most_similar_indices = distances.argsort()[:n]\n","```\n","\n","* Extracts the customer data for the most similar customers\n","\n","```\n","similar_customers = df.iloc[most_similar_indices]\n","```\n","\n","* Merges the similar customers DataFrame with the ID DataFrame to select only the customers who did not churn\n","\n","```\n","similar_customers = pd.merge(similar_customers,df_id[['Customer ID','Month of Joining','Month','Churn Value']],on=['Customer ID','Month of Joining','Month','Churn Value'])\n","```\n","\n","* Selects the customers that did not churn\n","\n","```\n","similar_customers = similar_customers[similar_customers['Churn Value']==0]\n","```\n","\n","* Counts the top offers of the non-churned customers, calculates the percentage of each offer among the top offers, and selects the top max_offers_to_return offers whose percentage is above the minimal_threshold\n","\n","```\n","top_offers = similar_customers[['Customer ID','offer']].groupby(['offer']).agg({'Customer ID':'count'}).reset_index().sort_values(by = 'Customer ID', ascending = False)\n","top_offers['perc_total'] = top_offers['Customer ID']/top_offers['Customer ID'].sum()\n","top_offers_min = top_offers[top_offers['perc_total']>minimal_threshold].head\n","```"]},{"cell_type":"markdown","metadata":{"id":"xN7zrzdztpDY"},"source":["## **The minimal threshold parameter**\n","\n","Whenever we are building any framework, and especially unsupervised ones, it is important that we establish parameters that can help us have confidence in what we are doing.\n","\n","In this particular case, we are assigning a minimal threshold of 10% for an offer to be potentially chosen to customers.\n","\n","This comes from the fact that we have 10 offers (A-> J). If we were to randomly assign an offer to a customer, we would likely give each offer an equal probability of being assigned, so 100%/10%. So given that, if we are going to recommend something, it needs to be better than the random assignment."]},{"cell_type":"markdown","metadata":{"id":"smDVZ6BYSzxx"},"source":["To identify similar customers, we are going to treat each feature that we selected now in the train dataframe as a customer feature"]},{"cell_type":"markdown","metadata":{"id":"AdHeArLXrvmZ"},"source":["## **Which distance to choose?**\n","\n","In Summary\n","*  Manhattan distance: This metric measures the distance between two points by summing the absolute differences between their coordinates. It is also called the \"taxicab\" or \"city block\" distance because it measures the distance a taxicab would have to travel to get from one point to another on a city grid.\n","\n","*  Cosine similarity: This metric measures the cosine of the angle between two vectors in a high-dimensional space. It is commonly used in text analysis and information retrieval to measure the similarity between documents. Cosine similarity is often preferred over Euclidean distance when the magnitude of the vectors is not important, and only the direction matters.\n","\n","*  Euclidean distance: This metric measures the distance between two points in a straight line. It is the most common distance metric used in machine learning and data science. Euclidean distance is useful when the data is dense, and the features have similar scales.\n","\n","In general, if you have high-dimensional data or sparse data, Manhattan or cosine distance may be more appropriate. If you have dense data with similar scales, Euclidean distance is a good choice.\n","\n","For this problem in particular, we recommend Manhattan or Cosine"]},{"cell_type":"markdown","metadata":{"id":"_Ee2SDtMAiYH"},"source":["## **Applying this framework to a specific customer**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N1Ul8kdCuPqS"},"outputs":[],"source":["train[train['Customer ID']=='sirifvlkipkel21']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g1w_ygYu-LnI"},"outputs":[],"source":["customer_id = 'sirifvlkipkel21' # This Customer Id needs to be there from the training dataset\n","month = 12\n","distance_func = 'euclidean'\n","n = 1000\n","minimal_threshold= 0.10\n","max_offers_to_return = 3\n","model_snapshot(\"fcTel3\")\n","id_cols=['Customer ID','Month','Month of Joining','Churn Value','offer']\n","\n","offers = get_recommended_offers (train, train[id_cols], customer_id,month,distance_func,n,minimal_threshold,max_offers_to_return)\n","\n","print('The first offer to recommend is ' + str(offers[0]))\n","print('The second offer to recommend is ' + str(offers[1]))\n","print('The third offer to recommend is ' + str(offers[2]))"]},{"cell_type":"markdown","metadata":{"id":"aUp9hsxQXDYr"},"source":["Instead of adding this result to a list, we could also add it to a dataframe:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3RMo8QnwMJTt"},"outputs":[],"source":["frame = pd.DataFrame()\n","\n","data = {'Customer ID': [customer_id],\n","        'Month': [month],\n","        'offer 1': [str(offers[0])],\n","        'offer 2': [str(offers[1])],\n","        'offer 3': [str(offers[2])]}\n","\n","frame= pd.DataFrame(data)\n","frame.head(2)"]},{"cell_type":"markdown","metadata":{"id":"gPWTqNYVujTR"},"source":["This essentially tells us for the particular customer id we can recommend C, G, F offers respectively in month 12.\n"]},{"cell_type":"markdown","metadata":{"id":"84cgyD7pswmz"},"source":["## **Bootstrapping the framework**\n","\n","\n","Especially in unsupervised learning problems, it is always a good idea to run several approaches ('bootstrap') and chose the most common answer amongst the different models. This mechanism is similar to what algorithms like random forest do, for example: they fit several trees and each tree votes the final classification of a sample.\n","\n","We are going to play with the 3 distances we have in our function + the number of customers we pull the data from in order to get a voted answer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bopJoKsKnxXT"},"outputs":[],"source":["def find_similar_customers_multiple(df:pd.DataFrame, df_id:pd.DataFrame,customer_id:str,month:int,distance_funcs:list,n_values,minimal_threshold:float,max_offers_to_return:int):\n","    \"\"\"\n","    Given a dataframe, a customer_id, n values, and distance functions,\n","    run multiple iterations of the find_similar_customers function with different parameter combinations,\n","    and return the top 3 most common answers among those.\n","    \"\"\"\n","    results = []\n","    for n in n_values:\n","      for distance_func in distance_funcs:\n","          result = get_recommended_offers (df,df_id ,customer_id,month,distance_func,n,minimal_threshold,max_offers_to_return)\n","          results.append(result)\n","          # concatenate the arrays together\n","          concatenated_array = np.concatenate(results)\n","          # convert the concatenated array to a Python list\n","          result_list = list(concatenated_array)\n","          result_list\n","    if len(results) == 0:\n","        return None\n","    else:\n","        result_counts = pd.Series(result_list).value_counts()\n","        most_common_result = [result_counts.index[0],result_counts.index[1],result_counts.index[2]]\n","        return most_common_result"]},{"cell_type":"markdown","metadata":{"id":"lnlTkG9Vu8Ax"},"source":["#### **Function Overview**\n","\n","This function takes a dataframe, a customer ID, a month, a list of distance functions, a list of n values, a minimal threshold, and a max number of offers to return as input parameters. It uses these parameters to run multiple iterations of the get_recommended_offers function and returns the top 3 most common recommendations among them.\n","\n","First, the function initializes an empty results list to store the results of each iteration. Then, for each n value and distance function, it calls the get_recommended_offers function with the given parameters and appends the result to the results list. After all iterations are completed, the function concatenates the arrays together and converts them to a Python list.\n","\n","If the results list is empty, meaning no recommendations were generated, the function returns None. Otherwise, it uses the value_counts() function to count the frequency of each recommended offer in the result_list, and returns a list of the top 3 most common recommendations.\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xfOteTFcVNgE"},"source":["## **Applying this framework to a given customer** ##"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-FvGfXpPxxU"},"outputs":[],"source":["n_values = [100, 250, 500, 1000]\n","distance_funcs = ['euclidean', 'manhattan', 'cosine']\n","customer_id = 'sirifvlkipkel21'\n","month = 12\n","minimal_threshold= 0.10\n","max_offers_to_return = 3\n","model_snapshot(\"fcTel3\")\n","id_cols=['Customer ID','Month','Month of Joining','Churn Value','offer']\n","find_similar_customers_multiple(train, train[id_cols],customer_id,month,distance_funcs,n_values,minimal_threshold=0.10,max_offers_to_return=3)"]},{"cell_type":"markdown","metadata":{"id":"8VX33xqEV95H"},"source":["### Seeing how this function work in details"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rUpsERFQvQ16"},"outputs":[],"source":["# parameter values\n","n_values = [100, 250, 500, 1000]\n","distance_funcs = ['euclidean', 'manhattan', 'cosine']\n","customer_id = 'sirifvlkipkel21'\n","month = 12\n","minimal_threshold= 0.10\n","max_offers_to_return = 3\n","results = []\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qiwuSjAvWW8S"},"outputs":[],"source":["# for each value in n_values\n","# The number of other customers we want to base our recommendations on\n","for n in n_values:\n","  # chose distance metric from a list\n","  for distance_func in distance_funcs:\n","\n","      # get the offer recommendation by using get_recommended_offers function\n","      result = get_recommended_offers (train, train[id_cols],customer_id,month,distance_func,n,minimal_threshold,max_offers_to_return)\n","\n","      results.append(result)\n","      # concatenate the arrays together\n","      concatenated_array = np.concatenate(results)\n","      # convert the concatenated array to a Python list\n","      result_list = list(concatenated_array)\n","if len(results) == 0:\n","    None\n","else:\n","    result_counts = pd.Series(result_list).value_counts()\n","    most_common_result = [result_counts.index[0],result_counts.index[1],result_counts.index[2]]\n","    most_common_result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1TCudb-CUP_E"},"outputs":[],"source":["result_counts"]},{"cell_type":"markdown","metadata":{"id":"lzMCVp-rWgdR"},"source":["We can see above that the different iterations returned 7 potential offers. 3 of those were, however, much more common than 6 of the rest."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NtNODT4XU4gp"},"outputs":[],"source":["most_common_result"]},{"cell_type":"markdown","metadata":{"id":"J05M1V9rWtFU"},"source":["The most_common_result list will aggregate the top 3 offers"]},{"cell_type":"markdown","metadata":{"id":"aytaysL0ElML"},"source":["## **Applying this to the whole dataframe**"]},{"cell_type":"markdown","metadata":{"id":"rcUh5OWkBLRa"},"source":["This code defines a function called production_model that takes three arguments: df, distance_func, and n. df is a Pandas DataFrame that contains data about customer transactions. The distance_func is a function that takes two arguments and returns a distance measure between them. The n parameter specifies the number of recommended offers to return for each customer and month.\n","\n","The function initializes an empty DataFrame called frame and then iterates over each unique customer in the input DataFrame. For each customer, the function iterates over each unique month for that customer and calls another function called get_recommended_offers. This function returns a list of recommended offers for that customer and month based on their transaction history, using the distance_func and n parameters.\n","\n","The function then creates a new DataFrame called frame1 containing the Customer ID, Month, and the top three recommended offers for that customer and month, and appends it to the frame DataFrame. Finally, the function returns the frame DataFrame containing the recommendations for all customers and months in the input DataFrame."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ujgum4Xhjfxl"},"outputs":[],"source":["train.head(2)"]},{"cell_type":"markdown","metadata":{"id":"IyYnabWlBVUo"},"source":["### **For the function without bootstrapping**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MqwOA7J1mE3B"},"outputs":[],"source":["train_id.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S5S-zvZcMGoz"},"outputs":[],"source":["def production_model (df,train,production,distance_func,n):\n","  frame = pd.DataFrame()\n","  # For each customer in each month\n","  for customer in list(df['Customer ID'].unique()):\n","    for month in list(df[df['Customer ID']==customer]['Month'].unique()):\n","      #This part of the code adds the line we want to get offers to the training set, so we can use the distance formula\n","      data = pd.DataFrame()\n","      data = train.append(production[(production['Customer ID']==customer)&(production['Month']==month)])\n","      data= data.reset_index()\n","      data_id=data[['Customer ID', 'Month', 'Month of Joining','Churn Value']]\n","      results = get_recommended_offers(data,data_id,customer,month,distance_func,n,minimal_threshold=0.10,max_offers_to_return=3)\n","      data = {'Customer ID': [customer],\n","              'Month': [month],\n","              'offers': [results]}\n","      frame1 =  pd.DataFrame(data)\n","      frame = frame.append(frame1)\n","  return frame"]},{"cell_type":"markdown","metadata":{"id":"A4o5IzODw2mu"},"source":["#### **Function Overview**\n","\n","The production_model function takes in a dataframe df, a distance function distance_func, and a value n, and returns a new dataframe frame with recommended offer in each month.\n","\n","For each unique Customer ID and Month combination in the input dataframe, the function adds the line for that combination to a new dataframe data that includes all previous training data plus the current combination. The get_recommended_offers function is then called on this new data dataframe, using the given distance_func and n values, to obtain a list of recommended offers for that combination.\n","\n","A dictionary is then created to store the Customer ID, Month, and offers data, and a new dataframe frame1 is created from this dictionary. This new dataframe is then appended to the existing frame dataframe.\n","\n","Finally, the frame dataframe is returned, which contains the recommended offers for each unique Customer ID and Month combination in the input dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N_evBTFpYL1e"},"outputs":[],"source":["save_point(\"fcTel3\")\n","frame_production_100_samples = production_model (production.head(100),train=train,production=production,distance_func = 'euclidean',n = 250)\n","frame_production_100_samples.to_csv('offer_recommendation_without_bootstap.csv',index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0WaWaNHfpVQk"},"outputs":[],"source":["frame_production_100_samples.head(5)"]},{"cell_type":"markdown","metadata":{"id":"nvr46dg4BcFa"},"source":["### **For the function with bootstrapping**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Mb3Lu1JYPvT"},"outputs":[],"source":["def production_model_bootstrap (df,train,production,distance_funcs,n_values):\n","\n","  frame = pd.DataFrame()\n","  for customer in list(df['Customer ID'].unique()):\n","    for month in list(df[df['Customer ID']==customer]['Month'].unique()):\n","      #This part of the code adds the line we want to get offers to to the training set, so we can use the distance formula\n","      data = pd.DataFrame()\n","      data = train.append(production[(production['Customer ID']==customer)&(production['Month']==month)])\n","      data= data.reset_index()\n","      data_id=data[['Customer ID', 'Month', 'Month of Joining','Churn Value']]\n","      results = find_similar_customers_multiple(data,data_id,customer,month,distance_funcs=distance_funcs,n_values=n_values,minimal_threshold=0.10,max_offers_to_return=3)\n","      data = {'Customer ID': [customer],\n","              'Month': [month],\n","              'offers': [results]}\n","      frame1 =  pd.DataFrame(data)\n","      frame = frame.append(frame1)\n","  return frame"]},{"cell_type":"markdown","metadata":{"id":"Zh2_UwRVxJAL"},"source":["#### **Function Overview**\n","\n","The production_model function takes in a dataframe df, a distance function distance_func, and a value n, and returns a new dataframe frame with recommended offer in each month.\n","\n","For each unique Customer ID and Month combination in the input dataframe, the function adds the line for that combination to a new dataframe data that includes all previous training data plus the current combination. The find_similar_customers_multiple(boostraping function) function is then called on this new data dataframe, using the given distance_func and n values, to obtain a list of recommended offers for that combination.\n","\n","A dictionary is then created to store the Customer ID, Month, and offers data, and a new dataframe frame1 is created from this dictionary. This new dataframe is then appended to the existing frame dataframe.\n","\n","Finally, the frame dataframe is returned, which contains the recommended offers for each unique Customer ID and Month combination in the input dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wRHGvvzh-Q7a"},"outputs":[],"source":["save_point(\"fcTel3\")\n","frame_production_100_samples_bootstrap = production_model_bootstrap (production.head(100),train=train,production=production,distance_funcs=['euclidean', 'manhattan', 'cosine'],n_values=[250,500,1000])\n","frame_production_100_samples_bootstrap.to_csv('offer_recommendation_bootstrap.csv',index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sF_QVqxa3QKZ"},"outputs":[],"source":["frame_production_100_samples_bootstrap.head(5)"]},{"cell_type":"markdown","metadata":{"id":"W6xwPul03C5I"},"source":["### **Think about it**\n","\n","Is there any difference between the model developed with and without bootstrapping?\n","\n","Can you conclude anything on model performance?"]},{"cell_type":"markdown","metadata":{"id":"J_TSQUIJFrN3"},"source":["# **Try it out**\n","\n","- Can you modify the similarity functions to take into consideration other types of distances?\n","\n","- Can you modify the similarity functions to take into consideration different folds (parts) of the 'train' dataset?"]},{"cell_type":"markdown","metadata":{"id":"xzvBJoctGOIw"},"source":["## **Think about it**\n","\n","How would you validate this approach?\n","- You ideally could design a test where, for some control group, you offer a random offer (or use whatever method is used today). For some treatment, you use your model to assign the offer. You could then measure retention of these two groups;\n","\n","- Another option would be to train this model on part of the training set, and use another part of the training set to test it. You could check if you have recommended a 'winning offer', where 'winning' = an offer where that was taken by the customer and avoided churn in the next month"]},{"cell_type":"markdown","metadata":{"id":"I42k50Ms0UUF"},"source":["## **Conclusion**\n","\n","In this project we used a simple yet effective unsupervised model to provide offers to customers.\n","\n","A collaborative offer recommendation system can be a valuable tool for telecom companies to increase customer satisfaction and revenue. By using data on customer behavior and preferences, the system can provide personalized offers that are more likely to be accepted by customers.\n","\n","To implement such a system, several steps need to be taken, including collecting and cleaning data, creating customer profiles, selecting appropriate distance functions, and validating the system's performance. It is also essential to consider ethical considerations related to data privacy and security.\n","\n","Overall, a collaborative offer recommendation system can be a powerful tool for telecom companies to enhance their marketing strategies and provide better services to their customers. However, it is important to continuously evaluate and update the system to ensure its effectiveness and address any potential issues.\n","\n","\n","In conclusion, a successful data science project requires a clear understanding of the business problem and the data available, as well as the ability to select and apply appropriate data preprocessing techniques, feature engineering methods, and machine learning algorithms. It is also important to assess and optimize the performance of the model and communicate the results effectively to stakeholders."]},{"cell_type":"markdown","metadata":{"id":"UuYMNzwX0XHS"},"source":["## **Interview Questions**\n","\n","### **Collaborative Filtering:**\n","\n","* What is collaborative filtering and how does it work?\n","* Can you explain the difference between user-based and item-based collaborative filtering?\n","* Can you give an example of how collaborative filtering is used in real-world applications?\n","\n","### **Distance Measures:**\n","\n","* What are distance measures and why are they important in data science?\n","* Can you explain the difference between Euclidean distance and Manhattan distance?\n","\n","\n","### **Code Implementation:**\n","\n","* Can you walk me through the code for the get_recommended_offers function?\n","* How does the find_similar_customers_multiple function work?\n","* Can you explain the purpose of the production_model function and how it uses the get_recommended_offers function?\n","* How would you modify these functions to handle larger datasets or improve their performance?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"V59WCYKv0S0F","outputId":"bcb577df-0589-43e7-ac19-631a28a959ba"},"outputs":[{"data":{"text/html":["\n","    <iframe \n","\t    src=\"https://docs.google.com/forms/d/e/1FAIpQLScs70rjp77cH8TmVpH7jFhas7dmlCWSKEnNJnooa8meVUSnZA/viewform?usp=sf_link\" \n","\t    width=\"100%\" \n","\t    height=\"1200px\" \n","\t    frameborder=\"0\" \n","\t    marginheight=\"0\" \n","\t    marginwidth=\"0\">\n","\t    Loading...\n","    </iframe>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["feedback()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aAE-ZiKr3j9N"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["TD4mUs2O5nn4","6-AGRDjswx3C","6-8sAipDw7cV","Mhjzt133qQr8","tzcnEleB7xtB","5GKk8NGIDiGk","40sMBDeUESmO","Hu8BDftlyvXp","DyP1FgDpbLko","STkrMrK_iT32","sjOGx5iUMEoh"],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}